{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading files: 100%|██████████| 5/5 [00:52<00:00, 10.53s/file]\n",
      "Parsing nodes: 100%|██████████| 2496/2496 [00:01<00:00, 1660.13it/s]\n",
      "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:38<00:00, 52.67it/s]\n",
      "Generating embeddings: 100%|██████████| 496/496 [00:09<00:00, 53.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/Books\").load_data(show_progress=True)\n",
    "\n",
    "# Defina um caminho de cache personalizado\n",
    "cache_folder = \"C:/Users/claud/Desktop/embedding_model\"\n",
    "\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\", cache_folder=cache_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\claud\\Desktop\\IC_RAG\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    system_found = False\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>\\n{message.content}<|end|>\\n\"\n",
    "            system_found = True\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{message.content}<|end|>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>\\n{message.content}<|end|>\\n\"\n",
    "        else:\n",
    "            prompt += f\"<|user|>\\n{message.content}<|end|>\\n\"\n",
    "\n",
    "    # trailing prompt\n",
    "    prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "    if not system_found:\n",
    "        prompt = (\n",
    "            \"<|system|>\\nYou are a helpful AI assistant.<|end|>\\n\" + prompt\n",
    "        )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    model_kwargs={\n",
    "        \"trust_remote_code\": False,\n",
    "    },\n",
    "    generate_kwargs={\"do_sample\": True, \"temperature\": 0.1},\n",
    "    tokenizer_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    query_wrapper_prompt=(\n",
    "        \"<|system|>\\n\"\n",
    "        \"You are a helpful AI assistant.<|end|>\\n\"\n",
    "        \"<|user|>\\n\"\n",
    "        \"{query_str}<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    ),\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine(show_progress=True, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best activation function for neural networks depends on the specific problem and the characteristics of the data. However, ReLU (Rectified Linear Unit) activation function is commonly used due to its simplicity and efficiency. It helps avoid the vanishing gradient problem and allows for faster convergence during training. Other activation functions like ELU (Exponential Linear Unit) and SELU (Scaled Exponential Linear Unit) also have their advantages, such as self-normalizing properties and faster convergence rates. Ultimately, the choice of activation function should be based on the specific requirements of the neural network and the problem being solved."
     ]
    }
   ],
   "source": [
    "\n",
    "streaming_response = query_engine.query(\"What is the best activation function for neural networks?\")\n",
    "streaming_response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
