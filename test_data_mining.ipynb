{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfsyntax import readfile, metadata\n",
    "\n",
    "PATH = \"Livros/Futuros/MachineLearning/2024_Python_Deep_Learning_w_pacc21.pdf\"\n",
    "\n",
    "doc = readfile(PATH)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Cover', 1]\n",
      "[1, 'Title Page', 2]\n",
      "[1, 'Copyright and Credit', 3]\n",
      "[1, 'Contributors\\r', 4]\n",
      "[1, 'Table of Contents', 6]\n",
      "[1, 'Preface', 12]\n",
      "[1, 'Part 1: Introduction to Neural Networks', 18]\n",
      "[1, 'Chapter 1: Machine Learning – an Introduction', 20]\n",
      "[2, 'Technical requirements', 20]\n",
      "[2, 'Introduction to ML', 21]\n",
      "[2, 'Different ML approaches', 22]\n",
      "[3, 'Supervised learning', 22]\n",
      "[3, 'Unsupervised learning', 28]\n",
      "[3, 'Reinforcement learning', 32]\n",
      "[3, 'Components of an ML solution', 35]\n",
      "[3, 'Neural networks', 38]\n",
      "[3, 'Introducing PyTorch', 39]\n",
      "[2, 'Summary', 43]\n",
      "[1, 'Chapter 2: Neural Networks', 44]\n",
      "[2, 'Technical requirements', 44]\n",
      "[2, 'The need for NNs', 45]\n",
      "[2, 'The math of NNs', 45]\n",
      "[3, 'Linear algebra', 46]\n",
      "[3, 'An introduction to probability', 50]\n",
      "[3, 'Differential calculus', 56]\n",
      "[2, 'An introduction to NNs', 58]\n",
      "[3, 'Units – the smallest NN building block', 59]\n",
      "[3, 'Layers as operations', 61]\n",
      "[3, 'Multi-layer NNs', 63]\n",
      "[3, 'Activation functions', 64]\n",
      "[3, 'The universal approximation theorem', 66]\n",
      "[2, 'Training NNs', 69]\n",
      "[3, 'GD', 69]\n",
      "[3, 'Backpropagation', 73]\n",
      "[3, 'A code example of an NN for the XOR function', 75]\n",
      "[2, 'Summary', 81]\n",
      "[1, 'Chapter 3: Deep Learning Fundamentals', 82]\n",
      "[2, 'Technical requirements', 82]\n",
      "[2, 'Introduction to DL', 83]\n",
      "[2, 'Fundamental DL concepts', 84]\n",
      "[3, 'Feature learning', 85]\n",
      "[3, 'The reasons for DL’s popularity', 86]\n",
      "[2, 'Deep neural networks', 87]\n",
      "[2, 'Training deep neural networks', 88]\n",
      "[3, 'Improved activation functions', 89]\n",
      "[3, 'DNN regularization', 93]\n",
      "[2, 'Applications of DL', 96]\n",
      "[2, 'Introducing popular DL libraries', 99]\n",
      "[3, 'Classifying digits with Keras', 99]\n",
      "[3, 'Classifying digits with PyTorch', 103]\n",
      "[2, 'Summary', 106]\n",
      "[1, 'Part 2: Deep Neural Networks for Computer Vision', 108]\n",
      "[1, 'Chapter 4: Computer Vision with Convolutional Networks', 110]\n",
      "[2, 'Technical requirements', 111]\n",
      "[2, 'Intuition and justification for CNNs', 111]\n",
      "[2, 'Convolutional layers', 112]\n",
      "[3, 'A coding example of the convolution operation', 115]\n",
      "[3, 'Cross-channel and depthwise convolutions', 117]\n",
      "[3, 'Stride and padding in convolutional layers', 120]\n",
      "[2, 'Pooling layers', 121]\n",
      "[2, 'The structure of a convolutional network', 123]\n",
      "[2, 'Classifying images with PyTorch and Keras', 124]\n",
      "[3, 'Convolutional layers in deep learning libraries', 124]\n",
      "[3, 'Data augmentation', 124]\n",
      "[3, 'Classifying images with PyTorch', 125]\n",
      "[3, 'Classifying images with Keras', 128]\n",
      "[2, 'Advanced types of convolutions', 130]\n",
      "[3, '1D, 2D, and 3D convolutions', 130]\n",
      "[3, '1×1 convolutions', 131]\n",
      "[3, 'Depthwise separable convolutions', 131]\n",
      "[3, 'Dilated convolutions', 132]\n",
      "[3, 'Transposed convolutions', 133]\n",
      "[2, 'Advanced CNN models', 136]\n",
      "[3, 'Introducing residual networks', 137]\n",
      "[3, 'Inception networks', 140]\n",
      "[3, 'Introducing Xception', 145]\n",
      "[3, 'Squeeze-and-Excitation Networks', 146]\n",
      "[3, 'Introducing MobileNet', 147]\n",
      "[3, 'EfficientNet', 149]\n",
      "[3, 'Using pre-trained models with PyTorch and Keras', 150]\n",
      "[2, 'Summary', 151]\n",
      "[1, 'Chapter 5: Advanced Computer Vision Applications', 152]\n",
      "[2, 'Technical requirements', 153]\n",
      "[2, 'Transfer learning (TL)', 153]\n",
      "[3, 'Transfer learning with PyTorch', 155]\n",
      "[3, 'Transfer learning with Keras', 158]\n",
      "[2, 'Object detection', 162]\n",
      "[3, 'Approaches to object detection', 163]\n",
      "[3, 'Object detection with YOLO', 165]\n",
      "[3, 'Object detection with Faster R-CNN', 170]\n",
      "[2, 'Introducing image segmentation', 176]\n",
      "[3, 'Semantic segmentation with U-Net', 177]\n",
      "[3, 'Instance segmentation with Mask R-CNN', 179]\n",
      "[2, 'Image generation with diffusion models', 182]\n",
      "[3, 'Introducing generative models', 183]\n",
      "[3, 'Denoising Diffusion Probabilistic Models', 184]\n",
      "[2, 'Summary', 187]\n",
      "[1, 'Part 3: Natural Language Processing and Transformers', 188]\n",
      "[1, 'Chapter 6: Natural Language Processing and Recurrent Neural Networks', 190]\n",
      "[2, 'Technical requirements', 191]\n",
      "[2, 'Natural language processing', 191]\n",
      "[3, 'Tokenization', 192]\n",
      "[3, 'Introducing word embeddings', 197]\n",
      "[3, 'Word2Vec', 199]\n",
      "[3, 'Visualizing embedding vectors', 203]\n",
      "[3, 'Language modeling', 204]\n",
      "[2, 'Introducing RNNs', 206]\n",
      "[3, 'RNN implementation and training', 209]\n",
      "[3, 'Backpropagation through time', 211]\n",
      "[3, 'Vanishing and exploding gradients', 214]\n",
      "[3, 'Long-short term memory', 216]\n",
      "[3, 'Gated recurrent units', 220]\n",
      "[2, 'Implementing text classification', 221]\n",
      "[2, 'Summary', 226]\n",
      "[1, 'Chapter 7: The Attention Mechanism and Transformers', 228]\n",
      "[2, 'Technical requirements', 228]\n",
      "[2, 'Introducing seq2seq models', 229]\n",
      "[2, 'Understanding the attention mechanism', 231]\n",
      "[3, 'Bahdanau attention', 231]\n",
      "[3, 'Luong attention', 234]\n",
      "[3, 'General attention', 235]\n",
      "[3, 'Transformer attention', 237]\n",
      "[3, 'Implementing TA', 241]\n",
      "[2, 'Building transformers with attention', 244]\n",
      "[3, 'Transformer encoder', 245]\n",
      "[3, 'Transformer decoder', 248]\n",
      "[3, 'Putting it all together', 251]\n",
      "[3, 'Decoder-only and encoder-only models', 253]\n",
      "[3, 'Bidirectional Encoder Representations from Transformers', 253]\n",
      "[3, 'Generative Pre-trained Transformer', 258]\n",
      "[2, 'Summary', 261]\n",
      "[1, 'Chapter 8: Exploring Large Language Models in Depth', 262]\n",
      "[2, 'Technical requirements', 263]\n",
      "[2, 'Introducing LLMs', 263]\n",
      "[2, 'LLM architecture', 264]\n",
      "[3, 'LLM attention variants', 264]\n",
      "[3, 'Prefix decoder', 271]\n",
      "[3, 'Transformer nuts and bolts', 272]\n",
      "[3, 'Models', 275]\n",
      "[2, 'Training LLMs', 276]\n",
      "[3, 'Training datasets', 277]\n",
      "[3, 'Pre-training properties', 280]\n",
      "[3, 'FT with RLHF', 285]\n",
      "[2, 'Emergent abilities of LLMs', 287]\n",
      "[2, 'Introducing Hugging Face Transformers', 289]\n",
      "[2, 'Summary', 293]\n",
      "[1, 'Chapter 9: Advanced Applications of Large Language Models', 294]\n",
      "[2, 'Technical requirements', 294]\n",
      "[2, 'Classifying images with Vision Transformer', 295]\n",
      "[3, 'Using ViT with Hugging Face Transformers', 297]\n",
      "[2, 'Understanding the DEtection TRansformer', 299]\n",
      "[3, 'Using DetR with Hugging Face Transformers', 303]\n",
      "[2, 'Generating images with stable diffusion', 305]\n",
      "[3, 'Autoencoder', 306]\n",
      "[3, 'Conditioning transformer', 307]\n",
      "[3, 'Diffusion model', 309]\n",
      "[3, 'Using stable diffusion with Hugging Face Transformers', 310]\n",
      "[2, 'Exploring fine-tuning transformers', 313]\n",
      "[2, 'Harnessing the power of LLMs with LangChain', 315]\n",
      "[3, 'Using LangChain in practice', 316]\n",
      "[2, 'Summary', 319]\n",
      "[1, 'Part 4: Developing and Deploying Deep Neural Networks', 320]\n",
      "[1, 'Chapter 10: Machine Learning Operations (MLOps)', 322]\n",
      "[2, 'Technical requirements', 323]\n",
      "[2, 'Understanding model development', 323]\n",
      "[3, 'Choosing an NN framework', 323]\n",
      "[3, 'PyTorch versus TensorFlow versus JAX', 323]\n",
      "[3, 'Open Neural Network Exchange', 324]\n",
      "[3, 'Introducing TensorBoard', 329]\n",
      "[3, 'Developing NN models for edge devices with TF Lite', 333]\n",
      "[3, 'Mixed-precision training with PyTorch', 336]\n",
      "[2, 'Exploring model deployment', 337]\n",
      "[3, 'Deploying NN models with Flask', 337]\n",
      "[3, 'Building ML web apps with Gradio', 339]\n",
      "[2, 'Summary', 342]\n",
      "[1, 'Index', 344]\n",
      "[1, 'Other Books You May Enjoy', 359]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Step 2: Define the path to the PDF file\n",
    "PATH = \"Livros/Futuros/MachineLearning/2024_Python_Deep_Learning_w_pacc21.pdf\"\n",
    "\n",
    "# Step 3: Open the PDF file\n",
    "doc = fitz.open(PATH)\n",
    "\n",
    "# Step 4: Extract the bookmarks (table of contents)\n",
    "toc = doc.get_toc()\n",
    "\n",
    "# Step 5: Print the bookmarks\n",
    "for item in toc:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Cover', 1]\n",
      "[1, 'Title Page', 2]\n",
      "[1, 'Copyright and Credit', 3]\n",
      "[1, 'Contributors\\r', 4]\n",
      "[1, 'Table of Contents', 6]\n",
      "[1, 'Preface', 12]\n",
      "[1, 'Part 1: Introduction to Neural Networks', 18]\n",
      "[1, 'Chapter 1: Machine Learning – an Introduction', 20]\n",
      "[1, 'Chapter 2: Neural Networks', 44]\n",
      "[1, 'Chapter 3: Deep Learning Fundamentals', 82]\n",
      "[1, 'Part 2: Deep Neural Networks for Computer Vision', 108]\n",
      "[1, 'Chapter 4: Computer Vision with Convolutional Networks', 110]\n",
      "[1, 'Chapter 5: Advanced Computer Vision Applications', 152]\n",
      "[1, 'Part 3: Natural Language Processing and Transformers', 188]\n",
      "[1, 'Chapter 6: Natural Language Processing and Recurrent Neural Networks', 190]\n",
      "[1, 'Chapter 7: The Attention Mechanism and Transformers', 228]\n",
      "[1, 'Chapter 8: Exploring Large Language Models in Depth', 262]\n",
      "[1, 'Chapter 9: Advanced Applications of Large Language Models', 294]\n",
      "[1, 'Part 4: Developing and Deploying Deep Neural Networks', 320]\n",
      "[1, 'Chapter 10: Machine Learning Operations (MLOps)', 322]\n",
      "[1, 'Index', 344]\n",
      "[1, 'Other Books You May Enjoy', 359]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(toc)):\n",
    "    if(toc[i][0] == 1):\n",
    "        print(toc[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a ui for the user to select the chapters\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a dropdown widget for chapter selection\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a ui for the user to select the chapters\n",
    "# Create a dropdown widget for chapter selection\n",
    "chapter_options = [(item[1], i) for i, item in enumerate(toc) if item[0] == 1]\n",
    "chapter_dropdown = widgets.Dropdown(\n",
    "    options=chapter_options,\n",
    "    description='Chapter:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(chapter_dropdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
