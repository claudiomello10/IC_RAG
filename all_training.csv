Name,Importance_index,Page,IsChapter
Cover,1,1,False
Copyright,1,4,False
Table of Contents,1,5,False
Preface,1,13,False
The Machine Learning Tsunami,2,13,False
Machine Learning in Your Projects,2,13,False
Objective and Approach,2,14,False
Prerequisites,2,15,False
Roadmap,2,15,False
Other Resources,2,17,False
Conventions Used in This Book,2,18,False
Code Examples,2,19,False
Using Code Examples,2,20,False
O’Reilly Safari,2,20,False
How to Contact Us,2,20,False
Changes in the Second Edition,2,21,False
Acknowledgments,2,25,False
Part I. The Fundamentals of Machine Learning,1,27,False
Chapter 1. The Machine Learning Landscape,2,29,True
What Is Machine Learning?,3,30,False
Why Use Machine Learning?,3,30,False
Types of Machine Learning Systems,3,34,False
Supervised/Unsupervised Learning,4,34,False
Batch and Online Learning,4,41,False
Instance-Based Versus Model-Based Learning,4,44,False
Main Challenges of Machine Learning,3,50,False
Insufficient Quantity of Training Data,4,50,False
Nonrepresentative Training Data,4,52,False
Poor-Quality Data,4,53,False
Irrelevant Features,4,53,False
Overfitting the Training Data,4,54,False
Underfitting the Training Data,4,56,False
Stepping Back,4,56,False
Testing and Validating,3,57,False
Hyperparameter Tuning and Model Selection,4,58,False
Data Mismatch,4,59,False
Exercises,3,60,False
Chapter 2. End-to-End Machine Learning Project,2,63,True
Working with Real Data,3,64,False
Look at the Big Picture,3,65,False
Frame the Problem,4,65,False
Select a Performance Measure,4,68,False
Check the Assumptions,4,71,False
Get the Data,3,71,False
Create the Workspace,4,71,False
Download the Data,4,75,False
Take a Quick Look at the Data Structure,4,76,False
Create a Test Set,4,80,False
Discover and Visualize the Data to Gain Insights,3,84,False
Visualizing Geographical Data,4,85,False
Looking for Correlations,4,88,False
Experimenting with Attribute Combinations,4,91,False
Prepare the Data for Machine Learning Algorithms,3,92,False
Data Cleaning,4,93,False
Handling Text and Categorical Attributes,4,95,False
Custom Transformers,4,97,False
Feature Scaling,4,98,False
Transformation Pipelines,4,99,False
Select and Train a Model,3,101,False
Training and Evaluating on the Training Set,4,101,False
Better Evaluation Using Cross-Validation,4,102,False
Fine-Tune Your Model,3,105,False
Grid Search,4,105,False
Randomized Search,4,107,False
Ensemble Methods,4,108,False
Analyze the Best Models and Their Errors,4,108,False
Evaluate Your System on the Test Set,4,109,False
"Launch, Monitor, and Maintain Your System",3,110,False
Try It Out!,3,111,False
Exercises,3,111,False
Chapter 3. Classification,2,113,True
MNIST,3,113,False
Training a Binary Classifier,3,116,False
Performance Measures,3,116,False
Measuring Accuracy Using Cross-Validation,4,117,False
Confusion Matrix,4,118,False
Precision and Recall,4,120,False
Precision/Recall Tradeoff,4,121,False
The ROC Curve,4,125,False
Multiclass Classification,3,128,False
Error Analysis,3,130,False
Multilabel Classification,3,134,False
Multioutput Classification,3,135,False
Exercises,3,136,False
Chapter 4. Training Models,2,139,True
Linear Regression,3,140,False
The Normal Equation,4,142,False
Computational Complexity,4,145,False
Gradient Descent,3,145,False
Batch Gradient Descent,4,149,False
Stochastic Gradient Descent,4,152,False
Mini-batch Gradient Descent,4,155,False
Polynomial Regression,3,156,False
Learning Curves,3,158,False
Regularized Linear Models,3,162,False
Ridge Regression,4,163,False
Lasso Regression,4,165,False
Elastic Net,4,168,False
Early Stopping,4,168,False
Logistic Regression,3,170,False
Estimating Probabilities,4,170,False
Training and Cost Function,4,171,False
Decision Boundaries,4,172,False
Softmax Regression,4,175,False
Exercises,3,179,False
Chapter 5. Support Vector Machines,2,181,True
Linear SVM Classification,3,181,False
Soft Margin Classification,4,182,False
Nonlinear SVM Classification,3,185,False
Polynomial Kernel,4,186,False
Adding Similarity Features,4,187,False
Gaussian RBF Kernel,4,188,False
Computational Complexity,4,189,False
SVM Regression,3,190,False
Under the Hood,3,192,False
Decision Function and Predictions,4,192,False
Training Objective,4,193,False
Quadratic Programming,4,195,False
The Dual Problem,4,196,False
Kernelized SVM,4,197,False
Online SVMs,4,200,False
Exercises,3,201,False
Chapter 6. Decision Trees,2,203,True
Training and Visualizing a Decision Tree,3,203,False
Making Predictions,3,205,False
Estimating Class Probabilities,3,207,False
The CART Training Algorithm,3,208,False
Computational Complexity,3,209,False
Gini Impurity or Entropy?,3,209,False
Regularization Hyperparameters,3,210,False
Regression,3,211,False
Instability,3,214,False
Exercises,3,215,False
Chapter 7. Ensemble Learning and Random Forests,2,217,True
Voting Classifiers,3,218,False
Bagging and Pasting,3,221,False
Bagging and Pasting in Scikit-Learn,4,222,False
Out-of-Bag Evaluation,4,223,False
Random Patches and Random Subspaces,3,224,False
Random Forests,3,225,False
Extra-Trees,4,226,False
Feature Importance,4,226,False
Boosting,3,227,False
AdaBoost,4,228,False
Gradient Boosting,4,231,False
Stacking,3,236,False
Exercises,3,239,False
Chapter 8. Dimensionality Reduction,2,241,True
The Curse of Dimensionality,3,242,False
Main Approaches for Dimensionality Reduction,3,244,False
Projection,4,244,False
Manifold Learning,4,246,False
PCA,3,248,False
Preserving the Variance,4,248,False
Principal Components,4,249,False
Projecting Down to d Dimensions,4,250,False
Using Scikit-Learn,4,250,False
Explained Variance Ratio,4,251,False
Choosing the Right Number of Dimensions,4,251,False
PCA for Compression,4,252,False
Randomized PCA,4,253,False
Incremental PCA,4,253,False
Kernel PCA,3,254,False
Selecting a Kernel and Tuning Hyperparameters,4,255,False
LLE,3,258,False
Other Dimensionality Reduction Techniques,3,260,False
Exercises,3,261,False
Chapter 9. Unsupervised Learning Techniques,2,263,True
Clustering,3,264,False
K-Means,4,266,False
Limits of K-Means,4,276,False
Using clustering for image segmentation,4,277,False
Using Clustering for Preprocessing,4,278,False
Using Clustering for Semi-Supervised Learning,4,280,False
DBSCAN,4,282,False
Other Clustering Algorithms,4,285,False
Gaussian Mixtures,3,286,False
Anomaly Detection using Gaussian Mixtures,4,292,False
Selecting the Number of Clusters,4,293,False
Bayesian Gaussian Mixture Models,4,296,False
Other Anomaly Detection and Novelty Detection Algorithms,4,300,False
Part II. Neural Networks and Deep Learning,1,301,False
Chapter 10. Introduction to Artificial Neural Networks with Keras,2,303,True
From Biological to Artificial Neurons,3,304,False
Biological Neurons,4,305,False
Logical Computations with Neurons,4,307,False
The Perceptron,4,307,False
Multi-Layer Perceptron and Backpropagation,4,312,False
Regression MLPs,4,315,False
Classification MLPs,4,316,False
Implementing MLPs with Keras,3,318,False
Installing TensorFlow 2,4,319,False
Building an Image Classifier Using the Sequential API,4,320,False
Building a Regression MLP Using the Sequential API,4,329,False
Building Complex Models Using the Functional API,4,330,False
Building Dynamic Models Using the Subclassing API,4,335,False
Saving and Restoring a Model,4,337,False
Using Callbacks,4,337,False
Visualization Using TensorBoard,4,339,False
Fine-Tuning Neural Network Hyperparameters,3,341,False
Number of Hidden Layers,4,345,False
Number of Neurons per Hidden Layer,4,346,False
"Learning Rate, Batch Size and Other Hyperparameters",4,346,False
Exercises,3,348,False
Chapter 11. Training Deep Neural Networks,2,351,True
Vanishing/Exploding Gradients Problems,3,352,False
Glorot and He Initialization,4,353,False
Nonsaturating Activation Functions,4,355,False
Batch Normalization,4,359,False
Gradient Clipping,4,364,False
Reusing Pretrained Layers,3,365,False
Transfer Learning With Keras,4,367,False
Unsupervised Pretraining,4,369,False
Pretraining on an Auxiliary Task,4,370,False
Faster Optimizers,3,370,False
Momentum Optimization,4,371,False
Nesterov Accelerated Gradient,4,372,False
AdaGrad,4,373,False
RMSProp,4,375,False
Adam and Nadam Optimization,4,375,False
Learning Rate Scheduling,4,378,False
Avoiding Overfitting Through Regularization,3,382,False
ℓ1 and ℓ2 Regularization,4,382,False
Dropout,4,383,False
Monte-Carlo (MC) Dropout,4,386,False
Max-Norm Regularization,4,388,False
Summary and Practical Guidelines,3,389,False
Exercises,3,390,False
Chapter 12. Custom Models and Training with TensorFlow,2,393,True
A Quick Tour of TensorFlow,3,394,False
Using TensorFlow like NumPy,3,397,False
Tensors and Operations,4,397,False
Tensors and NumPy,4,399,False
Type Conversions,4,400,False
Variables,4,400,False
Other Data Structures,4,401,False
Customizing Models and Training Algorithms,3,402,False
Custom Loss Functions,4,402,False
Saving and Loading Models That Contain Custom Components,4,403,False
"Custom Activation Functions, Initializers, Regularizers, and Constraints",4,405,False
Custom Metrics,4,406,False
Custom Layers,4,409,False
Custom Models,4,412,False
Losses and Metrics Based on Model Internals,4,414,False
Computing Gradients Using Autodiff,4,415,False
Custom Training Loops,4,419,False
TensorFlow Functions and Graphs,3,422,False
Autograph and Tracing,4,424,False
TF Function Rules,4,426,False
Chapter 13. Loading and Preprocessing Data with TensorFlow,2,429,True
The Data API,3,430,False
Chaining Transformations,4,431,False
Shuffling the Data,4,432,False
Preprocessing the Data,4,435,False
Putting Everything Together,4,436,False
Prefetching,4,437,False
Using the Dataset With tf.keras,4,439,False
The TFRecord Format,3,440,False
Compressed TFRecord Files,4,441,False
A Brief Introduction to Protocol Buffers,4,441,False
TensorFlow Protobufs,4,442,False
Loading and Parsing Examples,4,444,False
Handling Lists of Lists Using the SequenceExample Protobuf,4,445,False
The Features API,3,446,False
Categorical Features,4,447,False
Crossed Categorical Features,4,447,False
Encoding Categorical Features Using One-Hot Vectors,4,448,False
Encoding Categorical Features Using Embeddings,4,449,False
Using Feature Columns for Parsing,4,452,False
Using Feature Columns in Your Models,4,452,False
TF Transform,3,454,False
The TensorFlow Datasets (TFDS) Project,3,455,False
Chapter 14. Deep Computer Vision Using Convolutional Neural Networks,2,457,True
The Architecture of the Visual Cortex,3,458,False
Convolutional Layer,3,460,False
Filters,4,462,False
Stacking Multiple Feature Maps,4,463,False
TensorFlow Implementation,4,465,False
Memory Requirements,4,467,False
Pooling Layer,3,468,False
TensorFlow Implementation,4,470,False
CNN Architectures,3,472,False
LeNet-5,4,475,False
AlexNet,4,476,False
GoogLeNet,4,478,False
VGGNet,4,482,False
ResNet,4,483,False
Xception,4,485,False
SENet,4,487,False
Implementing a ResNet-34 CNN Using Keras,3,490,False
Using Pretrained Models From Keras,3,491,False
Pretrained Models for Transfer Learning,3,493,False
Classification and Localization,3,495,False
Object Detection,3,497,False
Fully Convolutional Networks (FCNs),4,499,False
You Only Look Once (YOLO),4,501,False
Semantic Segmentation,3,504,False
Exercises,3,508,False
About the Author,1,510,False
Colophon,1,510,False
Front Cover,1,1,False
Machine Learning: A Bayesian and Optimization Perspective,1,4,False
Copyright ,1,5,False
Contents,1,6,False
Preface,1,18,False
Acknowledgments,1,20,False
Notation,1,22,False
Dedication ,1,24,False
Chapter 1: Introduction,1,26,True
1.1 What Machine Learning is About,2,26,False
1.1.1 Classification,3,27,False
1.1.2 Regression,3,28,False
1.2 Structure and a Road Map of the Book,2,30,False
References,2,33,False
Chapter 2: Probability and Stochastic Processes ,1,34,True
2.1 Introduction,2,35,False
2.2 Probability and Random Variables,2,35,False
2.2.1 Probability,3,36,False
Relative frequency definition,4,36,False
Axiomatic definition,4,36,False
2.2.2 Discrete Random Variables,3,37,False
Joint and conditional probabilities,4,37,False
Bayes theorem,4,38,False
2.2.3 Continuous Random Variables,3,39,False
2.2.4 Mean and Variance,3,40,False
Complex random variables,4,41,False
2.2.5 Transformation of Random Variables,3,42,False
2.3 Examples of Distributions,2,43,False
2.3.1 Discrete Variables,3,43,False
The Bernoulli distribution,4,43,False
The Binomial distribution,4,43,False
The Multinomial distribution,4,44,False
2.3.2 Continuous Variables,3,45,False
The uniform distribution,4,45,False
The Gaussian distribution,4,45,False
The central limit theorem,4,49,False
The exponential distribution,4,50,False
The beta distribution,4,50,False
The gamma distribution,4,51,False
The Dirichlet distribution,4,52,False
2.4 Stochastic Processes,2,54,False
2.4.1 First and Second Order Statistics,3,55,False
2.4.2 Stationarity and Ergodicity,3,55,False
2.4.3 Power Spectral Density,3,58,False
Properties of the autocorrelation sequence,4,58,False
Power spectral density,4,60,False
Transmission through a linear system,4,60,False
Physical interpretation of the PSD,4,62,False
2.4.4 Autoregressive Models,3,63,False
2.5 Information Theory,2,66,False
2.5.1 Discrete Random Variables,3,67,False
Information,4,67,False
Mutual and conditional information,4,68,False
Entropy and average mutual information,4,69,False
2.5.2 Continuous Random Variables,3,70,False
Average mutual information and conditional information,4,72,False
Relative entropy or Kullback-Leibler divergence,4,72,False
2.6 Stochastic Convergence,2,73,False
Convergence everywhere,3,73,False
Convergence almost everywhere,3,74,False
Convergence in the mean-square sense,3,74,False
Convergence in probability,3,74,False
Convergence in distribution,3,74,False
Problems,2,74,False
References,2,76,False
Chapter 3: Learning in Parametric Modeling: Basic Concepts and Directions ,1,78,True
3.1 Introduction,2,78,False
3.2 Parameter Estimation: The Deterministic Point of View,2,79,False
3.3 Linear Regression,2,82,False
3.4 Classification,2,85,False
Generative versus discriminative learning,3,88,False
"Supervised, semisupervised, and unsupervised learning",3,89,False
3.5 Biased Versus Unbiased Estimation,2,89,False
3.5.1 Biased or Unbiased Estimation?,3,90,False
3.6 The Cramér-Rao Lower Bound,2,92,False
3.7 Sufficient Statistic,2,95,False
3.8 Regularization,2,97,False
Inverse problems: Ill-conditioning and overfitting,3,99,False
3.9 The Bias-Variance Dilemma,2,102,False
3.9.1 Mean-Square Error Estimation,3,102,False
3.9.2 Bias-Variance Tradeoff,3,103,False
3.10 Maximum Likelihood Method,2,107,False
3.10.1 Linear Regression: The Nonwhite Gaussian Noise Case,3,109,False
3.11 Bayesian Inference,2,109,False
3.11.1 The Maximum A Posteriori Probability Estimation Method,3,113,False
3.12 Curse of Dimensionality,2,114,False
3.13 Validation,2,116,False
Cross-validation,3,117,False
3.14 Expected and Empirical Loss Functions,2,118,False
3.15 Nonparametric Modeling and Estimation,2,120,False
Problems,2,122,False
References,2,127,False
Chapter 4: Mean-Square Error Linear Estimation,1,130,True
4.1 Introduction,2,130,False
4.2 Mean-Square Error Linear Estimation: The Normal Equations,2,131,False
4.2.1 The Cost Function Surface,3,132,False
4.3 A Geometric Viewpoint: Orthogonality Condition,2,134,False
4.4 Extension to Complex-Valued Variables,2,136,False
4.4.1 Widely Linear Complex-Valued Estimation,3,138,False
Circularity conditions,4,139,False
4.4.2 Optimizing with Respect to Complex-Valued Variables: Wirtinger Calculus,3,141,False
4.5 Linear Filtering,2,143,False
4.6 MSE Linear Filtering: A Frequency Domain Point of View,2,145,False
Deconvolution: image deblurring,3,146,False
4.7 Some Typical Applications,2,149,False
4.7.1 Interference Cancellation,3,149,False
4.7.2 System Identification,3,150,False
4.7.3 Deconvolution: Channel Equalization,3,151,False
4.8 Algorithmic Aspects,2,157,False
Forward and backward MSE optimal predictors,3,159,False
4.8.1 The Lattice-Ladder Scheme,3,162,False
Orthogonality of the optimal backward errors,4,163,False
4.9 Mean-Square Error Estimation of Linear Models,2,165,False
4.9.1 The Gauss-Markov Theorem,3,168,False
4.9.2 Constrained Linear Estimation: The Beamforming Case,3,170,False
4.10 Time-Varying Statistics: Kalman Filtering,2,173,False
Problems,2,179,False
MATLAB Exercises,3,181,False
References,2,183,False
Chapter 5: Stochastic Gradient Descent: The LMS Algorithm and its Family,1,186,True
5.1 Introduction,2,187,False
5.2 The Steepest Descent Method,2,188,False
5.3 Application to the Mean-Square Error Cost Function,2,192,False
Time-varying step-sizes,3,199,False
5.3.1 The Complex-Valued Case,3,200,False
5.4 Stochastic Approximation,2,202,False
Application to the MSE linear estimation,3,203,False
5.5 The Least-Mean-Squares Adaptive Algorithm,2,204,False
5.5.1 Convergence and Steady-State Performance of the LMS in Stationary Environments,3,206,False
Convergence of the parameter error vector,4,206,False
5.5.2 Cumulative Loss Bounds,3,211,False
5.6 The Affine Projection Algorithm,2,213,False
Geometric interpretation of APA,3,214,False
Orthogonal projections,3,216,False
5.6.1 The Normalized LMS,3,218,False
5.7 The Complex-Valued Case,2,219,False
The widely linear LMS,3,220,False
The widely linear APA,3,220,False
5.8 Relatives of the LMS,2,221,False
The sign-error LMS,3,221,False
The least-mean-fourth (LMF) algorithm,3,221,False
Transform-domain LMS,3,222,False
5.9 Simulation Examples,2,224,False
5.10 Adaptive Decision Feedback Equalization,2,227,False
5.11 The Linearly Constrained LMS,2,229,False
5.12 Tracking Performance of the LMS in Nonstationary Environments,2,231,False
5.13 Distributed Learning: The Distributed LMS,2,233,False
5.13.1 Cooperation Strategies,3,234,False
Centralized networks,4,234,False
Decentralized networks,4,235,False
5.13.2 The Diffusion LMS,3,236,False
5.13.3 Convergence and Steady-State Performance: Some Highlights,3,243,False
5.13.4 Consensus-Based Distributed Schemes,3,245,False
5.14  A Case Study: Target Localization,2,247,False
5.15 Some Concluding Remarks: Consensus Matrix,2,248,False
Problems,2,249,False
MATLAB Exercises,3,251,False
References,2,252,False
Chapter 6: The Least-Squares Family,1,258,True
6.1 Introduction,2,259,False
6.2 Least-Squares Linear Regression: A Geometric Perspective,2,259,False
6.3 Statistical Properties of the LS Estimator,2,261,False
The LS estimator is unbiased,3,261,False
Covariance matrix of the LS estimator,3,261,False
The LS estimator is BLUE in the presence of white noise,3,262,False
The LS estimator achieves the Cramér-Rao bound for white Gaussian noise,3,263,False
Asymptotic distribution of the LS estimator,3,263,False
6.4 Orthogonalizing the Column Space of X: The SVD Method,2,264,False
Pseudo-inverse matrix and SVD,3,265,False
6.5 Ridge Regression,2,268,False
Principal components regression,3,269,False
6.6 The Recursive Least-Squares Algorithm,2,270,False
"Time-iterative computations of Φn, pn",3,271,False
Time updating of θn,3,272,False
6.7 Newton's Iterative Minimization Method,2,273,False
6.7.1 RLS and Newton's Method,3,276,False
6.8 Steady-State Performance of the RLS,2,277,False
6.9 Complex-Valued Data: The Widely Linear RLS,2,279,False
6.10 Computational Aspects of the LS Solution,2,280,False
Cholesky factorization,3,280,False
QR factorization,3,280,False
Fast RLS versions,3,281,False
6.11 The Coordinate and Cyclic Coordinate Descent Methods,2,283,False
6.12 Simulation Examples,2,284,False
6.13 Total-Least-Squares,2,286,False
Geometric interpretation of the total-least-squares method,3,291,False
Problems,2,293,False
MATLAB Exercises,3,296,False
References,2,297,False
Chapter 7: Classification: A Tour of the Classics,1,300,True
7.1 Introduction,2,300,False
7.2 Bayesian Classification,2,301,False
The Bayesian classifier minimizes the misclassification error,3,302,False
7.2.1 Average Risk,3,303,False
7.3 Decision (Hyper)Surfaces,2,305,False
7.3.1 The Gaussian Distribution Case,3,307,False
Minimum distance classifiers,4,310,False
7.4 The Naive Bayes Classifier,2,312,False
7.5 The Nearest Neighbor Rule,2,313,False
7.6 Logistic Regression,2,315,False
7.7 Fisher's Linear Discriminant,2,319,False
7.8 Classification Trees,2,325,False
7.9 Combining Classifiers,2,329,False
Experimental comparisons,3,329,False
Schemes for combining classifiers,3,330,False
7.10 The Boosting Approach,2,332,False
The AdaBoost algorithm,3,332,False
The log-loss function,3,336,False
7.11 Boosting Trees,2,338,False
7.12 A Case Study: Protein Folding Prediction,2,339,False
Protein folding prediction as a classification task,3,341,False
Classification of folding prediction via decision trees,3,343,False
Problems,2,343,False
MATLAB Exercises,3,345,False
References,2,348,False
Chapter 8: Parameter Learning: A Convex Analytic Path,1,352,True
8.1 Introduction,2,353,False
8.2 Convex Sets and Functions,2,354,False
8.2.1 Convex Sets,3,354,False
8.2.2 Convex Functions,3,355,False
8.3 Projections onto Convex Sets,2,358,False
8.3.1 Properties of Projections,3,362,False
8.4 Fundamental Theorem of Projections onto Convex Sets,2,366,False
8.5 A Parallel Version of POCS,2,369,False
8.6 From Convex Sets to Parameter Estimation and Machine Learning,2,370,False
8.6.1 Regression,3,370,False
8.6.2 Classification,3,372,False
8.7 Infinite Many Closed Convex Sets: The Online Learning Case,2,374,False
8.7.1 Convergence of APSM,3,376,False
Some practical hints,4,377,False
8.8 Constrained Learning,2,381,False
8.9 The Distributed APSM,2,382,False
8.10 Optimizing Nonsmooth Convex Cost Functions,2,383,False
8.10.1 Subgradients and Subdifferentials,3,384,False
8.10.2 Minimizing Nonsmooth Continuous Convex Loss Functions: The BatchLearning Case,3,387,False
The subgradient method,4,388,False
The generic projected subgradient scheme,4,390,False
The projected gradient method (PGM),4,390,False
Projected subgradient method,4,391,False
8.10.3 Online Learning for Convex Optimization,3,392,False
The PEGASOS algorithm,4,394,False
8.11 Regret Analysis,2,395,False
Regret analysis of the subgradient algorithm,3,397,False
8.12 Online Learning and Big Data Applications: A Discussion,2,399,False
"Approximation, estimation and optimization errors",3,399,False
Batch versus online learning,3,401,False
8.13 Proximal Operators,2,404,False
8.13.1 Properties of the Proximal Operator,3,407,False
8.13.2 Proximal Minimization,3,408,False
Resolvent of the subdifferential mapping,4,409,False
8.14 Proximal Splitting Methods for Optimization,2,410,False
The proximal forward-backward splitting operator,3,411,False
Alternating direction method of multipliers (ADMM),3,412,False
Mirror descent algorithms,3,413,False
Problems,2,414,False
MATLAB Exercises,3,417,False
8.15 Appendix to Chapter 8,2,418,False
References,2,423,False
Chapter 9: Sparsity-Aware Learning: Concepts and Theoretical Foundations,1,428,True
9.1 Introduction,2,428,False
9.2 Searching for a Norm,2,429,False
9.3 The Least Absolute Shrinkage and Selection Operator (LASSO),2,432,False
9.4 Sparse Signal Representation,2,436,False
9.5 In Search of the Sparsest Solution,2,440,False
The l2 norm minimizer,3,442,False
The l0 norm minimizer,3,442,False
The l1 norm minimizer,3,443,False
Characterization of the l1 norm minimizer,3,444,False
Geometric interpretation,3,444,False
9.6 Uniqueness of the l0 Minimizer,2,447,False
9.6.1 Mutual Coherence,3,449,False
9.7 Equivalence of l0 and l1 Minimizers: Sufficiency Conditions,2,451,False
9.7.1 Condition Implied by the Mutual Coherence Number,3,451,False
9.7.2 The Restricted Isometry Property (RIP),3,452,False
Constructing matrices that obey the RIP of order k,4,453,False
9.8 Robust Sparse Signal Recovery from Noisy Measurements,2,454,False
9.9 Compressed Sensing: The Glory of Randomness,2,455,False
Compressed sensing,3,456,False
9.9.1 Dimensionality Reduction and Stable Embeddings,3,458,False
9.9.2 Sub-Nyquist Sampling: Analog-to-Information Conversion,3,459,False
9.10 A Case Study: Image De-Noising,2,463,False
Problems,2,465,False
MATLAB Exercises,3,467,False
References,2,469,False
Chapter 10: Sparsity-Aware Learning: Algorithms and Applications ,1,474,True
10.1 Introduction,2,475,False
10.2 Sparsity-Promoting Algorithms,2,475,False
10.2.1 Greedy Algorithms,3,476,False
OMP can recover optimal sparse solutions: sufficiency condition,4,478,False
The LARS algorithm,4,479,False
Compressed sensing matching pursuit (CSMP) algorithms,4,480,False
10.2.2 Iterative Shrinkage/Thresholding (IST) Algorithms,3,481,False
10.2.3 Which Algorithm?: Some Practical Hints,3,487,False
10.3 Variations on the Sparsity-Aware Theme,2,492,False
10.4 Online Sparsity-Promoting Algorithms,2,500,False
10.4.1 LASSO: Asymptotic Performance,3,500,False
10.4.2 The Adaptive Norm-Weighted LASSO,3,502,False
10.4.3 Adaptive CoSaMP (AdCoSaMP) Algorithm,3,504,False
10.4.4 Sparse Adaptive Projection Subgradient Method (SpAPSM),3,505,False
Projection onto the weighted l1 ball,4,507,False
10.5 Learning Sparse Analysis Models,2,510,False
10.5.1 Compressed Sensing for Sparse Signal Representation in Coherent Dictionaries,3,512,False
10.5.2 Cosparsity,3,513,False
10.6 A Case Study: Time-Frequency Analysis,2,515,False
Gabor transform and frames,3,515,False
Time-frequency resolution,3,517,False
Gabor frames,3,518,False
Time-frequency analysis of echolocation signals emitted by bats,3,518,False
10.7 Appendix to Chapter 10: Some Hints from the Theory of Frames,2,522,False
Problems,2,525,False
MATLAB Exercises,3,526,False
References,2,527,False
Chapter 11: Learning in Reproducing Kernel Hilbert Spaces,1,534,True
11.1 Introduction,2,535,False
11.2 Generalized Linear Models,2,535,False
"11.3 Volterra, Wiener, and Hammerstein Models",2,536,False
11.4 Cover's Theorem: Capacity of a Space in Linear Dichotomies,2,539,False
11.5 Reproducing Kernel Hilbert Spaces,2,542,False
11.5.1 Some Properties and Theoretical Highlights,3,544,False
11.5.2 Examples of Kernel Functions,3,545,False
Constructing kernels,4,548,False
String kernels,4,549,False
11.6 Representer Theorem,2,550,False
11.6.1 Semiparametric Representer Theorem,3,552,False
11.6.2 Nonparametric Modeling: A Discussion,3,553,False
11.7 Kernel Ridge Regression,2,553,False
11.8 Support Vector Regression,2,555,False
11.8.1 The Linear ε-Insensitive Optimal Regression,3,556,False
The solution,4,558,False
Solving the optimization task,4,559,False
11.9 Kernel Ridge Regression Revisited,2,562,False
11.10 Optimal Margin Classification: Support Vector Machines,2,563,False
11.10.1 Linearly Separable Classes: Maximum Margin Classifiers ,3,565,False
The solution,4,568,False
The optimization task,4,569,False
11.10.2 Nonseparable Classes,3,570,False
The solution,4,571,False
The optimization task,4,571,False
11.10.3 Performance of SVMs and Applications,3,575,False
11.10.4 Choice of Hyperparameters,3,575,False
11.11 Computational Considerations,2,576,False
11.11.1 Multiclass Generalizations,3,577,False
11.12 Online Learning in RKHS,2,578,False
11.12.1 The Kernel LMS (KLMS),3,578,False
11.12.2 The Naive Online Rreg Minimization Algorithm (NORMA),3,581,False
Classification: the hinge loss function,4,583,False
Regression: the linear ε-insensitive loss function,4,584,False
Error bounds and convergence performance,4,584,False
11.12.3 The Kernel APSM Algorithm,3,585,False
Regression,4,585,False
Classification,4,586,False
11.13 Multiple Kernel Learning,2,592,False
11.14 Nonparametric Sparsity-Aware Learning: Additive Models,2,593,False
11.15 A Case Study: Authorship Identification,2,595,False
Problems,2,599,False
MATLAB Exercises,3,600,False
References,2,603,False
Chapter 12: Bayesian Learning: Inference and the EM Algorithm,1,610,True
12.1 Introduction,2,611,False
12.2 Regression: A Bayesian Perspective,2,611,False
12.2.1 The Maximum Likelihood Estimator,3,612,False
12.2.2 The MAP Estimator,3,613,False
12.2.3 The Bayesian Approach,3,614,False
12.3 The Evidence Function and Occam's Razor Rule,2,618,False
Laplacian approximation and the evidence function,3,621,False
12.4 Exponential Family of Probability Distributions,2,625,False
12.4.1 The Exponential Family and the Maximum Entropy Method,3,630,False
12.5 Latent Variables and the EM Algorithm,2,631,False
12.5.1 The Expectation-Maximization Algorithm,3,631,False
12.5.2 The EM Algorithm: A Lower Bound Maximization View,3,633,False
12.6 Linear Regression and the EM Algorithm,2,635,False
12.7 Gaussian Mixture Models,2,638,False
12.7.1 Gaussian Mixture Modeling and Clustering,3,642,False
12.8 Combining Learning Models: A Probabilistic Point of View,2,646,False
12.8.1 Mixing Linear Regression Models,3,647,False
Mixture of experts,4,649,False
Hierarchical mixture of experts,4,650,False
12.8.2 Mixing Logistic Regression Models,3,650,False
Problems,2,653,False
MATLAB Exercises,3,654,False
12.9 Appendix to Chapter 12,2,656,False
12.9.1 PDFs with Exponent of Quadratic Form,3,656,False
12.9.2 The Conditional from the Joint Gaussian pdf,3,657,False
12.9.3 The Marginal from the Joint Gaussian Pdf,3,658,False
12.9.4 The Posterior from Gaussian Prior and Conditional Pdfs,3,659,False
References,2,662,False
Chapter 13: Bayesian Learning: Approximate Inference and Nonparametric Models,1,664,True
13.1 Introduction,2,665,False
13.2 Variational Approximation in Bayesian Learning,2,665,False
The mean field approximation,3,666,False
13.2.1 The Case of the Exponential Family of Probability Distributions,3,669,False
13.3 A Variational Bayesian Approach to Linear Regression,2,670,False
Computation of the lower bound,3,675,False
13.4 A Variational Bayesian Approach to Gaussian Mixture Modeling,2,676,False
13.5 When Bayesian Inference Meets Sparsity,2,680,False
13.6 Sparse Bayesian Learning (SBL),2,682,False
13.6.1 The Spike and Slab Method,3,685,False
13.7 The Relevance Vector Machine Framework,2,686,False
13.7.1 Adopting the Logistic Regression Model for Classification,3,687,False
13.8 Convex Duality and Variational Bounds,2,691,False
13.9 Sparsity-Aware Regression: A Variational Bound Bayesian Path,2,696,False
13.10 Sparsity-Aware Learning: Some Concluding Remarks,2,700,False
Parameter identifiability and sparse Bayesian modeling,3,703,False
13.11 Expectation Propagation,2,704,False
Minimizing the KL divergence,3,705,False
The expectation propagation algorithm,3,706,False
13.12 Nonparametric Bayesian Modeling,2,708,False
13.12.1 The Chinese Restaurant Process,3,709,False
13.12.2 Inference,3,709,False
13.12.3 Dirichlet Processes,3,709,False
13.12.4 The Stick-Breaking Construction of a DP,3,710,False
13.13 Gaussian Processes,2,712,False
13.13.1 Covariance Functions and Kernels,3,713,False
13.13.2 Regression,3,715,False
Dealing with hyperparameters,4,716,False
Computational considerations,4,717,False
13.13.3 Classification,3,717,False
13.14 A Case Study: Hyperspectral Image Unmixing,2,718,False
13.14.1 Hierarchical Bayesian Modeling,3,720,False
13.14.2 Experimental Results,3,721,False
Problems,2,724,False
MATLAB Exercises,3,726,False
References,2,727,False
Chapter 14: Monte Carlo Methods,1,732,True
14.1 Introduction,2,732,False
14.2 Monte Carlo Methods: The Main Concept,2,733,False
14.2.1 Random number generation,3,734,False
14.3 Random Sampling Based on Function Transformation,2,736,False
14.4 Rejection Sampling,2,740,False
14.5 Importance Sampling,2,743,False
14.6 Monte Carlo Methods and the EM Algorithm,2,745,False
14.7 Markov Chain Monte Carlo Methods,2,746,False
14.7.1 Ergodic Markov Chains,3,748,False
14.8 The Metropolis Method,2,753,False
14.8.1 Convergence Issues,3,756,False
14.9 Gibbs Sampling,2,758,False
14.10 In Search of More Efficient Methods: A Discussion,2,760,False
Variational inference or Monte Carlo methods,3,761,False
14.11 A Case Study: Change-Point Detection,2,762,False
Problems,2,765,False
MATLAB Exercise,3,767,False
References,2,767,False
Chapter 15: Probabilistic Graphical Models: Part I,1,770,True
15.1 Introduction,2,770,False
15.2 The Need for Graphical Models,2,771,False
15.3 Bayesian Networks and the Markov Condition,2,773,False
15.3.1 Graphs: Basic Definitions,3,774,False
15.3.2 Some Hints on Causality,3,778,False
15.3.3 d-separation,3,780,False
15.3.4 Sigmoidal Bayesian Networks,3,783,False
15.3.5 Linear Gaussian Models,3,784,False
15.3.6 Multiple-Cause Networks,3,785,False
"15.3.7 I-Maps, Soundness, Faithfulness, and Completeness",3,786,False
15.4 Undirected Graphical Models,2,787,False
15.4.1 Independencies and I-Maps in Markov Random Fields,3,788,False
15.4.2 The Ising Model and Its Variants,3,790,False
15.4.3 Conditional Random Fields (CRFs),3,792,False
15.5 Factor Graphs,2,793,False
15.5.1 Graphical Models for Error-Correcting Codes,3,795,False
15.6 Moralization of Directed Graphs,2,797,False
15.7 Exact Inference Methods: Message-Passing Algorithms,2,798,False
15.7.1 Exact Inference in Chains,3,798,False
15.7.2 Exact Inference in Trees,3,802,False
15.7.3 The Sum-Product Algorithm,3,803,False
15.7.4 The Max-Product and Max-Sum Algorithms,3,807,False
Problems,2,814,False
References,2,816,False
Chapter 16: Probabilistic Graphical Models: Part II,1,820,True
16.1 Introduction,2,820,False
16.2 Triangulated Graphs and Junction Trees,2,821,False
16.2.1 Constructing a Join Tree,3,824,False
16.2.2 message-passing in Junction Trees,3,826,False
16.3 Approximate Inference Methods,2,829,False
16.3.1 Variational Methods: Local Approximation,3,829,False
Multiple-cause networks and the noisy-OR model,4,830,False
The Boltzmann machine,4,832,False
16.3.2 Block Methods for Variational Approximation,3,834,False
The mean field approximation and the Boltzmann machine,4,835,False
16.3.3 Loopy Belief Propagation,3,838,False
16.4 Dynamic Graphical Models,2,841,False
16.5 Hidden Markov Models,2,843,False
16.5.1 Inference,3,846,False
16.5.2 Learning the Parameters in an HMM,3,850,False
16.5.3 Discriminative Learning,3,853,False
16.6 Beyond HMMs: A Discussion ,2,854,False
16.6.1 Factorial Hidden Markov Models,3,854,False
16.6.2 Time-Varying Dynamic Bayesian Networks,3,857,False
16.7 Learning Graphical Models,2,858,False
16.7.1 Parameter Estimation,3,858,False
16.7.2 Learning the Structure,3,862,False
Problems,2,863,False
References,2,865,False
Chapter 17: Particle Filtering,1,870,True
17.1 Introduction,2,870,False
17.2 Sequential Importance Sampling,2,870,False
17.2.1 Importance Sampling Revisited,3,871,False
17.2.2 Resampling,3,872,False
17.2.3 Sequential Sampling,3,874,False
17.3 Kalman and Particle Filtering,2,876,False
17.3.1 Kalman Filtering: A Bayesian Point of View,3,877,False
17.4 Particle Filtering,2,879,False
17.4.1 Degeneracy,3,883,False
17.4.2 Generic Particle Filtering,3,885,False
17.4.3 Auxiliary Particle Filtering,3,887,False
Problems,2,893,False
MATLAB Exercises,3,896,False
References,2,897,False
Chapter 18: Neural Networks and Deep Learning,1,900,True
18.1 Introduction,2,901,False
18.2 The Perceptron,2,902,False
18.2.1 The Kernel Perceptron Algorithm,3,906,False
18.3 Feed-Forward Multilayer Neural Networks,2,907,False
18.4 The Backpropagation Algorithm,2,911,False
18.4.1 The Gradient Descent Scheme,3,912,False
Speeding up the convergence rate,4,918,False
Some practical hints,4,919,False
18.4.2 Beyond the Gradient Descent Rationale,3,920,False
18.4.3 Selecting a Cost Function,3,921,False
18.5 Pruning the Network,2,922,False
18.6 Universal Approximation Property of Feed-Forward Neural Networks,2,924,False
18.7 Neural Networks: A Bayesian Flavor,2,927,False
18.8 Learning Deep Networks,2,928,False
18.8.1 The Need for Deep Architectures,3,929,False
18.8.2 Training Deep Networks,3,930,False
Distributed representations,4,932,False
18.8.3 Training Restricted Boltzmann Machines,3,933,False
Computation of the conditional probabilities,4,935,False
Contrastive divergence,4,936,False
18.8.4 Training Deep Feed-Forward Networks,3,939,False
18.9 Deep Belief Networks,2,941,False
18.10 Variations on the Deep Learning Theme,2,943,False
18.10.1 Gaussian Units,3,943,False
18.10.2 Stacked Autoencoders,3,944,False
18.10.3 The Conditional RBM,3,945,False
18.11 Case Study: A Deep Network for Optical Character Recognition,2,948,False
18.12 CASE Study: A Deep Autoencoder,2,950,False
18.13 Example: Generating Data via a DBN,2,953,False
Problems,2,954,False
MATLAB Exercises,3,956,False
References,2,957,False
Chapter 19: Dimensionality Reduction and Latent variables Modeling ,1,962,True
19.1 Introduction,2,963,False
19.2 Intrinsic Dimensionality,2,964,False
19.3 Principle Component Analysis,2,964,False
"PCA, SVD, and low-Rank matrix factorization",3,966,False
Minimum error interpretation,3,968,False
PCA and information retrieval,3,968,False
Orthogonalizing properties of PCA and feature generation,3,968,False
Latent variables,3,969,False
19.4 Canonical Correlation Analysis,2,975,False
19.4.1 Relatives of CCA,3,978,False
Partial least-squares,4,979,False
19.5 Independent Component Analysis,2,980,False
19.5.1 ICA and Gaussianity,3,981,False
19.5.2 ICA and Higher Order Cumulants,3,982,False
ICA ambiguities,4,983,False
19.5.3 Non-Gaussianity and Independent Components,3,983,False
19.5.4 ICA Based on Mutual Information,3,984,False
19.5.5 Alternative Paths to ICA,3,987,False
The cocktail party problem,4,988,False
19.6 Dictionary Learning: The k-SVD Algorithm,2,991,False
Why the name k-SVD,3,993,False
19.7 Nonnegative Matrix Factorization,2,996,False
19.8 Learning Low-Dimensional Models: A Probabilistic Perspective,2,997,False
19.8.1 Factor Analysis,3,997,False
19.8.2 Probabilistic PCA,3,999,False
19.8.3 Mixture of Factors Analyzers: A Bayesian View to Compressed Sensing,3,1002,False
19.9 Nonlinear Dimensionality Reduction,2,1005,False
19.9.1 Kernel PCA,3,1005,False
19.9.2 Graph-Based Methods,3,1007,False
Laplacian eigenmaps,4,1007,False
Local linear embedding (LLE),4,1011,False
Isometric mapping (ISOMAP),4,1012,False
19.10 Low-Rank Matrix Factorization: A Sparse Modeling Path,2,1016,False
19.10.1 Matrix Completion,3,1016,False
19.10.2 Robust PCA,3,1020,False
19.10.3 Applications of Matrix Completion and ROBUST PCA,3,1021,False
Matrix completion,4,1021,False
Robust PCA/PCP,4,1022,False
19.11 A Case Study: fMRI Data Analysis,2,1023,False
Problems,2,1027,False
MATLAB Exercises,3,1027,False
References,2,1028,False
Appendix A: Linear Algebra,1,1038,True
A.1 Properties of Matrices,2,1038,False
Matrix inversion lemmas,3,1039,False
Matrix derivatives,3,1039,False
A.2 Positive Definite and Symmetric Matrices,2,1040,False
A.3 Wirtinger Calculus,2,1041,False
References,2,1042,False
Appendix B: Probability Theory and Statistics,1,1044,True
B.1 Cramér-Rao Bound,2,1044,False
B.2 Characteristic Functions,2,1045,False
B.3 Moments and Cumulants,2,1045,False
B.4 Edgeworth Expansion of a pdf,2,1046,False
Reference,2,1047,False
Appendix C: Hints on Constrained Optimization,1,1048,True
C.1 Equality Constraints,2,1048,False
C.2 Inequality Constraints,2,1050,False
The Karush-Kuhn-Tucker (KKT) conditions,3,1050,False
Min-Max duality,3,1051,False
Saddle point condition,3,1052,False
Lagrangian duality,3,1052,False
Convex programming,3,1053,False
Wolfe dual representation,3,1054,False
References,2,1054,False
Index,1,1056,False
Contents,1,4,False
Figures,1,14,False
Tables,1,19,False
Preface,1,21,False
--- Intro to Data Mining,1,28,False
What’s it all about?,1,29,True
DATA MINING AND MACHINE LEARNING,2,30,False
SIMPLE EXAMPLES: THE WEATHER PROBLEM AND OTHERS,2,35,False
FIELDED APPLICATIONS,2,47,False
THE DATA MINING PROCESS,2,54,False
MACHINE LEARNING AND STATISTICS,2,56,False
GENERALIZATION AS SEARCH,2,57,False
DATA MINING AND ETHICS,2,61,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,64,False
Input - Concepts Instances Attributes,1,68,True
WHAT’S A CONCEPT?,2,69,False
WHAT’S IN AN EXAMPLE?,2,71,False
WHAT’S IN AN ATTRIBUTE?,2,78,False
PREPARING THE INPUT,2,81,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,90,False
Output - Knowledge Representation,1,92,True
TABLES,2,93,False
LINEAR MODELS,2,93,False
TREES,2,95,False
RULES,2,100,False
INSTANCE-BASED REPRESENTATION,2,109,False
CLUSTERS,2,112,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,113,False
Algorithms - basic Methods,1,115,True
INFERRING RUDIMENTARY RULES,2,117,False
SIMPLE PROBABILISTIC MODELING,2,120,False
DIVIDE-AND-CONQUER: CONSTRUCTING DECISION TREES,2,129,False
COVERING ALGORITHMS: CONSTRUCTING RULES,2,137,False
MINING ASSOCIATION RULES,2,144,False
LINEAR MODELS,2,152,False
INSTANCE-BASED LEARNING,2,159,False
CLUSTERING,2,165,False
MULTI-INSTANCE LEARNING,2,180,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,182,False
WEKA IMPLEMENTATIONS,2,184,False
Credibility,1,185,True
TRAINING AND TESTING,2,187,False
PREDICTING PERFORMANCE,2,189,False
CROSS-VALIDATION,2,191,False
OTHER ESTIMATES,2,193,False
HYPERPARAMETER SELECTION,2,195,False
COMPARING DATA MINING SCHEMES,2,196,False
PREDICTING PROBABILITIES,2,200,False
COUNTING THE COST,2,203,False
EVALUATING NUMERIC PREDICTION,2,218,False
THE MDL PRINCIPLE,2,221,False
APPLYING THE MDL PRINCIPLE TO CLUSTERING,2,224,False
USING A VALIDATION SET FOR MODEL SELECTION,2,225,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,226,False
--- More advanced ML Schemes,1,228,False
Trees & Rules,1,232,True
DECISION TREES,2,233,False
CLASSIFICATION RULES,2,244,False
ASSOCIATION RULES,2,257,False
WEKA IMPLEMENTATIONS,2,265,False
Extending Instance-based & Linear Models,1,266,True
INSTANCE-BASED LEARNING,2,267,False
EXTENDING LINEAR MODELS,2,275,False
NUMERIC PREDICTION WITH LOCAL LINEAR MODELS,2,296,False
WEKA IMPLEMENTATIONS,2,307,False
Data Transformations,1,308,True
ATTRIBUTE SELECTION,2,311,False
DISCRETIZING NUMERIC ATTRIBUTES,2,319,False
PROJECTIONS,2,327,False
SAMPLING,2,338,False
CLEANSING,2,339,False
TRANSFORMING MULTIPLE CLASSES TO BINARY ONES,2,345,False
CALIBRATING CLASS PROBABILITIES,2,351,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,354,False
WEKA IMPLEMENTATIONS,2,357,False
Probabilistic Methods,1,358,True
FOUNDATIONS,2,359,False
BAYESIAN NETWORKS,2,362,False
CLUSTERING AND PROBABILITY DENSITY ESTIMATION,2,375,False
HIDDEN VARIABLE MODELS,2,386,False
BAYESIAN ESTIMATION AND PREDICTION,2,390,False
GRAPHICAL MODELS AND FACTOR GRAPHS,2,393,False
CONDITIONAL PROBABILITY MODELS,2,415,False
SEQUENTIAL AND TEMPORAL MODELS,2,426,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,433,False
WEKA IMPLEMENTATIONS,2,439,False
Deep Learning,1,440,True
DEEP FEEDFORWARD NETWORKS,2,443,False
TRAINING AND EVALUATING DEEP NETWORKS,2,454,False
CONVOLUTIONAL NEURAL NETWORKS,2,460,False
AUTOENCODERS,2,468,False
STOCHASTIC DEEP NETWORKS,2,472,False
RECURRENT NEURAL NETWORKS,2,479,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,484,False
DEEP LEARNING SOFTWARE AND NETWORK IMPLEMENTATIONS,2,487,False
WEKA IMPLEMENTATIONS,2,489,False
Beyond Supervised & Unsupervised Learning,1,490,True
SEMISUPERVISED LEARNING,2,491,False
MULTI-INSTANCE LEARNING,2,495,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,500,False
WEKA IMPLEMENTATIONS,2,501,False
Ensemble Learning,1,502,True
COMBINING MULTIPLE MODELS,2,503,False
BAGGING,2,504,False
RANDOMIZATION,2,507,False
BOOSTING,2,509,False
ADDITIVE REGRESSION,2,513,False
INTERPRETABLE ENSEMBLES,2,516,False
STACKING,2,520,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,522,False
WEKA IMPLEMENTATIONS,2,524,False
Applications & beyond,1,525,True
APPLYING MACHINE LEARNING,2,526,False
LEARNING FROM MASSIVE DATASETS,2,528,False
DATA STREAM LEARNING,2,531,False
INCORPORATING DOMAIN KNOWLEDGE,2,534,False
TEXT MINING,2,537,False
WEB MINING,2,541,False
IMAGES AND SPEECH,2,544,False
ADVERSARIAL SITUATIONS,2,546,False
UBIQUITOUS DATA MINING,2,549,False
FURTHER READING AND BIBLIOGRAPHIC NOTES,2,551,False
WEKA IMPLEMENTATIONS,2,554,False
Theoretical Foundations,1,555,True
A.1 MATRIX ALGEBRA,2,555,False
A.2 FUNDAMENTAL ELEMENTS OF PROBABILISTIC METHODS,2,564,False
WEKA workbench,1,575,False
B.1 WHAT’S IN WEKA?,2,575,False
B.2 THE PACKAGE MANAGEMENT SYSTEM,2,577,False
B.3 THE EXPLORER,2,579,False
B.4 THE KNOWLEDGE FLOW INTERFACE,2,586,False
B.5 THE EXPERIMENTER,2,590,False
Refs,1,594,False
Index,1,622,False
Contents at a Glance,1,5,False
Contents,1,6,False
About the Author,1,11,False
About the Technical Reviewer,1,12,False
Acknowledgments,1,13,False
Chapter 1: Introduction to Deep Learning,1,14,False
Historical Context,2,14,False
Advances in Related Fields,2,16,False
Prerequisites,2,16,False
Overview of Subsequent Chapters,2,17,False
Installing the Required Libraries,2,18,False
Chapter 2: Machine Learning Fundamentals,1,19,False
Intuition,2,19,False
Binary Classification,2,19,False
Regression,2,20,False
Generalization,2,21,False
Regularization,2,26,False
Summary,2,28,False
Chapter 3: Feed Forward Neural Networks,1,29,False
Unit,2,29,False
Overall Structure of a Neural Network,3,31,False
Expressing the Neural Network in Vector Form,3,32,False
Evaluating the output of the Neural Network,3,33,False
Training the Neural Network,3,35,False
Deriving Cost Functions using Maximum Likelihood,2,36,False
Binary Cross Entropy,3,37,False
Cross Entropy,3,37,False
Squared Error,3,38,False
Summary of Loss Functions,3,39,False
Types of Units/Activation Functions/Layers,2,39,False
Linear Unit,3,40,False
Sigmoid Unit,3,40,False
Softmax Layer,3,41,False
Rectified Linear Unit (ReLU),3,41,False
Hyperbolic Tangent,3,42,False
Neural Network Hands-on with AutoGrad,2,45,False
Summary,2,45,False
Chapter 4: Introduction to Theano,1,46,False
What is Theano,2,46,False
Theano Hands-On,2,47,False
Summary,2,72,False
Chapter 5: Convolutional Neural Networks,1,73,False
Convolution Operation,2,73,False
Pooling Operation,2,80,False
Convolution-Detector-Pooling Building Block,2,82,False
Convolution Variants,2,86,False
Intuition behind CNNs,2,87,False
Summary,2,88,False
Chapter 6: Recurrent Neural Networks,1,89,False
RNN Basics,2,89,False
Training RNNs,2,94,False
Bidirectional RNNs,2,101,False
Gradient Explosion and Vanishing,2,102,False
Gradient Clipping,2,103,False
Long Short Term Memory,2,105,False
Summary,2,106,False
Chapter 7: Introduction to Keras,1,107,False
Summary,2,121,False
Chapter 8: Stochastic Gradient Descent,1,122,False
Optimization Problems,2,122,False
Method of Steepest Descent,2,123,False
"Batch, Stochastic (Single and Mini-batch) Descent",2,124,False
Batch,3,125,False
Stochastic Single Example,3,125,False
Stochastic Mini-batch,3,125,False
Batch vs. Stochastic,3,125,False
Challenges with SGD,2,125,False
Local Minima,3,125,False
Saddle Points,3,126,False
Selecting the Learning Rate,3,127,False
Slow Progress in Narrow Valleys,3,128,False
Algorithmic Variations on SGD,2,128,False
Momentum,3,129,False
Nesterov Accelerated Gradient (NAS),3,130,False
Annealing and Learning Rate Schedules,3,130,False
Adagrad,3,130,False
RMSProp,3,131,False
Adadelta,3,132,False
Adam,3,132,False
Resilient Backpropagation,3,132,False
Equilibrated SGD,3,133,False
Tricks and Tips for using SGD,2,133,False
Preprocessing Input Data,3,133,False
Choice of Activation Function,3,133,False
Preprocessing Target Value,3,134,False
Initializing Parameters,3,134,False
Shuffling Data,3,134,False
Batch Normalization,3,134,False
Early Stopping,3,134,False
Gradient Noise,3,134,False
Parallel and Distributed SGD,2,135,False
Hogwild,3,135,False
Downpour,3,135,False
Hands-on SGD with Downhill,2,136,False
Summary,2,141,False
Chapter 9: Automatic Differentiation,1,142,False
Numerical Differentiation,2,142,False
Symbolic Differentiation,2,143,False
Automatic Differentiation Fundamentals,2,144,False
Forward/Tangent Linear Mode,3,145,False
Reverse/Cotangent/Adjoint Linear Mode,3,149,False
Implementation of Automatic Differentiation,3,152,False
Source Code Transformation,4,152,False
Operator Overloading,4,153,False
Hands-on Automatic Differentiation with Autograd,2,154,False
Summary,2,157,False
Chapter 10: Introduction to GPUs,1,158,False
Summary,2,167,False
Chapter 11: Introduction to Tensorflow,1,168,False
Summary,2,203,False
Chapter 12: Introduction to PyTorch,1,204,False
Summary,2,217,False
Chapter 13: Regularization Techniques,1,218,False
"Model Capacity, Overfitting, and Underfitting",2,218,False
Regularizing the Model,2,219,False
Early Stopping,2,219,False
Norm Penalties,2,221,False
Dropout,2,222,False
Summary,2,223,False
Chapter 14: Training Deep Learning Models,1,224,False
Performance Metrics,2,224,False
Data Procurement,2,227,False
Splitting Data for Training/Validation/Test,2,228,False
Establishing Achievable Limits on the Error Rate,2,228,False
Establishing the Baseline with Standard Choices,2,229,False
"Building an Automated, End-to-End Pipeline",2,229,False
Orchestration for Visibility,3,229,False
Analysis of Overfitting and Underfitting,2,229,False
Hyper-Parameter Tuning,2,231,False
Summary,2,231,False
Index,1,232,False
Contents at a Glance,1,5,False
Contents,1,6,False
About the Author,1,11,False
About the Technical Reviewer,1,12,False
Acknowledgments,1,13,False
Chapter 1: Introduction to Deep Learning,1,14,True
Historical Context,2,14,False
Advances in Related Fields,2,16,False
Prerequisites,2,16,False
Overview of Subsequent Chapters,2,17,False
Installing the Required Libraries,2,18,False
Chapter 2: Machine Learning Fundamentals,1,19,True
Intuition,2,19,False
Binary Classification,2,19,False
Regression,2,20,False
Generalization,2,21,False
Regularization,2,26,False
Summary,2,28,False
Chapter 3: Feed Forward Neural Networks,1,29,True
Unit,2,29,False
Overall Structure of a Neural Network,3,31,False
Expressing the Neural Network in Vector Form,3,32,False
Evaluating the output of the Neural Network,3,33,False
Training the Neural Network,3,35,False
Deriving Cost Functions using Maximum Likelihood,2,36,False
Binary Cross Entropy,3,37,False
Cross Entropy,3,37,False
Squared Error,3,38,False
Summary of Loss Functions,3,39,False
Types of Units/Activation Functions/Layers,2,39,False
Linear Unit,3,40,False
Sigmoid Unit,3,40,False
Softmax Layer,3,41,False
Rectified Linear Unit (ReLU),3,41,False
Hyperbolic Tangent,3,42,False
Neural Network Hands-on with AutoGrad,2,45,False
Summary,2,45,False
Chapter 4: Introduction to Theano,1,46,True
What is Theano,2,46,False
Theano Hands-On,2,47,False
Summary,2,72,False
Chapter 5: Convolutional Neural Networks,1,73,True
Convolution Operation,2,73,False
Pooling Operation,2,80,False
Convolution-Detector-Pooling Building Block,2,82,False
Convolution Variants,2,86,False
Intuition behind CNNs,2,87,False
Summary,2,88,False
Chapter 6: Recurrent Neural Networks,1,89,True
RNN Basics,2,89,False
Training RNNs,2,94,False
Bidirectional RNNs,2,101,False
Gradient Explosion and Vanishing,2,102,False
Gradient Clipping,2,103,False
Long Short Term Memory,2,105,False
Summary,2,106,False
Chapter 7: Introduction to Keras,1,107,True
Summary,2,121,False
Chapter 8: Stochastic Gradient Descent,1,122,True
Optimization Problems,2,122,False
Method of Steepest Descent,2,123,False
"Batch, Stochastic (Single and Mini-batch) Descent",2,124,False
Batch,3,125,False
Stochastic Single Example,3,125,False
Stochastic Mini-batch,3,125,False
Batch vs. Stochastic,3,125,False
Challenges with SGD,2,125,False
Local Minima,3,125,False
Saddle Points,3,126,False
Selecting the Learning Rate,3,127,False
Slow Progress in Narrow Valleys,3,128,False
Algorithmic Variations on SGD,2,128,False
Momentum,3,129,False
Nesterov Accelerated Gradient (NAS),3,130,False
Annealing and Learning Rate Schedules,3,130,False
Adagrad,3,130,False
RMSProp,3,131,False
Adadelta,3,132,False
Adam,3,132,False
Resilient Backpropagation,3,132,False
Equilibrated SGD,3,133,False
Tricks and Tips for using SGD,2,133,False
Preprocessing Input Data,3,133,False
Choice of Activation Function,3,133,False
Preprocessing Target Value,3,134,False
Initializing Parameters,3,134,False
Shuffling Data,3,134,False
Batch Normalization,3,134,False
Early Stopping,3,134,False
Gradient Noise,3,134,False
Parallel and Distributed SGD,2,135,False
Hogwild,3,135,False
Downpour,3,135,False
Hands-on SGD with Downhill,2,136,False
Summary,2,141,False
Chapter 9: Automatic Differentiation,1,142,True
Numerical Differentiation,2,142,False
Symbolic Differentiation,2,143,False
Automatic Differentiation Fundamentals,2,144,False
Forward/Tangent Linear Mode,3,145,False
Reverse/Cotangent/Adjoint Linear Mode,3,149,False
Implementation of Automatic Differentiation,3,152,False
Source Code Transformation,4,152,False
Operator Overloading,4,153,False
Hands-on Automatic Differentiation with Autograd,2,154,False
Summary,2,157,False
Chapter 10: Introduction to GPUs,1,158,True
Summary,2,167,False
Chapter 11: Introduction to Tensorflow,1,168,True
Summary,2,203,False
Chapter 12: Introduction to PyTorch,1,204,True
Summary,2,217,False
Chapter 13: Regularization Techniques,1,218,True
"Model Capacity, Overfitting, and Underfitting",2,218,False
Regularizing the Model,2,219,False
Early Stopping,2,219,False
Norm Penalties,2,221,False
Dropout,2,222,False
Summary,2,223,False
Chapter 14: Training Deep Learning Models,1,224,True
Performance Metrics,2,224,False
Data Procurement,2,227,False
Splitting Data for Training/Validation/Test,2,228,False
Establishing Achievable Limits on the Error Rate,2,228,False
Establishing the Baseline with Standard Choices,2,229,False
"Building an Automated, End-to-End Pipeline",2,229,False
Orchestration for Visibility,3,229,False
Analysis of Overfitting and Underfitting,2,229,False
Hyper-Parameter Tuning,2,231,False
Summary,2,231,False
Index,1,232,False
Contents,1,5,False
About the Author,1,12,False
About the Technical Reviewer,1,13,False
Acknowledgments,1,14,False
Introduction,1,15,False
Chapter 1: Mathematical Foundations,1,18,True
Linear Algebra,2,19,False
Vector,3,20,False
Scalar,3,21,False
Matrix,3,21,False
Tensor,3,22,False
Matrix Operations and Manipulations,3,22,False
Addition of Two Matrices,4,23,False
Subtraction of Two Matrices,4,23,False
Product of Two Matrices,4,23,False
Transpose of a Matrix,4,24,False
Dot Product of Two Vectors,4,24,False
Matrix Working on a Vector,4,25,False
Linear Independence of Vectors,3,26,False
Rank of a Matrix,3,27,False
Identity Matrix or Operator,3,28,False
Determinant of a Matrix,3,29,False
Interpretation of Determinant,4,30,False
Inverse of a Matrix,3,31,False
Norm of a Vector,3,32,False
Pseudo Inverse of a Matrix,3,33,False
Unit Vector in the Direction of a Specific Vector,3,34,False
Projection of a Vector in the Direction of Another Vector,3,34,False
Eigen Vectors,3,35,False
Characteristic Equation of a Matrix,4,36,False
Power Iteration Method for Computing Eigen Vector,4,39,False
Calculus,2,40,False
Differentiation,3,40,False
Gradient of a Function,3,41,False
Successive Partial Derivatives,3,42,False
Hessian Matrix of a Function,3,42,False
Maxima and Minima of Functions,3,43,False
Rules for Maxima and Minima for a Univariate Function,4,43,False
Local Minima and Global Minima,3,45,False
Positive Semi-Definite and Positive Definite,3,46,False
Convex Set,3,46,False
Convex Function,3,47,False
Non-convex Function,3,48,False
Multivariate Convex and Non-convex Functions Examples,3,48,False
Taylor Series,3,51,False
Probability,2,51,False
"Unions, Intersection, and Conditional Probability",3,52,False
Chain Rule of Probability for Intersection of Event,3,54,False
Mutually Exclusive Events,3,54,False
Independence of Events,3,54,False
Conditional Independence of Events,3,55,False
Bayes Rule,3,55,False
Probability Mass Function,3,55,False
Probability Density Function,3,56,False
Expectation of a Random Variable,3,56,False
Variance of a Random Variable,3,56,False
Skewness and Kurtosis,3,57,False
Covariance,3,61,False
Correlation Coefficient,3,61,False
Some Common Probability Distribution,3,62,False
Uniform Distribution,4,62,False
Normal Distribution,4,63,False
Multivariate Normal Distribution,4,65,False
Bernoulli Distribution,4,65,False
Binomial Distribution,4,66,False
Poisson Distribution,4,67,False
Likelihood Function,3,68,False
Maximum Likelihood Estimate,3,69,False
Hypothesis Testing and p Value,3,70,False
Formulation of Machine-Learning Algorithm and Optimization Techniques,2,72,False
Supervised Learning,3,73,False
Linear Regression as a Supervised Learning Method,4,73,False
Linear Regression Through Vector Space Approach,4,76,False
Classification,4,78,False
Hyperplanes and Linear Classifiers,4,81,False
Unsupervised Learning,3,82,False
Optimization Techniques for Machine Learning,3,83,False
Gradient Descent,4,83,False
Gradient Descent for a Multivariate Cost Function,4,84,False
Contour Plot and Contour Lines,4,85,False
Steepest Descent,4,87,False
Stochastic Gradient Descent,4,88,False
Newton’s Method,4,91,False
Linear Curve,4,91,False
Negative Curvature,4,92,False
Positive Curvature,4,93,False
Constrained Optimization Problem,3,94,False
A Few Important Topics in Machine Learning,2,96,False
Dimensionality Reduction Methods,3,96,False
Principal Component Analysis,4,97,False
When Will PCA Be Useful in Data Reduction?,5,99,False
How Do You Know How Much Variance Is Retained by the Selected Principal Components?,5,100,False
Singular Value Decomposition,4,100,False
Regularization,3,101,False
Regularization Viewed as a Constraint Optimization Problem,3,103,False
Summary,2,104,False
Chapter 2: Introduction to Deep-Learning Concepts and TensorFlow,1,105,True
Deep Learning and Its Evolution,2,105,False
Perceptrons and Perceptron Learning Algorithm,2,108,False
Geometrical Interpretation of Perceptron Learning,3,112,False
Limitations of Perceptron Learning,3,113,False
Need for Non-linearity,3,115,False
Hidden Layer Perceptrons’ Activation Function for Non-linearity,3,116,False
Different Activation Functions for a Neuron/Perceptron,3,118,False
Linear Activation Function,4,118,False
Binary Threshold Activation Function,4,118,False
Sigmoid Activation Function,4,119,False
SoftMax Activation Function,4,120,False
Rectified Linear Unit(ReLU) Activation Function,4,122,False
Tanh Activation Function,4,123,False
Learning Rule for Multi-Layer Perceptrons Network,3,124,False
Backpropagation for Gradient Computation,3,125,False
Generalizing the Backpropagation Method for Gradient Computation,3,127,False
Deep Learning Versus Traditional Methods,4,132,False
TensorFlow,2,134,False
Common Deep-Learning Packages,3,134,False
TensorFlow Installation,3,135,False
TensorFlow Basics for Development,3,135,False
Gradient-Descent Optimization Methods from a Deep-Learning Perspective,3,139,False
Elliptical Contours,4,139,False
Non-convexity of Cost Functions,4,142,False
Saddle Points in the High-Dimensional Cost Functions,4,143,False
Learning Rate in Mini-batch Approach to Stochastic Gradient Descent,3,145,False
Optimizers in TensorFlow,3,146,False
GradientDescentOptimizer,4,146,False
Usage,5,146,False
AdagradOptimizer,4,146,False
Usage,5,147,False
RMSprop,4,147,False
Usage,5,149,False
AdadeltaOptimizer,4,149,False
Usage,5,150,False
AdamOptimizer,4,151,False
Usage,5,151,False
MomentumOptimizer and Nesterov Algorithm,4,152,False
Usage,5,153,False
"Epoch, Number of Batches, and Batch Size",4,154,False
XOR Implementation Using TensorFlow,3,154,False
TensorFlow Computation Graph for XOR network,4,156,False
Linear Regression in TensorFlow,3,159,False
Multi-class Classification with SoftMax Function Using Full-Batch Gradient Descent,3,162,False
Multi-class Classification with SoftMax Function Using Stochastic Gradient Descent,3,165,False
GPU,2,168,False
Summary,2,168,False
Chapter 3: Convolutional Neural Networks,1,169,True
Convolution Operation,2,169,False
Linear Time Invariant (LTI) / Linear Shift Invariant (LSI) Systems,3,169,False
Convolution for Signals in One Dimension,3,171,False
Analog and Digital Signals,2,174,False
2D and 3D signals,3,176,False
2D Convolution,2,177,False
Two-dimensional Unit Step Function,3,177,False
2D Convolution of a Signal with an LSI System Unit Step Response,3,179,False
2D Convolution of an Image to Different LSI System Responses,3,181,False
Common Image-Processing Filters,2,185,False
Mean Filter,3,185,False
Median Filter,3,187,False
Gaussian Filter,3,189,False
Gradient-based Filters,3,190,False
Sobel Edge-Detection Filter,3,191,False
Identity Transform,3,193,False
Convolution Neural Networks,2,194,False
Components of Convolution Neural Networks,2,195,False
Input Layer,3,196,False
Convolution Layer,3,196,False
TensorFlow Usage,4,197,False
Pooling Layer,3,198,False
TensorFlow Usage,4,198,False
Backpropagation Through the Convolutional Layer,2,198,False
Backpropagation Through the Pooling Layers,2,202,False
Weight Sharing Through Convolution and Its Advantages,2,203,False
Translation Equivariance,2,204,False
Translation Invariance Due to Pooling,2,205,False
Dropout Layers and Regularization,2,206,False
Convolutional Neural Network for Digit Recognition on the MNIST Dataset,2,208,False
Convolutional Neural Network for Solving Real-World Problems,2,212,False
Batch Normalization,2,220,False
Different Architectures in Convolutional Neural Networks,2,222,False
LeNet,3,222,False
AlexNet,3,224,False
VGG16,3,225,False
ResNet,3,226,False
Transfer Learning,2,227,False
Guidelines for Using Transfer Learning,3,228,False
Transfer Learning with Google’s InceptionV3,3,229,False
Transfer Learning with Pre-trained VGG16,3,232,False
Summary,2,237,False
Chapter 4: Natural Language Processing Using Recurrent Neural Networks,1,238,True
Vector Space Model (VSM),2,238,False
Vector Representation of Words,2,242,False
Word2Vec,2,243,False
Continuous Bag of Words (CBOW),3,243,False
Continuous Bag of Words Implementation in TensorFlow,3,246,False
Skip-Gram Model for Word Embedding,3,250,False
Skip-gram Implementation in TensorFlow,3,252,False
Global Co-occurrence Statistics–based Word Vectors,3,255,False
GloVe,3,260,False
Word Analogy with Word Vectors,3,264,False
Introduction to Recurrent Neural Networks,2,267,False
Language Modeling,3,269,False
Predicting the Next Word in a Sentence Through RNN Versus Traditional Methods,3,270,False
Backpropagation Through Time (BPTT) ,3,271,False
Vanishing and Exploding Gradient Problem in RNN,3,274,False
Solution to Vanishing and Exploding Gradients Problem in RNNs,3,275,False
Gradient Clipping,4,276,False
Smart Initialization of the Memory-to-Memory Weight Connection Matrix and ReLU units,4,276,False
Long Short-Term Memory (LSTM) ,3,277,False
LSTM in Reducing Exploding- and Vanishing -Gradient Problems,3,278,False
MNIST Digit Identification in TensorFlow Using Recurrent Neural Networks,3,280,False
Next-Word Prediction and Sentence Completion in TensorFlow Using Recurrent Neural Networks,4,283,False
Gated Recurrent Unit (GRU),3,289,False
Bidirectional RNN,3,291,False
Summary,2,293,False
Chapter 5: Unsupervised Learning with Restricted Boltzmann Machines and Auto-encoders,1,294,True
Boltzmann Distribution,2,294,False
"Bayesian Inference: Likelihood, Priors, and Posterior Probability Distribution",2,296,False
Markov Chain Monte Carlo Methods for Sampling,2,301,False
Metropolis Algorithm,3,304,False
Restricted Boltzmann Machines,2,309,False
Training a Restricted Boltzmann Machine,3,314,False
Gibbs Sampling,3,319,False
Block Gibbs Sampling,3,320,False
Burn-in Period and Generating Samples in Gibbs Sampling,3,321,False
Using Gibbs Sampling in Restricted Boltzmann Machines,3,321,False
Contrastive Divergence,3,323,False
A Restricted Boltzmann Implementation in TensorFlow,3,324,False
Collaborative Filtering Using Restricted Boltzmann Machines,3,328,False
Deep Belief Networks (DBNs),3,332,False
Auto-encoders,2,337,False
Feature Learning Through Auto-encoders for Supervised Learning,3,340,False
Kullback-Leibler (KL) Divergence,3,342,False
Sparse Auto-encoders,4,343,False
Sparse Auto-Encoder Implementation in TensorFlow,3,344,False
Denoising Auto-Encoder,3,348,False
A Denoising Auto-Encoder Implementation in TensorFlow,3,348,False
PCA and ZCA Whitening,2,355,False
Summary,2,358,False
Chapter 6: Advanced Neural Networks,1,359,True
Image Segmentation,2,359,False
Binary Thresholding Method Based on Histogram of Pixel Intensities,3,359,False
Otsu’s Method,3,360,False
Watershed Algorithm for Image Segmentation,3,363,False
Image Segmentation Using K-means Clustering,3,366,False
Semantic Segmentation,3,369,False
Sliding-Window Approach,3,369,False
Fully Convolutional Network (FCN),3,370,False
Fully Convolutional Network with Downsampling and Upsampling,3,372,False
Unpooling,4,373,False
Max Unpooling,4,374,False
Transpose Convolution,4,375,False
U-Net,3,378,False
Semantic Segmentation in TensorFlow with Fully Connected Neural Networks,3,379,False
Image Classification and Localization Network,2,387,False
Object Detection,2,389,False
R-CNN,3,390,False
Fast and Faster R-CNN,3,391,False
Generative Adversarial Networks,2,392,False
Maximin and Minimax Problem,3,393,False
Zero-sum Game ,3,395,False
Minimax and Saddle Points,3,396,False
GAN Cost Function and Training,3,397,False
Vanishing Gradient for the Generator,3,400,False
TensorFlow Implementation of a GAN Network,3,400,False
TensorFlow Models’ Deployment in Production,2,403,False
Summary,2,406,False
Index,1,407,False
"Preface",1,6,False
"Acknowledgments",1,9,False
"Contents",1,11,False
"Author Biography",1,20,False
1 An Introduction to Neural Networks,1,21,True
1.1 Introduction,2,21,False
1.1.1 Humans Versus Computers: Stretching the Limitsof Artificial Intelligence,3,23,False
1.2 The Basic Architecture of Neural Networks,2,24,False
1.2.1 Single Computational Layer: The Perceptron,3,25,False
1.2.1.1 What Objective Function Is the Perceptron Optimizing?,4,28,False
1.2.1.2 Relationship with Support Vector Machines,4,30,False
1.2.1.3 Choice of Activation and Loss Functions,4,31,False
1.2.1.4 Choice and Number of Output Nodes,4,34,False
1.2.1.5 Choice of Loss Function,4,34,False
1.2.1.6 Some Useful Derivatives of Activation Functions,4,36,False
1.2.2 Multilayer Neural Networks,3,37,False
1.2.3 The Multilayer Network as a Computational Graph,3,40,False
1.3 Training a Neural Network with Backpropagation,2,41,False
1.4 Practical Issues in Neural Network Training,2,44,False
1.4.1 The Problem of Overfitting,3,45,False
1.4.1.1 Regularization,4,46,False
1.4.1.2 Neural Architecture and Parameter Sharing,4,47,False
1.4.1.3 Early Stopping,4,47,False
1.4.1.4 Trading Off Breadth for Depth,4,47,False
1.4.1.5 Ensemble Methods,4,48,False
1.4.2 The Vanishing and Exploding Gradient Problems,3,48,False
1.4.3 Difficulties in Convergence,3,49,False
1.4.4 Local and Spurious Optima,3,49,False
1.4.5 Computational Challenges,3,49,False
1.5 The Secrets to the Power of Function Composition,2,50,False
1.5.1 The Importance of Nonlinear Activation,3,52,False
1.5.2 Reducing Parameter Requirements with Depth,3,54,False
1.5.3 Unconventional Neural Architectures,3,55,False
"1.5.3.1 Blurring the Distinctions Between Input, Hidden,and Output Layers",4,55,False
1.5.3.2 Unconventional Operations and Sum-Product Networks,4,56,False
1.6 Common Neural Architectures,2,57,False
1.6.1 Simulating Basic Machine Learning with Shallow Models,3,57,False
1.6.2 Radial Basis Function Networks,3,57,False
1.6.3 Restricted Boltzmann Machines,3,58,False
1.6.4 Recurrent Neural Networks,3,58,False
1.6.5 Convolutional Neural Networks,3,60,False
1.6.6 Hierarchical Feature Engineering and Pretrained Models,3,62,False
1.7 Advanced Topics,2,64,False
1.7.1 Reinforcement Learning,3,64,False
1.7.2 Separating Data Storage and Computations,3,65,False
1.7.3 Generative Adversarial Networks,3,65,False
1.8 Two Notable Benchmarks,2,66,False
1.8.1 The MNIST Database of Handwritten Digits,3,66,False
1.8.2 The ImageNet Database,3,67,False
1.9 Summary,2,68,False
1.10 Bibliographic Notes,2,68,False
1.10.1 Video Lectures,3,70,False
1.10.2 Software Resources,3,70,False
1.11 Exercises,2,71,False
2 Machine Learning with Shallow Neural Networks,1,73,True
2.1 Introduction,2,73,False
2.2 Neural Architectures for Binary Classification Models,2,75,False
2.2.1 Revisiting the Perceptron,3,76,False
2.2.2 Least-Squares Regression,3,78,False
2.2.2.1 Widrow-Hoff Learning,4,79,False
2.2.2.2 Closed Form Solutions,4,81,False
2.2.3 Logistic Regression,3,81,False
2.2.3.1 Alternative Choices of Activation and Loss,4,83,False
2.2.4 Support Vector Machines,3,83,False
2.3 Neural Architectures for Multiclass Models,2,85,False
2.3.1 Multiclass Perceptron,3,85,False
2.3.2 Weston-Watkins SVM,3,87,False
2.3.3 Multinomial Logistic Regression (Softmax Classifier),3,88,False
2.3.4 Hierarchical Softmax for Many Classes,3,89,False
2.4 Backpropagated Saliency for Feature Selection,2,90,False
2.5 Matrix Factorization with Autoencoders,2,90,False
2.5.1 Autoencoder: Basic Principles,3,91,False
2.5.1.1 Autoencoder with a Single Hidden Layer,4,92,False
2.5.1.2 Connections with Singular Value Decomposition,4,94,False
2.5.1.3 Sharing Weights in Encoder and Decoder,4,94,False
2.5.1.4 Other Matrix Factorization Methods,4,96,False
2.5.2 Nonlinear Activations,3,96,False
2.5.3 Deep Autoencoders,3,98,False
2.5.4 Application to Outlier Detection,3,100,False
2.5.5 When the Hidden Layer Is Broader than the Input Layer,3,101,False
2.5.5.1 Sparse Feature Learning,4,101,False
2.5.6 Other Applications,3,102,False
2.5.7 Recommender Systems: Row Index to Row Value Prediction,3,103,False
2.5.8 Discussion,3,106,False
2.6 Word2vec: An Application of Simple Neural Architectures,2,107,False
2.6.1 Neural Embedding with Continuous Bag of Words,3,107,False
2.6.2 Neural Embedding with Skip-Gram Model,3,110,False
2.6.3 Word2vec (SGNS) Is Logistic Matrix Factorization,3,115,False
2.6.4 Vanilla Skip-Gram Is Multinomial Matrix Factorization,3,118,False
2.7 Simple Neural Architectures for Graph Embeddings,2,118,False
2.7.1 Handling Arbitrary Edge Counts,3,120,False
2.7.2 Multinomial Model,3,120,False
2.7.3 Connections with DeepWalk and Node2vec,3,120,False
2.8 Summary,2,121,False
2.9 Bibliographic Notes,2,121,False
2.9.1 Software Resources,3,122,False
2.10 Exercises,2,123,False
3 Training Deep Neural Networks,1,125,True
3.1 Introduction,2,125,False
3.2 Backpropagation: The Gory Details,2,127,False
3.2.1 Backpropagation with the Computational Graph Abstraction,3,127,False
3.2.2 Dynamic Programming to the Rescue,3,131,False
3.2.3 Backpropagation with Post-Activation Variables,3,133,False
3.2.4 Backpropagation with Pre-activation Variables,3,135,False
3.2.5 Examples of Updates for Various Activations,3,137,False
3.2.5.1 The Special Case of Softmax,4,137,False
3.2.6 A Decoupled View of Vector-Centric Backpropagation,3,138,False
3.2.7 Loss Functions on Multiple Output Nodes and Hidden Nodes,3,141,False
3.2.8 Mini-Batch Stochastic Gradient Descent,3,141,False
3.2.9 Backpropagation Tricks for Handling Shared Weights,3,143,False
3.2.10 Checking the Correctness of Gradient Computation,3,144,False
3.3 Setup and Initialization Issues,2,145,False
3.3.1 Tuning Hyperparameters,3,145,False
3.3.2 Feature Preprocessing,3,146,False
3.3.3 Initialization,3,148,False
3.4 The Vanishing and Exploding Gradient Problems,2,149,False
3.4.1 Geometric Understanding of the Effect of Gradient Ratios,3,150,False
3.4.2 A Partial Fix with Activation Function Choice,3,153,False
3.4.3 Dying Neurons and ``Brain Damage'',3,153,False
3.4.3.1 Leaky ReLU,4,153,False
3.4.3.2 Maxout,4,154,False
3.5 Gradient-Descent Strategies,2,154,False
3.5.1 Learning Rate Decay,3,155,False
3.5.2 Momentum-Based Learning,3,156,False
3.5.2.1 Nesterov Momentum,4,157,False
3.5.3 Parameter-Specific Learning Rates,3,157,False
3.5.3.1 AdaGrad,4,158,False
3.5.3.2 RMSProp,4,158,False
3.5.3.3 RMSProp with Nesterov Momentum,4,159,False
3.5.3.4 AdaDelta,4,159,False
3.5.3.5 Adam,4,160,False
3.5.4 Cliffs and Higher-Order Instability,3,161,False
3.5.5 Gradient Clipping,3,162,False
3.5.6 Second-Order Derivatives,3,163,False
3.5.6.1 Conjugate Gradients and Hessian-Free Optimization,4,165,False
3.5.6.2 Quasi-Newton Methods and BFGS,4,168,False
3.5.6.3 Problems with Second-Order Methods: Saddle Points,4,169,False
3.5.7 Polyak Averaging,3,171,False
3.5.8 Local and Spurious Minima,3,171,False
3.6 Batch Normalization,2,172,False
3.7 Practical Tricks for Acceleration and Compression,2,176,False
3.7.1 GPU Acceleration,3,177,False
3.7.2 Parallel and Distributed Implementations,3,178,False
3.7.3 Algorithmic Tricks for Model Compression,3,180,False
3.8 Summary,2,183,False
3.9 Bibliographic Notes,2,183,False
3.9.1 Software Resources,3,185,False
3.10 Exercises,2,185,False
4 Teaching Deep Learners to Generalize,1,188,True
4.1 Introduction,2,188,False
4.2 The Bias-Variance Trade-Off,2,193,False
4.2.1 Formal View,3,194,False
4.3 Generalization Issues in Model Tuning and Evaluation,2,197,False
4.3.1 Evaluating with Hold-Out and Cross-Validation,3,198,False
4.3.2 Issues with Training at Scale,3,199,False
4.3.3 How to Detect Need to Collect More Data,3,200,False
4.4 Penalty-Based Regularization,2,200,False
4.4.1 Connections with Noise Injection,3,201,False
4.4.2 L1-Regularization,3,202,False
4.4.3 L1- or L2-Regularization?,3,203,False
4.4.4 Penalizing Hidden Units: Learning Sparse Representations,3,204,False
4.5 Ensemble Methods,2,205,False
4.5.1 Bagging and Subsampling,3,205,False
4.5.2 Parametric Model Selection and Averaging,3,206,False
4.5.3 Randomized Connection Dropping,3,207,False
4.5.4 Dropout,3,207,False
4.5.5 Data Perturbation Ensembles,3,210,False
4.6 Early Stopping,2,211,False
4.6.1 Understanding Early Stopping from the Variance Perspective,3,211,False
4.7 Unsupervised Pretraining,2,212,False
4.7.1 Variations of Unsupervised Pretraining,3,216,False
4.7.2 What About Supervised Pretraining?,3,216,False
4.8 Continuation and Curriculum Learning,2,218,False
4.8.1 Continuation Learning,3,218,False
4.8.2 Curriculum Learning,3,219,False
4.9 Parameter Sharing,2,219,False
4.10 Regularization in Unsupervised Applications,2,220,False
4.10.1 Value-Based Penalization: Sparse Autoencoders,3,221,False
4.10.2 Noise Injection: De-noising Autoencoders,3,221,False
4.10.3 Gradient-Based Penalization: Contractive Autoencoders,3,223,False
4.10.4 Hidden Probabilistic Structure: Variational Autoencoders,3,226,False
4.10.4.1 Reconstruction and Generative Sampling,4,229,False
4.10.4.2 Conditional Variational Autoencoders,4,231,False
4.10.4.3 Relationship with Generative Adversarial Networks,4,232,False
4.11 Summary,2,232,False
4.12 Bibliographic Notes,2,233,False
4.12.1 Software Resources,3,234,False
4.13 Exercises,2,234,False
5 Radial Basis Function Networks,1,236,True
5.1 Introduction,2,236,False
5.2 Training an RBF Network,2,239,False
5.2.1 Training the Hidden Layer,3,240,False
5.2.2 Training the Output Layer,3,241,False
5.2.2.1 Expression with Pseudo-Inverse,4,243,False
5.2.3 Orthogonal Least-Squares Algorithm,3,243,False
5.2.4 Fully Supervised Learning,3,244,False
5.3 Variations and Special Cases of RBF Networks,2,245,False
5.3.1 Classification with Perceptron Criterion,3,245,False
5.3.2 Classification with Hinge Loss,3,246,False
5.3.3 Example of Linear Separability Promoted by RBF,3,246,False
5.3.4 Application to Interpolation,3,247,False
5.4 Relationship with Kernel Methods,2,248,False
5.4.1 Kernel Regression as a Special Case of RBF Networks,3,248,False
5.4.2 Kernel SVM as a Special Case of RBF Networks,3,249,False
5.4.3 Observations,3,250,False
5.5 Summary,2,250,False
5.6 Bibliographic Notes,2,251,False
5.7 Exercises,2,251,False
6 Restricted Boltzmann Machines,1,253,True
6.1 Introduction,2,253,False
6.1.1 Historical Perspective,3,254,False
6.2 Hopfield Networks,2,255,False
6.2.1 Optimal State Configurations of a Trained Network,3,256,False
6.2.2 Training a Hopfield Network,3,258,False
6.2.3 Building a Toy Recommender and Its Limitations,3,259,False
6.2.4 Increasing the Expressive Power of the Hopfield Network,3,260,False
6.3 The Boltzmann Machine,2,261,False
6.3.1 How a Boltzmann Machine Generates Data,3,262,False
6.3.2 Learning the Weights of a Boltzmann Machine,3,263,False
6.4 Restricted Boltzmann Machines,2,265,False
6.4.1 Training the RBM,3,267,False
6.4.2 Contrastive Divergence Algorithm,3,268,False
6.4.3 Practical Issues and Improvisations,3,269,False
6.5 Applications of Restricted Boltzmann Machines,2,269,False
6.5.1 Dimensionality Reduction and Data Reconstruction,3,270,False
6.5.2 RBMs for Collaborative Filtering,3,272,False
6.5.3 Using RBMs for Classification,3,275,False
6.5.4 Topic Models with RBMs,3,278,False
6.5.5 RBMs for Machine Learning with Multimodal Data,3,280,False
6.6 Using RBMs Beyond Binary Data Types,2,281,False
6.7 Stacking Restricted Boltzmann Machines,2,282,False
6.7.1 Unsupervised Learning,3,284,False
6.7.2 Supervised Learning,3,285,False
6.7.3 Deep Boltzmann Machines and Deep Belief Networks,3,285,False
6.8 Summary,2,286,False
6.9 Bibliographic Notes,2,286,False
6.10 Exercises,2,288,False
7 Recurrent Neural Networks,1,289,True
7.1 Introduction,2,289,False
7.1.1 Expressiveness of Recurrent Networks,3,292,False
7.2 The Architecture of Recurrent Neural Networks,2,292,False
7.2.1 Language Modeling Example of RNN,3,295,False
7.2.1.1 Generating a Language Sample,4,296,False
7.2.2 Backpropagation Through Time,3,298,False
7.2.3 Bidirectional Recurrent Networks,3,301,False
7.2.4 Multilayer Recurrent Networks,3,302,False
7.3 The Challenges of Training Recurrent Networks,2,304,False
7.3.1 Layer Normalization,3,307,False
7.4 Echo-State Networks,2,308,False
7.5 Long Short-Term Memory (LSTM),2,310,False
7.6 Gated Recurrent Units (GRUs),2,313,False
7.7 Applications of Recurrent Neural Networks,2,315,False
7.7.1 Application to Automatic Image Captioning,3,316,False
7.7.2 Sequence-to-Sequence Learning and Machine Translation,3,317,False
7.7.2.1 Question-Answering Systems,4,319,False
7.7.3 Application to Sentence-Level Classification,3,321,False
7.7.4 Token-Level Classification with Linguistic Features,3,322,False
7.7.5 Time-Series Forecasting and Prediction,3,323,False
7.7.6 Temporal Recommender Systems,3,325,False
7.7.7 Secondary Protein Structure Prediction,3,327,False
7.7.8 End-to-End Speech Recognition,3,327,False
7.7.9 Handwriting Recognition,3,327,False
7.8 Summary,2,328,False
7.9 Bibliographic Notes,2,328,False
7.9.1 Software Resources,3,329,False
7.10 Exercises,2,330,False
8 Convolutional Neural Networks,1,332,True
8.1 Introduction,2,332,False
8.1.1 Historical Perspective and Biological Inspiration,3,333,False
8.1.2 Broader Observations About Convolutional Neural Networks,3,334,False
8.2 The Basic Structure of a Convolutional Network,2,335,False
8.2.1 Padding,3,339,False
8.2.2 Strides,3,341,False
8.2.3 Typical Settings,3,341,False
8.2.4 The ReLU Layer,3,342,False
8.2.5 Pooling,3,343,False
8.2.6 Fully Connected Layers,3,344,False
8.2.7 The Interleaving Between Layers,3,345,False
8.2.8 Local Response Normalization,3,347,False
8.2.9 Hierarchical Feature Engineering,3,348,False
8.3 Training a Convolutional Network,2,349,False
8.3.1 Backpropagating Through Convolutions,3,350,False
8.3.2 Backpropagation as Convolution with Inverted/Transposed Filter,3,351,False
8.3.3 Convolution/Backpropagation as Matrix Multiplications,3,352,False
8.3.4 Data Augmentation,3,354,False
8.4 Case Studies of Convolutional Architectures,2,355,False
8.4.1 AlexNet,3,356,False
8.4.2 ZFNet,3,358,False
8.4.3 VGG,3,359,False
8.4.4 GoogLeNet,3,362,False
8.4.5 ResNet,3,364,False
8.4.6 The Effects of Depth,3,367,False
8.4.7 Pretrained Models,3,368,False
8.5 Visualization and Unsupervised Learning,2,369,False
8.5.1 Visualizing the Features of a Trained Network,3,370,False
8.5.2 Convolutional Autoencoders,3,374,False
8.6 Applications of Convolutional Networks,2,380,False
8.6.1 Content-Based Image Retrieval,3,380,False
8.6.2 Object Localization,3,381,False
8.6.3 Object Detection,3,382,False
8.6.4 Natural Language and Sequence Learning,3,383,False
8.6.5 Video Classification,3,384,False
8.7 Summary,2,385,False
8.8 Bibliographic Notes,2,385,False
8.8.1 Software Resources and Data Sets,3,387,False
8.9 Exercises,2,388,False
9 Deep Reinforcement Learning,1,389,True
9.1 Introduction,2,389,False
9.2 Stateless Algorithms: Multi-Armed Bandits,2,391,False
9.2.1 Naïve Algorithm,3,392,False
9.2.2 ε-Greedy Algorithm,3,392,False
9.2.3 Upper Bounding Methods,3,392,False
9.3 The Basic Framework of Reinforcement Learning,2,393,False
9.3.1 Challenges of Reinforcement Learning,3,395,False
9.3.2 Simple Reinforcement Learning for Tic-Tac-Toe,3,396,False
9.3.3 Role of Deep Learning and a Straw-Man Algorithm,3,396,False
9.4 Bootstrapping for Value Function Learning,2,399,False
9.4.1 Deep Learning Models as Function Approximators,3,400,False
9.4.2 Example: Neural Network for Atari Setting,3,402,False
9.4.3 On-Policy Versus Off-Policy Methods: SARSA,3,403,False
9.4.4 Modeling States Versus State-Action Pairs,3,405,False
9.5 Policy Gradient Methods,2,407,False
9.5.1 Finite Difference Methods,3,408,False
9.5.2 Likelihood Ratio Methods,3,409,False
9.5.3 Combining Supervised Learning with Policy Gradients,3,411,False
9.5.4 Actor-Critic Methods,3,411,False
9.5.5 Continuous Action Spaces,3,413,False
9.5.6 Advantages and Disadvantages of Policy Gradients,3,413,False
9.6 Monte Carlo Tree Search,2,414,False
9.7 Case Studies,2,415,False
9.7.1 AlphaGo: Championship Level Play at Go,3,415,False
9.7.1.1 Alpha Zero: Enhancements to Zero Human Knowledge,4,418,False
9.7.2 Self-Learning Robots,3,420,False
9.7.2.1 Deep Learning of Locomotion Skills,4,420,False
9.7.2.2 Deep Learning of Visuomotor Skills,4,422,False
9.7.3 Building Conversational Systems: Deep Learning for Chatbots,3,423,False
9.7.4 Self-Driving Cars,3,426,False
9.7.5 Inferring Neural Architectures with Reinforcement Learning,3,428,False
9.8 Practical Challenges Associated with Safety,2,429,False
9.9 Summary,2,430,False
9.10 Bibliographic Notes,2,430,False
9.10.1 Software Resources and Testbeds,3,432,False
9.11 Exercises,2,432,False
10 Advanced Topics in Deep Learning,1,434,True
10.1 Introduction,2,434,False
10.2 Attention Mechanisms,2,436,False
10.2.1 Recurrent Models of Visual Attention,3,437,False
10.2.1.1 Application to Image Captioning,4,439,False
10.2.2 Attention Mechanisms for Machine Translation,3,440,False
10.3 Neural Networks with External Memory,2,444,False
10.3.1 A Fantasy Video Game: Sorting by Example,3,445,False
10.3.1.1 Implementing Swaps with Memory Operations,4,446,False
10.3.2 Neural Turing Machines,3,447,False
10.3.3 Differentiable Neural Computer: A Brief Overview,3,452,False
10.4 Generative Adversarial Networks (GANs),2,453,False
10.4.1 Training a Generative Adversarial Network,3,454,False
10.4.2 Comparison with Variational Autoencoder,3,457,False
10.4.3 Using GANs for Generating Image Data,3,457,False
10.4.4 Conditional Generative Adversarial Networks,3,459,False
10.5 Competitive Learning,2,464,False
10.5.1 Vector Quantization,3,465,False
10.5.2 Kohonen Self-Organizing Map,3,465,False
10.6 Limitations of Neural Networks,2,468,False
10.6.1 An Aspirational Goal: One-Shot Learning,3,468,False
10.6.2 An Aspirational Goal: Energy-Efficient Learning,3,470,False
10.7 Summary,2,471,False
10.8 Bibliographic Notes,2,472,False
10.8.1 Software Resources,3,473,False
10.9 Exercises,2,473,False
Bibliography,1,474,False
Index,1,508,False
Foreword,1,5,False
Preface,1,7,False
Contents,1,10,False
Contributors,1,11,False
Acronyms,1,12,False
1 A Joint Introduction to Natural Language Processing and to Deep Learning,1,15,True
1.1 Natural Language Processing: The Basics,2,15,False
1.2 The First Wave: Rationalism,2,16,False
1.3 The Second Wave: Empiricism,2,18,False
1.4 The Third Wave: Deep Learning,2,20,False
1.5 Transitions from Now to the Future,2,24,False
1.5.1 From Empiricism to Deep Learning: A Revolution,3,25,False
1.5.2 Limitations of Current Deep Learning Technology,3,25,False
1.6 Future Directions of NLP,2,26,False
1.6.1 Neural-Symbolic Integration,3,26,False
"1.6.2 Structure, Memory, and Knowledge",3,28,False
1.6.3 Unsupervised and Generative Deep Learning,3,28,False
1.6.4 Multimodal and Multitask Deep Learning,3,29,False
1.6.5 Meta-learning,3,30,False
1.7 Summary,2,31,False
References,2,32,False
2 Deep Learning in Conversational Language Understanding,1,37,True
2.1 Introduction,2,37,False
2.2 A Historical Perspective,2,39,False
2.3 Major Language Understanding Tasks,2,40,False
2.3.1 Domain Detection and Intent Determination,3,41,False
2.3.2 Slot Filling,3,41,False
2.4 Elevating State of the Art: From Statistical  Modeling to Deep Learning,2,42,False
2.4.1 Domain Detection and Intent Determination,3,42,False
2.4.2 Slot Filling,3,45,False
2.4.3 Joint Multitask Multi-domain Modeling,3,51,False
2.4.4 Understanding in Context,3,54,False
2.5 Summary,2,57,False
References,2,59,False
3 Deep Learning in Spoken and Text-Based Dialog Systems,1,63,True
3.1 Introduction,2,63,False
3.2 Learning Methodology for Components of a Dialog System,2,66,False
3.2.1 Discriminative Methods,3,66,False
3.2.2 Generative Methods,3,68,False
3.2.3 Decision-Making,3,68,False
3.3 Goal-Oriented Neural Dialog Systems,2,69,False
3.3.1 Neural Language Understanding,3,69,False
3.3.2 Dialog State Tracker,3,70,False
3.3.3 Deep Dialog Manager,3,70,False
3.4 Model-Based User Simulators,2,73,False
3.5 Natural Language Generation,2,74,False
3.6 End-to-End Deep Learning Approaches to Building Dialog Systems,2,77,False
3.7 Deep Learning for Open Dialog Systems,2,78,False
3.8 Datasets for Dialog Modeling,2,79,False
3.8.1 The Carnegie Mellon Communicator Corpus,3,79,False
3.8.2 ATIS—Air Travel Information System Pilot Corpus,3,79,False
3.8.3 Dialog State Tracking Challenge Dataset,3,80,False
3.8.4 Maluuba Frames Dataset,3,80,False
3.8.5 Facebook's Dialog Datasets,3,81,False
3.8.6 Ubuntu Dialog Corpus,3,81,False
3.9 Open Source Dialog Software,2,81,False
3.10 Dialog System Evaluation,2,84,False
3.11 Summary,2,86,False
References,2,87,False
4 Deep Learning in Lexical Analysis  and Parsing,1,93,True
4.1 Background,2,93,False
4.2 Typical Lexical Analysis and Parsing Tasks,2,94,False
4.2.1 Word Segmentation,3,94,False
4.2.2 POS Tagging,3,95,False
4.2.3 Syntactic Parsing,3,95,False
4.2.4 Structured Predication ,3,98,False
4.3 Structured Prediction Methods,2,100,False
4.3.1 Graph-Based Methods,3,100,False
4.3.2 Transition-Based Methods,3,102,False
4.4 Neural Graph-Based Methods,2,107,False
4.4.1 Neural Conditional Random Fields,3,107,False
4.4.2 Neural Graph-Based Dependency Parsing,3,108,False
4.5 Neural Transition-Based Methods,2,111,False
4.5.1 Greedy Shift-Reduce Dependency Parsing,3,111,False
4.5.2 Greedy Sequence Labeling,3,114,False
4.5.3 Globally Optimized Models,3,117,False
4.6 Summary,2,124,False
References,2,125,False
5 Deep Learning in Knowledge Graph,1,131,True
5.1 Introduction,2,131,False
5.1.1 Basic Concepts,3,132,False
5.1.2 Typical Knowledge Graphs,3,132,False
5.2 Knowledge Representation Learning,2,137,False
5.3 Neural Relation Extraction,2,138,False
5.3.1 Sentence-Level NRE,3,139,False
5.3.2 Document-Level NRE,3,144,False
5.4 Bridging Knowledge with Text: Entity Linking,2,146,False
5.4.1 The Entity Linking Framework,3,147,False
5.4.2 Deep Learning for Entity Linking,3,149,False
5.5 Summary,2,156,False
References,2,157,False
6 Deep Learning in Machine Translation,1,160,True
6.1 Introduction,2,160,False
6.2 Statistical Machine Translation and Its Challenges,2,161,False
6.2.1 Basics,3,161,False
6.2.2 Challenges in Statistical Machine Translation,3,164,False
6.3 Component-Wise Deep Learning for Machine Translation,2,165,False
6.3.1 Deep Learning for Word Alignment,3,165,False
6.3.2 Deep Learning for Translation Rule Probability Estimation,3,168,False
6.3.3 Deep Learning for Reordering Phrases,3,172,False
6.3.4 Deep Learning for Language Modeling,3,174,False
6.3.5 Deep Learning for Feature Combination,3,175,False
6.4 End-to-End Deep Learning for Machine Translation,2,177,False
6.4.1 The Encoder–Decoder Framework,3,177,False
6.4.2 Neural Attention in Machine Translation,3,180,False
6.4.3 Addressing Technical Challenges of Large Vocabulary,3,181,False
6.4.4 End-to-End Training to Optimize Evaluation Metric Directly,3,183,False
6.4.5 Incorporating Prior Knowledge,3,185,False
6.4.6 Low-Resource Language Translation,3,187,False
6.4.7 Network Structures in Neural Machine Translation,3,190,False
6.4.8 Combination of SMT and NMT,3,191,False
6.5 Summary,2,192,False
References,2,193,False
7 Deep Learning in Question Answering,1,197,True
7.1 Introduction,2,197,False
7.2 Deep Learning in Question Answering  over Knowledge Base,2,198,False
7.2.1 The Information Extraction Style,3,199,False
7.2.2 The Semantic Parsing Style,3,203,False
7.2.3 The Information Extraction Style Versus the Semantic Parsing Style,3,207,False
7.2.4 Datasets,3,208,False
7.2.5 Challenges,3,209,False
7.3 Deep Learning in Machine Comprehension,2,210,False
7.3.1 Task Description,3,210,False
7.3.2 Feature Engineering-Based Methods in Machine Comprehension,3,214,False
7.3.3 Deep Learning Methods in Machine Comprehension,3,219,False
7.4 Summary,2,224,False
References,2,227,False
8 Deep Learning in Sentiment Analysis,1,230,True
8.1 Introduction,2,230,False
8.2 Sentiment-Specific Word Embedding,2,232,False
8.3 Sentence-Level Sentiment Classification,2,236,False
8.3.1 Convolutional Neural Networks,3,237,False
8.3.2 Recurrent Neural Networks,3,240,False
8.3.3 Recursive Neural Networks,3,242,False
8.3.4 Integration of External Resources,3,245,False
8.4 Document-Level Sentiment Classification,2,246,False
8.5 Fine-Grained Sentiment Analysis,2,249,False
8.5.1 Opinion Mining,3,249,False
8.5.2 Targeted Sentiment Analysis,3,251,False
8.5.3 Aspect-Level Sentiment Analysis,3,254,False
8.5.4 Stance Detection,3,255,False
8.5.5 Sarcasm Recognition,3,258,False
8.6 Summary,2,259,False
References,2,259,False
9 Deep Learning in Social Computing,1,265,True
9.1 Introduction to Social Computing,2,265,False
9.2 Modeling User-Generated Content with Deep Learning,2,268,False
9.2.1 Traditional Semantic Representation Approaches,3,269,False
9.2.2 Semantic Representation with Shallow Embedding,3,270,False
9.2.3 Semantic Representation with Deep Neural Networks,3,272,False
9.2.4 Enhancing Semantic Representation with Attention Mechanism,3,276,False
9.3 Modeling Social Connections with Deep Learning,2,278,False
9.3.1 Social Connections on Social Media,3,278,False
9.3.2 A Network Representation Learning Approach to Modeling Social Connections ,3,278,False
9.3.3 Shallow Embedding Based Models,3,280,False
9.3.4 Deep Neural Network Based Models,3,283,False
9.3.5 Applications of Network Embedding,3,285,False
9.4 Recommendation with Deep Learning,2,285,False
9.4.1 Recommendation on Social Media,3,285,False
9.4.2 Traditional Recommendation Algorithms,3,286,False
9.4.3 Shallow Embedding Based Models,3,287,False
9.4.4 Deep Neural Network Based Models,3,289,False
9.5 Summary,2,294,False
References,2,294,False
10 Deep Learning in Natural Language Generation from Images,1,299,True
10.1 Introduction,2,299,False
10.2 Background,2,300,False
10.3 Deep Learning Frameworks to Generate Natural Language from an Image,2,301,False
10.3.1 The End-to-End Framework,3,301,False
10.3.2 The compositional framework,3,304,False
10.3.3 Other Frameworks,3,306,False
10.4 Evaluation Metrics and Benchmarks,2,306,False
10.5 Industrial Deployment of Image Captioning,2,307,False
10.6 Examples: Natural Language Descriptions of Images,2,308,False
10.7 Recent Research on Generating Stylistic Natural Language from Images,2,308,False
10.8 Summary,2,314,False
References,2,314,False
11 Epilogue: Frontiers of NLP in the Deep Learning Era,1,318,True
11.1 Introduction,2,318,False
11.2 Two New Perspectives,2,319,False
11.2.1 The Task-Centric Perspective,3,320,False
11.2.2 The Representation-Centric Perspective,3,321,False
11.3 Major Recent Advances in Deep Learning for NLP and Research Frontiers,2,323,False
11.3.1 Compositionality for Generalization,3,323,False
11.3.2 Unsupervised Learning for NLP,3,324,False
11.3.3 Reinforcement Learning for NLP,3,325,False
11.3.4 Meta-Learning for NLP,3,326,False
11.3.5 Interpretability: Weak-Sense and Strong-Sense,3,328,False
11.4 Summary,2,331,False
References,2,333,False
Appendix  Glossary,1,336,False
Table of Contents,1,5,False
About the Authors,1,12,False
About the Guest Authors of Chapter 7,1,14,False
About the Technical Reviewers,1,15,False
Acknowledgments,1,16,False
Foreword,1,17,False
Introduction,1,20,False
Part I: Getting Started with AI,1,23,False
Chapter 2: Overview of Deep Learning,2,48,True
Common Network Structures,3,49,False
Convolutional Neural Networks,4,50,False
Recurrent Neural Networks,4,54,False
Generative Adversarial Networks,4,56,False
Autoencoders,4,57,False
Deep Learning Workflow,3,58,False
Finding Relevant Data Set(s),4,59,False
Data Set Preprocessing,4,60,False
Training the Model,4,61,False
Validating and Tuning the Model,4,61,False
Deploy the Model,4,63,False
Deep Learning Frameworks & Compute,4,64,False
Jump Start Deep Learning: Transfer Learning and Domain Adaptation,4,68,False
Models Library,4,71,False
Summary,3,72,False
Chapter 3: Trends in Deep Learning,2,73,True
Variations on Network Architectures,3,73,False
Residual Networks and Variants,4,74,False
DenseNet,4,74,False
"Small Models, Fewer Parameters",4,75,False
Capsule Networks,4,76,False
Object Detection,4,78,False
Object Segmentation,4,80,False
More Sophisticated Networks,4,80,False
Automated Machine Learning,4,81,False
Hardware,3,83,False
More Specialized Hardware,4,84,False
Hardware on Azure,4,85,False
Quantum Computing,4,85,False
Limitations of Deep Learning,3,87,False
Be Wary of Hype,4,87,False
Limits on Ability to Generalize,4,88,False
"Data Hungry Models, Especially Labels",4,90,False
Reproducible Research and Underlying Theory,4,90,False
Looking Ahead: What Can We Expect from Deep Learning?,3,92,False
Ethics and Regulations,4,93,False
Summary,3,95,False
Chapter 1: Introduction to Artificial Intelligence,2,24,True
Microsoft and AI,3,27,False
Machine Learning,3,30,False
Deep Learning,3,35,False
Rise of Deep Learning,4,37,False
Applications of Deep Learning,4,42,False
Summary,3,46,False
Part II: Azure AI Platform and Experimentation Tools,1,96,False
Chapter 4: Microsoft AI Platform,2,97,True
Services,3,99,False
Prebuilt AI: Cognitive Services,4,100,False
Conversational AI: Bot Framework,4,102,False
Custom AI: Azure Machine Learning Services,4,102,False
Custom AI: Batch AI,4,103,False
Infrastructure,3,104,False
Data Science Virtual Machine,4,105,False
Spark,4,106,False
Container Hosting,4,107,False
Data Storage,4,109,False
Tools,3,110,False
Azure Machine Learning Studio,4,110,False
Integrated Development Environments,4,111,False
Deep Learning Frameworks,4,111,False
Broader Azure Platform,3,112,False
Getting Started with the Deep Learning Virtual Machine,3,113,False
Running the Notebook Server,4,115,False
Summary,3,116,False
Chapter 5: Cognitive Services and Custom Vision,2,117,True
Prebuilt AI: Why and How?,3,117,False
Cognitive Services,3,119,False
What Types of Cognitive Services Are Available?,3,122,False
Computer Vision APIs,4,124,False
How to Use Optical Character Recognition–,5,128,False
How to Recognize Celebrities and Landmarks,5,129,False
How Do I Get Started with Cognitive Services?,3,131,False
Custom Vision,3,137,False
Hello World! for Custom Vision,4,138,False
Exporting Custom Vision Models,4,145,False
Summary,3,146,False
Part III: AI Networks in Practice,1,147,False
Chapter 6: Convolutional Neural Networks,2,148,True
The Convolution in Convolution Neural Networks,3,149,False
Convolution Layer,4,151,False
Pooling Layer,4,152,False
Activation Functions,4,153,False
Sigmoid,5,153,False
Tanh,5,154,False
Rectified Linear Unit,5,155,False
CNN Architecture,3,156,False
Training Classification CNN,3,157,False
Why CNNs,3,159,False
Training CNN on CIFAR10,3,160,False
Training a Deep CNN on GPU,3,167,False
Model 1,4,168,False
Model 2,4,169,False
Model 3,4,171,False
Model 4,4,173,False
Transfer Learning,3,176,False
Summary,3,177,False
Chapter 7: Recurrent Neural Networks,2,178,True
RNN Architectures,3,181,False
Training RNNs,3,186,False
Gated RNNs,3,187,False
Sequence-to-Sequence Models and Attention Mechanism,3,189,False
RNN Examples,3,193,False
Example 1: Sentiment Analysis,4,193,False
Example 2: Image Classification,4,193,False
Example 3: Time Series,4,197,False
Summary,3,203,False
Chapter 8: Generative Adversarial Networks,2,204,True
What Are Generative Adversarial Networks?,3,205,False
Cycle-Consistent Adversarial Networks,3,211,False
The CycleGAN Code,4,213,False
Network Architecture for the Generator and Discriminator,4,217,False
Defining the CycleGAN Class,4,221,False
Adversarial and Cyclic Loss,4,223,False
Results,3,224,False
Summary,3,225,False
Part IV: AI Architectures and Best Practices,1,226,False
Chapter 9: Training AI Models,2,227,True
Training Options,3,227,False
Distributed Training,4,228,False
Deep Learning Virtual Machine,4,229,False
Batch Shipyard,4,231,False
Batch AI,4,232,False
Deep Learning Workspace,4,233,False
Examples to Follow Along,3,234,False
Training DNN on Batch Shipyard,4,234,False
Hyperparameter Tuning,5,239,False
Distributed Training,5,241,False
Training CNNs on Batch AI,5,242,False
Hyperparameter Tuning and Distributed Training,5,248,False
Variation of Batch AI with Python SDK,5,249,False
Azure Machine Learning Services,4,255,False
Other Options for AI Training on Azure,4,256,False
Summary,3,257,False
Chapter 10: Operationalizing AI Models,2,258,True
Operationalization Platforms,3,258,False
DLVM,4,260,False
Azure Container Instances,4,260,False
Azure Web Apps,4,262,False
Azure Kubernetes Services,4,262,False
Azure Service Fabric,4,265,False
Batch AI,4,266,False
AZTK,4,267,False
HDInsight and Databricks,4,269,False
SQL Server,4,270,False
Operationalization Overview,3,270,False
Azure Machine Learning Services,3,273,False
Summary,3,274,False
Appendix: Notes,1,275,False
Chapter 1,2,275,False
Chapter 2,2,278,False
Chapter 3,2,279,False
Chapter 4,2,284,False
Chapter 5,2,284,False
Chapter 6,2,284,False
Chapter 7,2,286,False
Chapter 8,2,288,False
Chapter 9,2,289,False
Chapter 10,2,290,False
Index,1,291,False
Preface,1,6,False
References,2,10,False
Contents,1,11,False
1 From Logic to Cognitive Science,1,14,True
1.1 The Beginnings of Artificial Neural Networks,2,14,False
1.2 The XOR Problem,2,18,False
1.3 From Cognitive Science to Deep Learning,2,21,False
1.4 Neural Networks in the General AI Landscape,2,24,False
1.5 Philosophical and Cognitive Aspects,2,25,False
2 Mathematical and Computational Prerequisites,1,30,True
2.1 Derivations and Function Minimization,2,30,False
"2.2 Vectors, Matrices and Linear Programming",2,38,False
2.3 Probability Distributions,2,45,False
2.4 Logic and Turing Machines,2,52,False
2.5 Writing Python Code,2,54,False
2.6 A Brief Overview of Python Programming,2,55,False
3 Machine Learning Basics,1,63,True
3.1 Elementary Classification Problem,2,63,False
3.2 Evaluating Classification Results,2,69,False
3.3 A Simple Classifier: Naive Bayes,2,71,False
3.4 A Simple Neural Network: Logistic Regression,2,73,False
3.5 Introducing the MNIST Dataset,2,80,False
3.6 Learning Without Labels: K-Means,2,82,False
3.7 Learning Different Representations: PCA,2,84,False
3.8 Learning Language: The Bag of Words Representation,2,86,False
4 Feedforward Neural Networks,1,90,True
4.1 Basic Concepts and Terminology for Neural Networks,2,90,False
4.2 Representing Network Components with Vectors  and Matrices,2,93,False
4.3 The Perceptron Rule,2,94,False
4.4 The Delta Rule,2,98,False
4.5 From the Logistic Neuron to Backpropagation,2,100,False
4.6 Backpropagation,2,104,False
4.7 A Complete Feedforward Neural Network,2,113,False
5 Modifications and Extensions  to a Feed-Forward Neural Network,1,117,True
5.1 The Idea of Regularization,2,117,False
5.2 L1 and L2 Regularization,2,119,False
"5.3 Learning Rate, Momentum and Dropout",2,121,False
5.4 Stochastic Gradient Descent and Online Learning,2,126,False
5.5 Problems for Multiple Hidden Layers: Vanishing  and Exploding Gradients,2,127,False
6 Convolutional Neural Networks,1,131,True
6.1 A Third Visit to Logistic Regression,2,131,False
6.2 Feature Maps and Pooling,2,134,False
6.3 A Complete Convolutional Network,2,137,False
6.4 Using a Convolutional Network to Classify Text,2,140,False
7 Recurrent Neural Networks,1,144,True
7.1 Sequences of Unequal Length,2,144,False
7.2 The Three Settings of Learning with Recurrent Neural Networks,2,145,False
7.3 Adding Feedback Loops and Unfolding a Neural Network,2,147,False
7.4 Elman Networks,2,149,False
7.5 Long Short-Term Memory,2,151,False
7.6 Using a Recurrent Neural Network for Predicting Following Words,2,154,False
8 Autoencoders,1,162,True
8.1 Learning Representations,2,162,False
8.2 Different Autoencoder Architectures,2,165,False
8.3 Stacking Autoencoders,2,167,False
8.4 Recreating the Cat Paper,2,170,False
9 Neural Language Models,1,173,True
9.1 Word Embeddings and Word Analogies,2,173,False
9.2 CBOW and Word2vec,2,174,False
9.3 Word2vec in Code,2,175,False
9.4 Walking Through the Word-Space: An Idea That Has Eluded Symbolic AI,2,178,False
10 An Overview of Different Neural Network Architectures,1,182,True
10.1 Energy-Based Models,2,182,False
10.2 Memory-Based Models,2,184,False
10.3 The Kernel of General Connectionist Intelligence: The bAbI Dataset,2,188,False
11 Conclusion,1,191,True
11.1 An Incomplete Overview of Open Research Questions,2,191,False
11.2 The Spirit of Connectionism and Philosophical Ties,2,192,False
 Index,1,194,False
Table of Contents,1,5,False
About the Authors,1,12,False
About the Technical Reviewer,1,13,False
Acknowledgments,1,15,False
Introduction,1,16,False
Part I: Background and Fundamentals,1,19,False
Chapter 1: Introduction,2,20,True
1.1 Scope and Motivation,3,21,False
1.2 Challenges in the Deep Learning Field,3,23,False
1.3 Target Audience,3,23,False
1.4 Plan and Organization,3,24,False
Chapter 2: Deep Learning: An Overview,2,25,True
2.1 From a Long Winter to a Blossoming Spring,3,27,False
2.2 Why Is DL Different?,3,30,False
2.2.1 The Age of the Machines,4,33,False
2.2.2 Some Criticism of DL,4,34,False
2.3 Resources,3,35,False
2.3.1 Books,4,35,False
2.3.2 Newsletters,4,36,False
2.3.3 Blogs,4,36,False
2.3.4 Online Videos and Courses,4,37,False
2.3.5 Podcasts,4,38,False
2.3.6 Other Web Resources,4,39,False
2.3.7 Some Nice Places to Start Playing,4,40,False
2.3.8 Conferences,4,41,False
2.3.9 Other Resources,4,42,False
2.3.10 DL Frameworks,4,42,False
2.3.11 DL As a Service,4,45,False
2.4 Recent Developments,3,48,False
2.4.1 2016,4,48,False
2.4.2 2017,4,49,False
2.4.3 Evolution Algorithms,4,50,False
2.4.4 Creativity,4,51,False
Chapter 3: Deep Neural Network Models,2,52,True
3.1 A Brief History of Neural Networks,3,53,False
3.1.1 The Multilayer Perceptron,4,55,False
3.2 What Are Deep Neural Networks?,3,57,False
3.3 Boltzmann Machines,3,60,False
3.3.1 Restricted Boltzmann Machines,4,63,False
Contrastive Divergence,5,64,False
3.3.2 Deep Belief Nets,4,65,False
3.3.3 Deep Boltzmann Machines,4,68,False
3.4 Convolutional Neural Networks,3,69,False
3.5 Deep Auto-encoders,3,70,False
3.6 Recurrent Neural Networks,3,71,False
3.6.1 RNNs for Reinforcement Learning,4,74,False
3.6.2 LSTMs,4,76,False
3.7 Generative Models,3,79,False
3.7.1 Variational Auto-encoders,4,80,False
3.7.2 Generative Adversarial Networks,4,84,False
Part II: Deep Learning: Core Applications,1,89,False
Chapter 4: Image Processing,2,90,True
4.1 CNN Models for Image Processing,3,91,False
4.2 ImageNet and Beyond,3,94,False
4.3 Image Segmentation,3,99,False
4.4 Image Captioning,3,102,False
4.5 Visual Q&A (VQA),3,103,False
4.6 Video Analysis,3,107,False
4.7 GANs and Generative Models,3,111,False
4.8 Other Applications,3,115,False
4.8.1 Satellite Images,4,116,False
4.9 News and Companies,3,118,False
4.10 Third-Party Tools and APIs,3,121,False
Chapter 5: Natural Language Processing and Speech,2,123,True
5.1 Parsing,3,125,False
5.2 Distributed Representations,3,126,False
5.3 Knowledge Representation and Graphs,3,128,False
5.4 Natural Language Translation,3,135,False
5.5 Other Applications,3,139,False
5.6 Multimodal Learning and Q&A,3,141,False
5.7 Speech Recognition,3,142,False
5.8 News and Resources,3,145,False
5.9 Summary and a Speculative Outlook,3,148,False
Chapter 6: Reinforcement Learning and Robotics,2,149,True
6.1 What Is Reinforcement Learning?,3,150,False
6.2 Traditional RL,3,152,False
6.3 DNN for Reinforcement Learning,3,154,False
6.3.1 Deterministic Policy Gradient,4,155,False
6.3.2 Deep Deterministic Policy Gradient,4,155,False
6.3.3 Deep Q-learning,4,156,False
6.3.4 Actor-Critic Algorithm,4,159,False
6.4 Robotics and Control,3,162,False
6.5 Self-Driving Cars,3,165,False
6.6 Conversational Bots (Chatbots),3,167,False
6.7 News Chatbots,3,171,False
6.8 Applications,3,173,False
6.9 Outlook and Future Perspectives,3,174,False
6.10 News About Self-Driving Cars,3,176,False
Part III: Deep Learning: Business Applications,1,181,False
Chapter 7: Recommendation Algorithms and E-commerce,2,182,True
7.1 Online User Behavior,3,183,False
7.2 Retargeting,3,184,False
7.3 Recommendation Algorithms,3,186,False
7.3.1 Collaborative Filters,4,187,False
7.3.2 Deep Learning Approaches to RSs,4,189,False
7.3.3 Item2Vec,4,191,False
7.4 Applications of Recommendation Algorithms,3,192,False
7.5 Future Directions,3,193,False
Chapter 8: Games and Art,2,196,True
8.1 The Early Steps in Chess,3,196,False
8.2 From Chess to Go,3,197,False
8.3 Other Games and News,3,199,False
8.3.1 Doom,4,199,False
8.3.2 Dota,4,199,False
8.3.3 Other Applications,4,200,False
8.4 Artificial Characters,3,202,False
8.5 Applications in Art,3,203,False
8.6 Music,3,206,False
8.7 Multimodal Learning,3,208,False
8.8 Other Applications,3,209,False
Chapter 9: Other Applications,2,217,True
9.1 Anomaly Detection and Fraud,3,218,False
9.1.1 Fraud Prevention,4,221,False
9.1.2 Fraud in Online Reviews,4,223,False
9.2 Security and Prevention,3,224,False
9.3 Forecasting,3,226,False
9.3.1 Trading and Hedge Funds,4,228,False
9.4 Medicine and Biomedical,3,231,False
9.4.1 Image Processing Medical Images,4,232,False
9.4.2 Omics,4,235,False
9.4.3 Drug Discovery,4,238,False
9.5 Other Applications,3,240,False
9.5.1 User Experience,4,240,False
9.5.2 Big Data,4,241,False
9.6 The Future,3,242,False
Part IV: Opportunities and Perspectives,1,244,False
Chapter 10: Business Impact of DL Technology,2,245,True
10.1 Deep Learning Opportunity,3,247,False
10.2 Computer Vision,3,248,False
10.3 AI Assistants,3,249,False
10.4 Legal,3,251,False
10.5 Radiology and Medical Imagery,3,252,False
10.6 Self-Driving Cars,3,254,False
10.7 Data Centers,3,255,False
10.8 Building a Competitive Advantage with DL,3,255,False
10.9 Talent,3,257,False
10.10 It’s Not Only About Accuracy,3,259,False
10.11 Risks,3,260,False
10.12 When Personal Assistants Become Better Than Us,3,261,False
Chapter 11: New Research and Future Directions,2,263,True
11.1 Research,3,264,False
11.1.1 Attention,4,265,False
11.1.2 Multimodal Learning,4,266,False
11.1.3 One-Shot Learning,4,267,False
11.1.4 Reinforcement Learning and Reasoning,4,269,False
11.1.5 Generative Neural Networks,4,271,False
11.1.6 Generative Adversarial Neural Networks,4,272,False
11.1.7 Knowledge Transfer and Learning How to Learn,4,274,False
11.2 When Not to Use Deep Learning,3,276,False
11.3 News,3,277,False
11.4 Ethics and Implications of AI in Society,3,279,False
11.5 Privacy and Public Policy in AI,3,282,False
11.6 Startups and VC Investment,3,284,False
11.7 The Future,3,287,False
11.7.1 Learning with Less Data,4,289,False
11.7.2 Transfer Learning,4,290,False
11.7.3 Multitask Learning,4,290,False
11.7.4 Adversarial Learning,4,291,False
11.7.5 Few-Shot Learning,4,291,False
11.7.6 Metalearning,4,292,False
11.7.7 Neural Reasoning,4,292,False
"Appendix A:Training DNN with  Keras",1,294,True
A.1 The Keras Framework,2,294,False
A.1.1 Installing Keras in Linux,3,295,False
A.1.2 Model,3,295,False
A.1.3 The Core Layers,3,296,False
A.1.4 The Loss Function,3,298,False
A.1.5 Training and Testing,3,298,False
A.1.6 Callbacks,3,299,False
A.1.7 Compile and Fit,3,299,False
A.2 The Deep and Wide Model,2,300,False
A.3 An FCN for Image Segmentation,2,310,False
A.3.1 Sequence to Sequence,3,314,False
A.4 The Backpropagation on a Multilayer Perceptron,2,317,False
References,1,325,False
Index,1,338,False
"Preface",1,6,False
"Acknowledgments",1,9,False
"Contents",1,11,False
"Author Biography",1,20,False
1 An Introduction to Neural Networks,1,21,True
1.1 Introduction,2,21,False
1.1.1 Humans Versus Computers: Stretching the Limitsof Artificial Intelligence,3,23,False
1.2 The Basic Architecture of Neural Networks,2,24,False
1.2.1 Single Computational Layer: The Perceptron,3,25,False
1.2.1.1 What Objective Function Is the Perceptron Optimizing?,4,28,False
1.2.1.2 Relationship with Support Vector Machines,4,30,False
1.2.1.3 Choice of Activation and Loss Functions,4,31,False
1.2.1.4 Choice and Number of Output Nodes,4,34,False
1.2.1.5 Choice of Loss Function,4,34,False
1.2.1.6 Some Useful Derivatives of Activation Functions,4,36,False
1.2.2 Multilayer Neural Networks,3,37,False
1.2.3 The Multilayer Network as a Computational Graph,3,40,False
1.3 Training a Neural Network with Backpropagation,2,41,False
1.4 Practical Issues in Neural Network Training,2,44,False
1.4.1 The Problem of Overfitting,3,45,False
1.4.1.1 Regularization,4,46,False
1.4.1.2 Neural Architecture and Parameter Sharing,4,47,False
1.4.1.3 Early Stopping,4,47,False
1.4.1.4 Trading Off Breadth for Depth,4,47,False
1.4.1.5 Ensemble Methods,4,48,False
1.4.2 The Vanishing and Exploding Gradient Problems,3,48,False
1.4.3 Difficulties in Convergence,3,49,False
1.4.4 Local and Spurious Optima,3,49,False
1.4.5 Computational Challenges,3,49,False
1.5 The Secrets to the Power of Function Composition,2,50,False
1.5.1 The Importance of Nonlinear Activation,3,52,False
1.5.2 Reducing Parameter Requirements with Depth,3,54,False
1.5.3 Unconventional Neural Architectures,3,55,False
"1.5.3.1 Blurring the Distinctions Between Input, Hidden,and Output Layers",4,55,False
1.5.3.2 Unconventional Operations and Sum-Product Networks,4,56,False
1.6 Common Neural Architectures,2,57,False
1.6.1 Simulating Basic Machine Learning with Shallow Models,3,57,False
1.6.2 Radial Basis Function Networks,3,57,False
1.6.3 Restricted Boltzmann Machines,3,58,False
1.6.4 Recurrent Neural Networks,3,58,False
1.6.5 Convolutional Neural Networks,3,60,False
1.6.6 Hierarchical Feature Engineering and Pretrained Models,3,62,False
1.7 Advanced Topics,2,64,False
1.7.1 Reinforcement Learning,3,64,False
1.7.2 Separating Data Storage and Computations,3,65,False
1.7.3 Generative Adversarial Networks,3,65,False
1.8 Two Notable Benchmarks,2,66,False
1.8.1 The MNIST Database of Handwritten Digits,3,66,False
1.8.2 The ImageNet Database,3,67,False
1.9 Summary,2,68,False
1.10 Bibliographic Notes,2,68,False
1.10.1 Video Lectures,3,70,False
1.10.2 Software Resources,3,70,False
1.11 Exercises,2,71,False
2 Machine Learning with Shallow Neural Networks,1,73,True
2.1 Introduction,2,73,False
2.2 Neural Architectures for Binary Classification Models,2,75,False
2.2.1 Revisiting the Perceptron,3,76,False
2.2.2 Least-Squares Regression,3,78,False
2.2.2.1 Widrow-Hoff Learning,4,79,False
2.2.2.2 Closed Form Solutions,4,81,False
2.2.3 Logistic Regression,3,81,False
2.2.3.1 Alternative Choices of Activation and Loss,4,83,False
2.2.4 Support Vector Machines,3,83,False
2.3 Neural Architectures for Multiclass Models,2,85,False
2.3.1 Multiclass Perceptron,3,85,False
2.3.2 Weston-Watkins SVM,3,87,False
2.3.3 Multinomial Logistic Regression (Softmax Classifier),3,88,False
2.3.4 Hierarchical Softmax for Many Classes,3,89,False
2.4 Backpropagated Saliency for Feature Selection,2,90,False
2.5 Matrix Factorization with Autoencoders,2,90,False
2.5.1 Autoencoder: Basic Principles,3,91,False
2.5.1.1 Autoencoder with a Single Hidden Layer,4,92,False
2.5.1.2 Connections with Singular Value Decomposition,4,94,False
2.5.1.3 Sharing Weights in Encoder and Decoder,4,94,False
2.5.1.4 Other Matrix Factorization Methods,4,96,False
2.5.2 Nonlinear Activations,3,96,False
2.5.3 Deep Autoencoders,3,98,False
2.5.4 Application to Outlier Detection,3,100,False
2.5.5 When the Hidden Layer Is Broader than the Input Layer,3,101,False
2.5.5.1 Sparse Feature Learning,4,101,False
2.5.6 Other Applications,3,102,False
2.5.7 Recommender Systems: Row Index to Row Value Prediction,3,103,False
2.5.8 Discussion,3,106,False
2.6 Word2vec: An Application of Simple Neural Architectures,2,107,False
2.6.1 Neural Embedding with Continuous Bag of Words,3,107,False
2.6.2 Neural Embedding with Skip-Gram Model,3,110,False
2.6.3 Word2vec (SGNS) Is Logistic Matrix Factorization,3,115,False
2.6.4 Vanilla Skip-Gram Is Multinomial Matrix Factorization,3,118,False
2.7 Simple Neural Architectures for Graph Embeddings,2,118,False
2.7.1 Handling Arbitrary Edge Counts,3,120,False
2.7.2 Multinomial Model,3,120,False
2.7.3 Connections with DeepWalk and Node2vec,3,120,False
2.8 Summary,2,121,False
2.9 Bibliographic Notes,2,121,False
2.9.1 Software Resources,3,122,False
2.10 Exercises,2,123,False
3 Training Deep Neural Networks,1,125,True
3.1 Introduction,2,125,False
3.2 Backpropagation: The Gory Details,2,127,False
3.2.1 Backpropagation with the Computational Graph Abstraction,3,127,False
3.2.2 Dynamic Programming to the Rescue,3,131,False
3.2.3 Backpropagation with Post-Activation Variables,3,133,False
3.2.4 Backpropagation with Pre-activation Variables,3,135,False
3.2.5 Examples of Updates for Various Activations,3,137,False
3.2.5.1 The Special Case of Softmax,4,137,False
3.2.6 A Decoupled View of Vector-Centric Backpropagation,3,138,False
3.2.7 Loss Functions on Multiple Output Nodes and Hidden Nodes,3,141,False
3.2.8 Mini-Batch Stochastic Gradient Descent,3,141,False
3.2.9 Backpropagation Tricks for Handling Shared Weights,3,143,False
3.2.10 Checking the Correctness of Gradient Computation,3,144,False
3.3 Setup and Initialization Issues,2,145,False
3.3.1 Tuning Hyperparameters,3,145,False
3.3.2 Feature Preprocessing,3,146,False
3.3.3 Initialization,3,148,False
3.4 The Vanishing and Exploding Gradient Problems,2,149,False
3.4.1 Geometric Understanding of the Effect of Gradient Ratios,3,150,False
3.4.2 A Partial Fix with Activation Function Choice,3,153,False
3.4.3 Dying Neurons and ``Brain Damage'',3,153,False
3.4.3.1 Leaky ReLU,4,153,False
3.4.3.2 Maxout,4,154,False
3.5 Gradient-Descent Strategies,2,154,False
3.5.1 Learning Rate Decay,3,155,False
3.5.2 Momentum-Based Learning,3,156,False
3.5.2.1 Nesterov Momentum,4,157,False
3.5.3 Parameter-Specific Learning Rates,3,157,False
3.5.3.1 AdaGrad,4,158,False
3.5.3.2 RMSProp,4,158,False
3.5.3.3 RMSProp with Nesterov Momentum,4,159,False
3.5.3.4 AdaDelta,4,159,False
3.5.3.5 Adam,4,160,False
3.5.4 Cliffs and Higher-Order Instability,3,161,False
3.5.5 Gradient Clipping,3,162,False
3.5.6 Second-Order Derivatives,3,163,False
3.5.6.1 Conjugate Gradients and Hessian-Free Optimization,4,165,False
3.5.6.2 Quasi-Newton Methods and BFGS,4,168,False
3.5.6.3 Problems with Second-Order Methods: Saddle Points,4,169,False
3.5.7 Polyak Averaging,3,171,False
3.5.8 Local and Spurious Minima,3,171,False
3.6 Batch Normalization,2,172,False
3.7 Practical Tricks for Acceleration and Compression,2,176,False
3.7.1 GPU Acceleration,3,177,False
3.7.2 Parallel and Distributed Implementations,3,178,False
3.7.3 Algorithmic Tricks for Model Compression,3,180,False
3.8 Summary,2,183,False
3.9 Bibliographic Notes,2,183,False
3.9.1 Software Resources,3,185,False
3.10 Exercises,2,185,False
4 Teaching Deep Learners to Generalize,1,188,True
4.1 Introduction,2,188,False
4.2 The Bias-Variance Trade-Off,2,193,False
4.2.1 Formal View,3,194,False
4.3 Generalization Issues in Model Tuning and Evaluation,2,197,False
4.3.1 Evaluating with Hold-Out and Cross-Validation,3,198,False
4.3.2 Issues with Training at Scale,3,199,False
4.3.3 How to Detect Need to Collect More Data,3,200,False
4.4 Penalty-Based Regularization,2,200,False
4.4.1 Connections with Noise Injection,3,201,False
4.4.2 L1-Regularization,3,202,False
4.4.3 L1- or L2-Regularization?,3,203,False
4.4.4 Penalizing Hidden Units: Learning Sparse Representations,3,204,False
4.5 Ensemble Methods,2,205,False
4.5.1 Bagging and Subsampling,3,205,False
4.5.2 Parametric Model Selection and Averaging,3,206,False
4.5.3 Randomized Connection Dropping,3,207,False
4.5.4 Dropout,3,207,False
4.5.5 Data Perturbation Ensembles,3,210,False
4.6 Early Stopping,2,211,False
4.6.1 Understanding Early Stopping from the Variance Perspective,3,211,False
4.7 Unsupervised Pretraining,2,212,False
4.7.1 Variations of Unsupervised Pretraining,3,216,False
4.7.2 What About Supervised Pretraining?,3,216,False
4.8 Continuation and Curriculum Learning,2,218,False
4.8.1 Continuation Learning,3,218,False
4.8.2 Curriculum Learning,3,219,False
4.9 Parameter Sharing,2,219,False
4.10 Regularization in Unsupervised Applications,2,220,False
4.10.1 Value-Based Penalization: Sparse Autoencoders,3,221,False
4.10.2 Noise Injection: De-noising Autoencoders,3,221,False
4.10.3 Gradient-Based Penalization: Contractive Autoencoders,3,223,False
4.10.4 Hidden Probabilistic Structure: Variational Autoencoders,3,226,False
4.10.4.1 Reconstruction and Generative Sampling,4,229,False
4.10.4.2 Conditional Variational Autoencoders,4,231,False
4.10.4.3 Relationship with Generative Adversarial Networks,4,232,False
4.11 Summary,2,232,False
4.12 Bibliographic Notes,2,233,False
4.12.1 Software Resources,3,234,False
4.13 Exercises,2,234,False
5 Radial Basis Function Networks,1,236,True
5.1 Introduction,2,236,False
5.2 Training an RBF Network,2,239,False
5.2.1 Training the Hidden Layer,3,240,False
5.2.2 Training the Output Layer,3,241,False
5.2.2.1 Expression with Pseudo-Inverse,4,243,False
5.2.3 Orthogonal Least-Squares Algorithm,3,243,False
5.2.4 Fully Supervised Learning,3,244,False
5.3 Variations and Special Cases of RBF Networks,2,245,False
5.3.1 Classification with Perceptron Criterion,3,245,False
5.3.2 Classification with Hinge Loss,3,246,False
5.3.3 Example of Linear Separability Promoted by RBF,3,246,False
5.3.4 Application to Interpolation,3,247,False
5.4 Relationship with Kernel Methods,2,248,False
5.4.1 Kernel Regression as a Special Case of RBF Networks,3,248,False
5.4.2 Kernel SVM as a Special Case of RBF Networks,3,249,False
5.4.3 Observations,3,250,False
5.5 Summary,2,250,False
5.6 Bibliographic Notes,2,251,False
5.7 Exercises,2,251,False
6 Restricted Boltzmann Machines,1,253,True
6.1 Introduction,2,253,False
6.1.1 Historical Perspective,3,254,False
6.2 Hopfield Networks,2,255,False
6.2.1 Optimal State Configurations of a Trained Network,3,256,False
6.2.2 Training a Hopfield Network,3,258,False
6.2.3 Building a Toy Recommender and Its Limitations,3,259,False
6.2.4 Increasing the Expressive Power of the Hopfield Network,3,260,False
6.3 The Boltzmann Machine,2,261,False
6.3.1 How a Boltzmann Machine Generates Data,3,262,False
6.3.2 Learning the Weights of a Boltzmann Machine,3,263,False
6.4 Restricted Boltzmann Machines,2,265,False
6.4.1 Training the RBM,3,267,False
6.4.2 Contrastive Divergence Algorithm,3,268,False
6.4.3 Practical Issues and Improvisations,3,269,False
6.5 Applications of Restricted Boltzmann Machines,2,269,False
6.5.1 Dimensionality Reduction and Data Reconstruction,3,270,False
6.5.2 RBMs for Collaborative Filtering,3,272,False
6.5.3 Using RBMs for Classification,3,275,False
6.5.4 Topic Models with RBMs,3,278,False
6.5.5 RBMs for Machine Learning with Multimodal Data,3,280,False
6.6 Using RBMs Beyond Binary Data Types,2,281,False
6.7 Stacking Restricted Boltzmann Machines,2,282,False
6.7.1 Unsupervised Learning,3,284,False
6.7.2 Supervised Learning,3,285,False
6.7.3 Deep Boltzmann Machines and Deep Belief Networks,3,285,False
6.8 Summary,2,286,False
6.9 Bibliographic Notes,2,286,False
6.10 Exercises,2,288,False
7 Recurrent Neural Networks,1,289,True
7.1 Introduction,2,289,False
7.1.1 Expressiveness of Recurrent Networks,3,292,False
7.2 The Architecture of Recurrent Neural Networks,2,292,False
7.2.1 Language Modeling Example of RNN,3,295,False
7.2.1.1 Generating a Language Sample,4,296,False
7.2.2 Backpropagation Through Time,3,298,False
7.2.3 Bidirectional Recurrent Networks,3,301,False
7.2.4 Multilayer Recurrent Networks,3,302,False
7.3 The Challenges of Training Recurrent Networks,2,304,False
7.3.1 Layer Normalization,3,307,False
7.4 Echo-State Networks,2,308,False
7.5 Long Short-Term Memory (LSTM),2,310,False
7.6 Gated Recurrent Units (GRUs),2,313,False
7.7 Applications of Recurrent Neural Networks,2,315,False
7.7.1 Application to Automatic Image Captioning,3,316,False
7.7.2 Sequence-to-Sequence Learning and Machine Translation,3,317,False
7.7.2.1 Question-Answering Systems,4,319,False
7.7.3 Application to Sentence-Level Classification,3,321,False
7.7.4 Token-Level Classification with Linguistic Features,3,322,False
7.7.5 Time-Series Forecasting and Prediction,3,323,False
7.7.6 Temporal Recommender Systems,3,325,False
7.7.7 Secondary Protein Structure Prediction,3,327,False
7.7.8 End-to-End Speech Recognition,3,327,False
7.7.9 Handwriting Recognition,3,327,False
7.8 Summary,2,328,False
7.9 Bibliographic Notes,2,328,False
7.9.1 Software Resources,3,329,False
7.10 Exercises,2,330,False
8 Convolutional Neural Networks,1,332,True
8.1 Introduction,2,332,False
8.1.1 Historical Perspective and Biological Inspiration,3,333,False
8.1.2 Broader Observations About Convolutional Neural Networks,3,334,False
8.2 The Basic Structure of a Convolutional Network,2,335,False
8.2.1 Padding,3,339,False
8.2.2 Strides,3,341,False
8.2.3 Typical Settings,3,341,False
8.2.4 The ReLU Layer,3,342,False
8.2.5 Pooling,3,343,False
8.2.6 Fully Connected Layers,3,344,False
8.2.7 The Interleaving Between Layers,3,345,False
8.2.8 Local Response Normalization,3,347,False
8.2.9 Hierarchical Feature Engineering,3,348,False
8.3 Training a Convolutional Network,2,349,False
8.3.1 Backpropagating Through Convolutions,3,350,False
8.3.2 Backpropagation as Convolution with Inverted/Transposed Filter,3,351,False
8.3.3 Convolution/Backpropagation as Matrix Multiplications,3,352,False
8.3.4 Data Augmentation,3,354,False
8.4 Case Studies of Convolutional Architectures,2,355,False
8.4.1 AlexNet,3,356,False
8.4.2 ZFNet,3,358,False
8.4.3 VGG,3,359,False
8.4.4 GoogLeNet,3,362,False
8.4.5 ResNet,3,364,False
8.4.6 The Effects of Depth,3,367,False
8.4.7 Pretrained Models,3,368,False
8.5 Visualization and Unsupervised Learning,2,369,False
8.5.1 Visualizing the Features of a Trained Network,3,370,False
8.5.2 Convolutional Autoencoders,3,374,False
8.6 Applications of Convolutional Networks,2,380,False
8.6.1 Content-Based Image Retrieval,3,380,False
8.6.2 Object Localization,3,381,False
8.6.3 Object Detection,3,382,False
8.6.4 Natural Language and Sequence Learning,3,383,False
8.6.5 Video Classification,3,384,False
8.7 Summary,2,385,False
8.8 Bibliographic Notes,2,385,False
8.8.1 Software Resources and Data Sets,3,387,False
8.9 Exercises,2,388,False
9 Deep Reinforcement Learning,1,389,True
9.1 Introduction,2,389,False
9.2 Stateless Algorithms: Multi-Armed Bandits,2,391,False
9.2.1 Naïve Algorithm,3,392,False
9.2.2 ε-Greedy Algorithm,3,392,False
9.2.3 Upper Bounding Methods,3,392,False
9.3 The Basic Framework of Reinforcement Learning,2,393,False
9.3.1 Challenges of Reinforcement Learning,3,395,False
9.3.2 Simple Reinforcement Learning for Tic-Tac-Toe,3,396,False
9.3.3 Role of Deep Learning and a Straw-Man Algorithm,3,396,False
9.4 Bootstrapping for Value Function Learning,2,399,False
9.4.1 Deep Learning Models as Function Approximators,3,400,False
9.4.2 Example: Neural Network for Atari Setting,3,402,False
9.4.3 On-Policy Versus Off-Policy Methods: SARSA,3,403,False
9.4.4 Modeling States Versus State-Action Pairs,3,405,False
9.5 Policy Gradient Methods,2,407,False
9.5.1 Finite Difference Methods,3,408,False
9.5.2 Likelihood Ratio Methods,3,409,False
9.5.3 Combining Supervised Learning with Policy Gradients,3,411,False
9.5.4 Actor-Critic Methods,3,411,False
9.5.5 Continuous Action Spaces,3,413,False
9.5.6 Advantages and Disadvantages of Policy Gradients,3,413,False
9.6 Monte Carlo Tree Search,2,414,False
9.7 Case Studies,2,415,False
9.7.1 AlphaGo: Championship Level Play at Go,3,415,False
9.7.1.1 Alpha Zero: Enhancements to Zero Human Knowledge,4,418,False
9.7.2 Self-Learning Robots,3,420,False
9.7.2.1 Deep Learning of Locomotion Skills,4,420,False
9.7.2.2 Deep Learning of Visuomotor Skills,4,422,False
9.7.3 Building Conversational Systems: Deep Learning for Chatbots,3,423,False
9.7.4 Self-Driving Cars,3,426,False
9.7.5 Inferring Neural Architectures with Reinforcement Learning,3,428,False
9.8 Practical Challenges Associated with Safety,2,429,False
9.9 Summary,2,430,False
9.10 Bibliographic Notes,2,430,False
9.10.1 Software Resources and Testbeds,3,432,False
9.11 Exercises,2,432,False
10 Advanced Topics in Deep Learning,1,434,True
10.1 Introduction,2,434,False
10.2 Attention Mechanisms,2,436,False
10.2.1 Recurrent Models of Visual Attention,3,437,False
10.2.1.1 Application to Image Captioning,4,439,False
10.2.2 Attention Mechanisms for Machine Translation,3,440,False
10.3 Neural Networks with External Memory,2,444,False
10.3.1 A Fantasy Video Game: Sorting by Example,3,445,False
10.3.1.1 Implementing Swaps with Memory Operations,4,446,False
10.3.2 Neural Turing Machines,3,447,False
10.3.3 Differentiable Neural Computer: A Brief Overview,3,452,False
10.4 Generative Adversarial Networks (GANs),2,453,False
10.4.1 Training a Generative Adversarial Network,3,454,False
10.4.2 Comparison with Variational Autoencoder,3,457,False
10.4.3 Using GANs for Generating Image Data,3,457,False
10.4.4 Conditional Generative Adversarial Networks,3,459,False
10.5 Competitive Learning,2,464,False
10.5.1 Vector Quantization,3,465,False
10.5.2 Kohonen Self-Organizing Map,3,465,False
10.6 Limitations of Neural Networks,2,468,False
10.6.1 An Aspirational Goal: One-Shot Learning,3,468,False
10.6.2 An Aspirational Goal: Energy-Efficient Learning,3,470,False
10.7 Summary,2,471,False
10.8 Bibliographic Notes,2,472,False
10.8.1 Software Resources,3,473,False
10.9 Exercises,2,473,False
Bibliography,1,474,False
Index,1,508,False
Preface,1,6,False
Acknowledgments,1,7,False
Contents,1,8,False
Acronyms,1,12,False
1 Embedded Deep Neural Networks,1,14,True
1.1 Introduction,2,14,False
1.2 Machine Learning,2,15,False
"1.2.1 Tasks, T",3,16,False
"1.2.2 Performance Measures, P",3,16,False
"1.2.3 Experience, E",3,16,False
1.2.3.1 Supervised Learning,4,17,False
1.2.3.2 Unsupervised Learning,4,17,False
1.3 Deep Learning,2,17,False
1.3.1 Deep Feed-Forward Neural Networks,3,19,False
1.3.2 Convolutional Neural Networks,3,20,False
1.3.3 Recurrent Neural Networks,3,28,False
1.3.4 Training Deep Neural Networks,3,30,False
1.3.4.1 Loss Functions,4,30,False
1.3.4.2 Backpropagation,4,31,False
1.3.4.3 Optimization,4,31,False
1.3.4.4 Data Sets,4,33,False
1.3.4.5 Regularization,4,34,False
1.3.4.6 Training Frameworks,4,35,False
1.4 Challenges for Embedded Deep Neural Networks,2,36,False
1.5 Book Contributions,2,38,False
References,2,40,False
2 Optimized Hierarchical Cascaded Processing,1,45,True
2.1 Introduction,2,45,False
2.2 Hierarchical Cascaded Systems,2,47,False
2.2.1 Generalizing Two-Stage Wake-Up Systems,3,47,False
"2.2.2 Hierarchical Cost, Precision, and Recall ",3,48,False
2.2.3 A Roofline Model for Hierarchical Classifiers,3,50,False
2.2.4 Optimized Hierarchical Cascaded Sensing,3,53,False
2.3 General Proof of Concept,2,53,False
2.3.1 System Description,3,54,False
2.3.2 Input Statistics,3,55,False
2.3.3 Experiments,3,56,False
2.3.3.1 Optimal Number of Stages,4,56,False
2.3.3.2 Optimal Stage Metrics in a Hierarchy,4,58,False
2.3.4 Conclusion,3,59,False
"2.4 Case study: Hierarchical, CNN-Based Face Recognition",2,59,False
2.4.1 A Face Recognition Hierarchy,3,59,False
"2.4.2 Hierarchical Cost, Precision, and Recall",3,61,False
2.4.3 An Optimized Face Recognition Hierarchy,3,62,False
2.5 Conclusion,2,64,False
References,2,66,False
3 Hardware-Algorithm Co-optimizations,1,67,True
3.1 An Introduction to Hardware-Algorithm Co-optimization,2,67,False
3.1.1 Exploiting Network Structure,3,68,False
3.1.2 Enhancing and Exploiting Sparsity,3,71,False
3.1.3 Enhancing and Exploiting Fault-Tolerance,3,72,False
3.2 Energy Gains in Low-Precision Neural Networks,2,75,False
3.2.1 Energy Consumption of Off-Chip Memory-Access,3,76,False
3.2.2 Generic Hardware Platform Modeling,3,76,False
3.3 Test-Time Fixed-Point Neural Networks,2,77,False
3.3.1 Analysis and Experiments,3,78,False
3.3.2 Influence of Quantization on Classification Accuracy,3,78,False
3.3.2.1 Uniform Quantization and Per-Layer Rescaling,4,79,False
3.3.2.2 Per-Layer Quantization,4,80,False
3.3.3 Energy in Sparse FPNNs,3,81,False
3.3.4 Results,3,83,False
3.3.5 Discussion,3,84,False
3.4 Train-Time Quantized Neural Networks,2,85,False
3.4.1 Training QNNs,3,86,False
3.4.1.1 Train-Time Quantized Weights,4,86,False
3.4.1.2 Train-Time Quantized Activations,4,87,False
3.4.1.3 QNN Input Layers,4,88,False
3.4.1.4 Quantized Training,4,88,False
3.4.2 Energy in QNNs,3,89,False
3.4.3 Experiments,3,89,False
3.4.3.1 Benchmarks,4,89,False
3.4.3.2 QNN Topologies,4,90,False
3.4.4 Results,3,91,False
3.4.5 Discussion,3,95,False
3.5 Clustered Neural Networks,2,95,False
3.6 Conclusion,2,96,False
References,2,97,False
4 Circuit Techniques for Approximate Computing,1,101,True
4.1 Introducing the Approximate Computing Paradigm,2,101,False
4.2 Approximate Computing Techniques,2,103,False
4.2.1 Resilience Identification and Quality Management,3,104,False
4.2.2 Approximate Circuits,3,105,False
4.2.3 Approximate Architectures,3,106,False
4.2.4 Approximate Software,3,107,False
4.2.5 Discussion,3,107,False
4.3 DVAFS: Dynamic-Voltage-Accuracy-Frequency-Scaling,2,108,False
4.3.1 DVAFS Basics,3,108,False
4.3.1.1 Introducing the DVAFS Energy-Accuracy Trade-Off,4,108,False
4.3.1.2 Precision Scaling in DVAFS,4,109,False
4.3.2 Resilience Identification for DVAFS,3,110,False
4.3.3 Energy Gains in DVAFS,3,112,False
4.3.3.1 DAS: Dynamic-Accuracy-Scaling,4,112,False
4.3.3.2 DVAS: Dynamic-Voltage-Accuracy-Scaling,4,112,False
4.3.3.3 DVAFS: Dynamic-Voltage-Accuracy-Frequency-Scaling,4,113,False
4.4 Performance Analysis of DVAFS,2,114,False
4.4.1 Block Level DVAFS,3,114,False
4.4.2 System-Level DVAFS,3,117,False
4.5 Implementation Challenges of DVAFS,2,119,False
4.5.1 Functional Implementation of Basic DVA(F)S Building Blocks,3,120,False
4.5.1.1 DAS and DVAS Compatible Building Blocks,4,120,False
4.5.1.2 DVAFS-Compatible Building Blocks,4,120,False
4.5.2 Physical Implementation of DVA(F)S Building Blocks,3,121,False
4.5.2.1 Granular Supply Scaling in DVAFS,4,122,False
4.5.2.2 Enforcing Critical Path Scaling in DVAFS,4,122,False
4.6 Overview and Discussion,2,123,False
References,2,123,False
5 ENVISION: Energy-Scalable Sparse Convolutional Neural Network Processing,1,126,True
5.1 Neural Network Acceleration,2,126,False
5.2 The Envision Processor Architecture,2,128,False
5.2.1 Processor Datapath,3,128,False
5.2.1.1 2D-MAC Array,4,128,False
5.2.1.2 Other Compute Units,4,132,False
5.2.2 On-Chip Memory Architecture,3,132,False
5.2.2.1 On-Chip Main Memory,4,132,False
5.2.2.2 Direct Memory Access Controller,4,133,False
5.2.3 Hardware Support for Exploiting Network Sparsity,3,133,False
5.2.3.1 Guarding Operations,4,133,False
5.2.3.2 Compressing IO Streams for Off-Chip Communication,4,135,False
5.2.4 Energy-Efficient Flexibility Through a Custom Instruction Set,3,135,False
5.2.5 Conclusion and Overview,3,136,False
5.3 DVAS Compatible Envision V1,2,136,False
5.3.1 RTL Level Hardware Support,3,137,False
5.3.2 Physical Implementation,3,138,False
5.3.3 Measurement Results,3,140,False
5.3.3.1 Performance of the Full Precision Baseline,4,140,False
5.3.3.2 Performance Under Dynamic Precision DVAS,4,141,False
5.3.3.3 Performance on Sparse Datastreams,4,143,False
5.3.3.4 Performance on Benchmarks,4,143,False
5.3.3.5 Comparison with the State of the Art,4,144,False
5.3.4 Envision V1 Overview,3,146,False
5.4 DVAFS-Compatible Envision V2,2,147,False
5.4.1 RTL Level Hardware Support,3,147,False
5.4.2 Physical Implementation,3,149,False
5.4.3 Measurement Results,3,150,False
5.4.3.1 Performance Under DVA(F)S,4,151,False
5.4.3.2 Influence of Optimal Body-Biasing,4,153,False
5.4.3.3 Performance on Sparse Datastreams,4,155,False
5.4.3.4 Performance on Benchmarks,4,155,False
5.4.3.5 Comparison with the State of the Art,4,157,False
5.4.4 Envision V2 Overview,3,159,False
5.5 Conclusion,2,160,False
References,2,161,False
6 BINAREYE: Digital and Mixed-Signal Always-On Binary Neural Network Processing,1,163,True
6.1 Binary Neural Networks,2,163,False
6.1.1 Introduction,3,163,False
6.1.2 Binary Neural Network Layers,3,164,False
6.2 Binary Neural Network Applications,2,168,False
6.3 A Programmable Input-to-Label Accelerator Architecture,2,169,False
6.3.1 256X: A Baseline BinaryNet Architecture,3,171,False
"6.3.1.1 Neuron Array, Weight Updates, and Input and Output Demuxing",4,171,False
6.3.1.2 Input Decoding,4,176,False
6.3.1.3 Dense Layers,4,177,False
6.3.1.4 System Control,4,178,False
6.3.2 SX: A Flexible DVAFS BinaryNet Architecture,3,180,False
6.4 MSBNN: A Mixed-Signal 256X Implementation,2,183,False
6.4.1 Switched-Capacitor Neuron Array,3,184,False
6.4.2 Measurement Results,3,185,False
6.4.3 Analog Signal Path Overhead,3,188,False
6.5 BinarEye: A Digital SX Implementation,2,189,False
6.5.1 An All-Digital Binary Neuron,3,189,False
6.5.2 Physical Implementation,3,190,False
6.5.3 Measurement Results,3,190,False
6.5.3.1 Benchmark Network Performance,4,191,False
6.5.3.2 Application-Level Performance,4,192,False
6.5.4 DVAFS in BinarEye,3,193,False
6.5.5 Comparison with the State-of-the-Art,3,195,False
6.6 Comparing Digital and Analog Binary Neural Network Implementations,2,197,False
6.7 Outlook and Future Work,2,200,False
6.8 Conclusion,2,202,False
References,2,203,False
"7 Conclusions, Contributions, and Future Work",1,205,True
7.1 Conclusions,2,206,False
7.2 Suggestions for Future Work,2,209,False
References,2,210,False
Index,1,211,False
Copyright,1,4,False
Table of Contents,1,5,False
Preface,1,17,False
The Machine Learning Tsunami,2,17,False
Machine Learning in Your Projects,2,17,False
Objective and Approach,2,18,False
Code Examples,2,19,False
Prerequisites,2,19,False
Roadmap,2,20,False
Changes Between the First and the Second Edition,2,21,False
Changes Between the Second and the Third Edition,2,22,False
Other Resources,2,23,False
Conventions Used in This Book,2,24,False
O’Reilly Online Learning,2,25,False
How to Contact Us,2,25,False
Acknowledgments,2,26,False
Part I. The Fundamentals of Machine Learning,1,29,False
Chapter 1. The Machine Learning Landscape,2,31,True
What Is Machine Learning?,3,32,False
Why Use Machine Learning?,3,33,False
Examples of Applications,3,36,False
Types of Machine Learning Systems,3,37,False
Training Supervision,4,38,False
Batch Versus Online Learning,4,45,False
Instance-Based Versus Model-Based Learning,4,49,False
Main Challenges of Machine Learning,3,55,False
Insufficient Quantity of Training Data,4,55,False
Nonrepresentative Training Data,4,56,False
Poor-Quality Data,4,58,False
Irrelevant Features,4,58,False
Overfitting the Training Data,4,58,False
Underfitting the Training Data,4,61,False
Stepping Back,4,61,False
Testing and Validating,3,62,False
Hyperparameter Tuning and Model Selection,4,62,False
Data Mismatch,4,63,False
Exercises,3,65,False
Chapter 2. End-to-End Machine Learning Project,2,67,True
Working with Real Data,3,67,False
Look at the Big Picture,3,69,False
Frame the Problem,4,69,False
Select a Performance Measure,4,71,False
Check the Assumptions,4,74,False
Get the Data,3,74,False
Running the Code Examples Using Google Colab,4,74,False
Saving Your Code Changes and Your Data,4,76,False
The Power and Danger of Interactivity,4,77,False
Book Code Versus Notebook Code,4,78,False
Download the Data,4,78,False
Take a Quick Look at the Data Structure,4,79,False
Create a Test Set,4,83,False
Explore and Visualize the Data to Gain Insights,3,88,False
Visualizing Geographical Data,4,89,False
Look for Correlations,4,91,False
Experiment with Attribute Combinations,4,94,False
Prepare the Data for Machine Learning Algorithms,3,95,False
Clean the Data,4,96,False
Handling Text and Categorical Attributes,4,99,False
Feature Scaling and Transformation,4,103,False
Custom Transformers,4,107,False
Transformation Pipelines,4,111,False
Select and Train a Model,3,116,False
Train and Evaluate on the Training Set,4,116,False
Better Evaluation Using Cross-Validation,4,117,False
Fine-Tune Your Model,3,119,False
Grid Search,4,119,False
Randomized Search,4,121,False
Ensemble Methods,4,123,False
Analyzing the Best Models and Their Errors,4,123,False
Evaluate Your System on the Test Set,4,124,False
"Launch, Monitor, and Maintain Your System",3,125,False
Try It Out!,3,128,False
Exercises,3,129,False
Chapter 3. Classification,2,131,True
MNIST,3,131,False
Training a Binary Classifier,3,134,False
Performance Measures,3,135,False
Measuring Accuracy Using Cross-Validation,4,135,False
Confusion Matrices,4,136,False
Precision and Recall,4,138,False
The Precision/Recall Trade-off,4,139,False
The ROC Curve,4,143,False
Multiclass Classification,3,147,False
Error Analysis,3,150,False
Multilabel Classification,3,153,False
Multioutput Classification,3,155,False
Exercises,3,157,False
Chapter 4. Training Models,2,159,True
Linear Regression,3,160,False
The Normal Equation,4,162,False
Computational Complexity,4,165,False
Gradient Descent,3,166,False
Batch Gradient Descent,4,170,False
Stochastic Gradient Descent,4,173,False
Mini-Batch Gradient Descent,4,176,False
Polynomial Regression,3,177,False
Learning Curves,3,179,False
Regularized Linear Models,3,183,False
Ridge Regression,4,184,False
Lasso Regression,4,186,False
Elastic Net Regression,4,189,False
Early Stopping,4,190,False
Logistic Regression,3,192,False
Estimating Probabilities,4,192,False
Training and Cost Function,4,193,False
Decision Boundaries,4,195,False
Softmax Regression,4,198,False
Exercises,3,201,False
Chapter 5. Support Vector Machines,2,203,True
Linear SVM Classification,3,203,False
Soft Margin Classification,4,204,False
Nonlinear SVM Classification,3,206,False
Polynomial Kernel,4,208,False
Similarity Features,4,209,False
Gaussian RBF Kernel,4,209,False
SVM Classes and Computational Complexity,4,211,False
SVM Regression,3,212,False
Under the Hood of Linear SVM Classifiers,3,214,False
The Dual Problem,3,217,False
Kernelized SVMs,4,218,False
Exercises,3,221,False
Chapter 6. Decision Trees,2,223,True
Training and Visualizing a Decision Tree,3,223,False
Making Predictions,3,225,False
Estimating Class Probabilities,3,227,False
The CART Training Algorithm,3,227,False
Computational Complexity,3,228,False
Gini Impurity or Entropy?,3,229,False
Regularization Hyperparameters,3,229,False
Regression,3,232,False
Sensitivity to Axis Orientation,3,234,False
Decision Trees Have a High Variance,3,235,False
Exercises,3,236,False
Chapter 7. Ensemble Learning and Random Forests,2,239,True
Voting Classifiers,3,240,False
Bagging and Pasting,3,243,False
Bagging and Pasting in Scikit-Learn,4,245,False
Out-of-Bag Evaluation,4,246,False
Random Patches and Random Subspaces,4,247,False
Random Forests,3,248,False
Extra-Trees,4,248,False
Feature Importance,4,249,False
Boosting,3,250,False
AdaBoost,4,250,False
Gradient Boosting,4,254,False
Histogram-Based Gradient Boosting,4,258,False
Stacking,3,260,False
Exercises,3,263,False
Chapter 8. Dimensionality Reduction,2,265,True
The Curse of Dimensionality,3,266,False
Main Approaches for Dimensionality Reduction,3,267,False
Projection,4,267,False
Manifold Learning,4,269,False
PCA,3,271,False
Preserving the Variance,4,271,False
Principal Components,4,272,False
Projecting Down to d Dimensions,4,273,False
Using Scikit-Learn,4,274,False
Explained Variance Ratio,4,274,False
Choosing the Right Number of Dimensions,4,275,False
PCA for Compression,4,277,False
Randomized PCA,4,278,False
Incremental PCA,4,278,False
Random Projection,3,280,False
LLE,3,282,False
Other Dimensionality Reduction Techniques,3,284,False
Exercises,3,285,False
Chapter 9. Unsupervised Learning Techniques,2,287,True
Clustering Algorithms: k-means and DBSCAN,3,288,False
k-means,4,291,False
Limits of k-means,4,300,False
Using Clustering for Image Segmentation,4,301,False
Using Clustering for Semi-Supervised Learning,4,303,False
DBSCAN,4,307,False
Other Clustering Algorithms,4,310,False
Gaussian Mixtures,3,311,False
Using Gaussian Mixtures for Anomaly Detection,4,316,False
Selecting the Number of Clusters,4,317,False
Bayesian Gaussian Mixture Models,4,320,False
Other Algorithms for Anomaly and Novelty Detection,4,321,False
Exercises,3,322,False
Part II. Neural Networks and Deep Learning,1,325,False
"Chapter 10. Introduction to Artificial Neural 
Networks with Keras",2,327,True
From Biological to Artificial Neurons,3,328,False
Biological Neurons,4,329,False
Logical Computations with Neurons,4,331,False
The Perceptron,4,332,False
The Multilayer Perceptron and Backpropagation,4,337,False
Regression MLPs,4,341,False
Classification MLPs,4,343,False
Implementing MLPs with Keras,3,345,False
Building an Image Classifier Using the Sequential API,4,346,False
Building a Regression MLP Using the Sequential API,4,356,False
Building Complex Models Using the Functional API,4,357,False
Using the Subclassing API to Build Dynamic Models,4,364,False
Saving and Restoring a Model,4,365,False
Using Callbacks,4,366,False
Using TensorBoard for Visualization,4,368,False
Fine-Tuning Neural Network Hyperparameters,3,372,False
Number of Hidden Layers,4,377,False
Number of Neurons per Hidden Layer,4,378,False
"Learning Rate, Batch Size, and Other Hyperparameters",4,379,False
Exercises,3,381,False
Chapter 11. Training Deep Neural Networks,2,385,True
The Vanishing/Exploding Gradients Problems,3,386,False
Glorot and He Initialization,4,387,False
Better Activation Functions,4,389,False
Batch Normalization,4,395,False
Gradient Clipping,4,400,False
Reusing Pretrained Layers,3,401,False
Transfer Learning with Keras,4,403,False
Unsupervised Pretraining,4,405,False
Pretraining on an Auxiliary Task,4,406,False
Faster Optimizers,3,407,False
Momentum,4,407,False
Nesterov Accelerated Gradient,4,409,False
AdaGrad,4,410,False
RMSProp,4,411,False
Adam,4,412,False
AdaMax,4,413,False
Nadam,4,414,False
AdamW,4,414,False
Learning Rate Scheduling,3,416,False
Avoiding Overfitting Through Regularization,3,420,False
ℓ1 and ℓ2 Regularization,4,421,False
Dropout,4,422,False
Monte Carlo (MC) Dropout,4,425,False
Max-Norm Regularization,4,427,False
Summary and Practical Guidelines,3,428,False
Exercises,3,430,False
Chapter 12. Custom Models and Training with TensorFlow,2,431,True
A Quick Tour of TensorFlow,3,431,False
Using TensorFlow like NumPy,3,435,False
Tensors and Operations,4,435,False
Tensors and NumPy,4,437,False
Type Conversions,4,437,False
Variables,4,438,False
Other Data Structures,4,438,False
Customizing Models and Training Algorithms,3,440,False
Custom Loss Functions,4,440,False
Saving and Loading Models That Contain Custom Components,4,441,False
"Custom Activation Functions, Initializers, Regularizers, 
and Constraints",4,443,False
Custom Metrics,4,444,False
Custom Layers,4,447,False
Custom Models,4,450,False
Losses and Metrics Based on Model Internals,4,452,False
Computing Gradients Using Autodiff,4,454,False
Custom Training Loops,4,458,False
TensorFlow Functions and Graphs,3,461,False
AutoGraph and Tracing,4,463,False
TF Function Rules,4,465,False
Exercises,3,466,False
Chapter 13. Loading and Preprocessing Data with TensorFlow,2,469,True
The tf.data API,3,470,False
Chaining Transformations,4,471,False
Shuffling the Data,4,473,False
Interleaving Lines from Multiple Files,4,474,False
Preprocessing the Data,4,476,False
Putting Everything Together,4,477,False
Prefetching,4,478,False
Using the Dataset with Keras,4,480,False
The TFRecord Format,3,481,False
Compressed TFRecord Files,4,482,False
A Brief Introduction to Protocol Buffers,4,482,False
TensorFlow Protobufs,4,484,False
Loading and Parsing Examples,4,485,False
Handling Lists of Lists Using the SequenceExample Protobuf,4,487,False
Keras Preprocessing Layers,3,487,False
The Normalization Layer,4,488,False
The Discretization Layer,4,491,False
The CategoryEncoding Layer,4,491,False
The StringLookup Layer,4,493,False
The Hashing Layer,4,494,False
Encoding Categorical Features Using Embeddings,4,494,False
Text Preprocessing,4,499,False
Using Pretrained Language Model Components,4,501,False
Image Preprocessing Layers,4,502,False
The TensorFlow Datasets Project,3,503,False
Exercises,3,505,False
Chapter 14. Deep Computer Vision Using Convolutional Neural Networks,2,507,True
The Architecture of the Visual Cortex,3,508,False
Convolutional Layers,3,509,False
Filters,4,512,False
Stacking Multiple Feature Maps,4,513,False
Implementing Convolutional Layers with Keras,4,515,False
Memory Requirements,4,518,False
Pooling Layers,3,519,False
Implementing Pooling Layers with Keras,3,521,False
CNN Architectures,3,523,False
LeNet-5,4,526,False
AlexNet,4,527,False
GoogLeNet,4,530,False
VGGNet,4,533,False
ResNet,4,533,False
Xception,4,537,False
SENet,4,538,False
Other Noteworthy Architectures,4,540,False
Choosing the Right CNN Architecture,4,542,False
Implementing a ResNet-34 CNN Using Keras,3,543,False
Using Pretrained Models from Keras,3,544,False
Pretrained Models for Transfer Learning,3,546,False
Classification and Localization,3,549,False
Object Detection,3,551,False
Fully Convolutional Networks,4,553,False
You Only Look Once,4,555,False
Object Tracking,3,558,False
Semantic Segmentation,3,559,False
Exercises,3,563,False
Chapter 15. Processing Sequences Using RNNs and CNNs,2,565,True
Recurrent Neurons and Layers,3,566,False
Memory Cells,4,568,False
Input and Output Sequences,4,569,False
Training RNNs,3,570,False
Forecasting a Time Series,3,571,False
The ARMA Model Family,4,577,False
Preparing the Data for Machine Learning Models,4,580,False
Forecasting Using a Linear Model,4,583,False
Forecasting Using a Simple RNN,4,584,False
Forecasting Using a Deep RNN,4,585,False
Forecasting Multivariate Time Series,4,587,False
Forecasting Several Time Steps Ahead,4,588,False
Forecasting Using a Sequence-to-Sequence Model,4,590,False
Handling Long Sequences,3,593,False
Fighting the Unstable Gradients Problem,4,593,False
Tackling the Short-Term Memory Problem,4,596,False
Exercises,3,604,False
Chapter 16. Natural Language Processing with RNNs and Attention,2,605,True
Generating Shakespearean Text Using a Character RNN,3,606,False
Creating the Training Dataset,4,607,False
Building and Training the Char-RNN Model,4,609,False
Generating Fake Shakespearean Text,4,610,False
Stateful RNN,4,612,False
Sentiment Analysis,3,615,False
Masking,4,618,False
Reusing Pretrained Embeddings and Language Models,4,621,False
An Encoder–Decoder Network for Neural Machine Translation,3,623,False
Bidirectional RNNs,4,629,False
Beam Search,4,631,False
Attention Mechanisms,3,632,False
Attention Is All You Need: The Original Transformer Architecture,4,637,False
An Avalanche of Transformer Models,3,648,False
Vision Transformers,3,652,False
Hugging Face’s Transformers Library,3,657,False
Exercises,3,661,False
"Chapter 17. Autoencoders, GANs, and Diffusion Models",2,663,True
Efficient Data Representations,3,665,False
Performing PCA with an Undercomplete Linear Autoencoder,3,667,False
Stacked Autoencoders,3,668,False
Implementing a Stacked Autoencoder Using Keras,4,669,False
Visualizing the Reconstructions,4,670,False
Visualizing the Fashion MNIST Dataset,4,671,False
Unsupervised Pretraining Using Stacked Autoencoders,4,672,False
Tying Weights,4,673,False
Training One Autoencoder at a Time,4,674,False
Convolutional Autoencoders,3,676,False
Denoising Autoencoders,3,677,False
Sparse Autoencoders,3,679,False
Variational Autoencoders,3,682,False
Generating Fashion MNIST Images,3,686,False
Generative Adversarial Networks,3,687,False
The Difficulties of Training GANs,4,691,False
Deep Convolutional GANs,4,693,False
Progressive Growing of GANs,4,696,False
StyleGANs,4,699,False
Diffusion Models,3,701,False
Exercises,3,709,False
Chapter 18. Reinforcement Learning,2,711,True
Learning to Optimize Rewards,3,712,False
Policy Search,3,713,False
Introduction to OpenAI Gym,3,715,False
Neural Network Policies,3,719,False
Evaluating Actions: The Credit Assignment Problem,3,721,False
Policy Gradients,3,722,False
Markov Decision Processes,3,727,False
Temporal Difference Learning,3,731,False
Q-Learning,3,732,False
Exploration Policies,4,734,False
Approximate Q-Learning and Deep Q-Learning,4,735,False
Implementing Deep Q-Learning,3,736,False
Deep Q-Learning Variants,3,741,False
Fixed Q-value Targets,4,741,False
Double DQN,4,742,False
Prioritized Experience Replay,4,742,False
Dueling DQN,4,743,False
Overview of Some Popular RL Algorithms,3,744,False
Exercises,3,748,False
Chapter 19. Training and Deploying TensorFlow Models at Scale,2,749,True
Serving a TensorFlow Model,3,750,False
Using TensorFlow Serving,4,750,False
Creating a Prediction Service on Vertex AI,4,760,False
Running Batch Prediction Jobs on Vertex AI,4,767,False
Deploying a Model to a Mobile or Embedded Device,3,769,False
Running a Model in a Web Page,3,772,False
Using GPUs to Speed Up Computations,3,774,False
Getting Your Own GPU,4,775,False
Managing the GPU RAM,4,777,False
Placing Operations and Variables on Devices,4,780,False
Parallel Execution Across Multiple Devices,4,781,False
Training Models Across Multiple Devices,3,784,False
Model Parallelism,4,784,False
Data Parallelism,4,787,False
Training at Scale Using the Distribution Strategies API,4,793,False
Training a Model on a TensorFlow Cluster,4,794,False
Running Large Training Jobs on Vertex AI,4,798,False
Hyperparameter Tuning on Vertex AI,4,800,False
Exercises,3,804,False
Thank You!,3,805,False
Appendix A. Machine Learning Project Checklist,1,807,True
Frame the Problem and Look at the Big Picture,2,807,False
Get the Data,2,808,False
Explore the Data,2,808,False
Prepare the Data,2,809,False
Shortlist Promising Models,2,810,False
Fine-Tune the System,2,810,False
Present Your Solution,2,811,False
Launch!,2,811,False
Appendix B. Autodiff,1,813,True
Manual Differentiation,2,813,False
Finite Difference Approximation,2,814,False
Forward-Mode Autodiff,2,815,False
Reverse-Mode Autodiff,2,818,False
Appendix C. Special Data Structures,1,821,True
Strings,2,821,False
Ragged Tensors,2,822,False
Sparse Tensors,2,823,False
Tensor Arrays,2,824,False
Sets,2,825,False
Queues,2,826,False
Appendix D. TensorFlow Graphs,1,829,True
TF Functions and Concrete Functions,2,829,False
Exploring Function Definitions and Graphs,2,831,False
A Closer Look at Tracing,2,833,False
Using AutoGraph to Capture Control Flow,2,834,False
Handling Variables and Other Resources in TF Functions,2,835,False
Using TF Functions with Keras (or Not),2,837,False
Index,1,839,False
About the Author,1,863,False
Colophon,1,863,False
Copyright,1,2,False
Preface,1,5,False
Chapter 1 Convex Optimization Basics,1,9,True
1.1 Overview of the Book,2,9,False
1.2 Definition of Convex Optimization,3,18,False
1.3 Tractability of Convex Optimization and Gradient Descent,3,26,False
Problem Set 1,3,32,False
1.4 Linear Program (LP),3,36,False
1.5 LP: Examples and Relaxation,3,43,False
1.6 LP: Algorithms,3,50,False
1.7 LP: CVXPY Implementation,3,58,False
Problem Set 2,3,62,False
Problem Set 2,3,62,False
1.8 Least Squares (LS),3,66,False
"1.9 LS: Test Error, Regularization and CVXPY Implementation",3,72,False
1.10 LS: Computed Tomography,3,80,False
Problem Set 3,3,88,False
Problem Set 3,3,88,False
1.11 Quadratic Program,3,94,False
1.12 Second-order Cone Program,3,101,False
1.13 Semi-definite Program (SDP),3,111,False
1.14 SDP Relaxation,3,117,False
Problem Set 4,3,124,False
Problem Set 4,3,124,False
Chapter 2 Duality,1,132,True
2.1 Strong Duality,2,132,False
2.2 Interior Point Method,3,139,False
Problem Set 5,3,148,False
Problem Set 5,3,148,False
2.3 Proof of Strong Duality Theorem (1/2),3,151,False
2.4 Proof of Strong Duality Theorem (2/2),3,158,False
Problem Set 6,3,165,False
Problem Set 6,3,165,False
2.5 Weak Duality,3,170,False
2.6 Lagrange Relaxation for Boolean Problems,3,177,False
2.7 Lagrange Relaxation for the MAXCUT Problem,3,183,False
Problem Set 7,3,190,False
Problem Set 7,3,190,False
Chapter 3 Machine Learning Applications,1,193,True
3.1 Supervised Learning and Optimization,2,193,False
3.2 Logistic Regression,3,201,False
3.3 Deep Learning I,3,209,False
3.4 Deep Learning II,3,218,False
3.5 Deep Learning: TensorFlow Implementation,3,229,False
Problem Set 8,3,237,False
Problem Set 8,3,237,False
3.6 Unsupervised Learning: Generative Modeling,3,248,False
3.7 Generative Adversarial Networks (GANs),3,254,False
3.8 GANs: TensorFlow Implementation,3,260,False
Problem Set 9,3,270,False
Problem Set 9,3,270,False
3.9 Wasserstein GAN I,3,279,False
3.10 Wasserstein GAN II,3,286,False
3.11 Wasserstein GAN: TensorFlow Implementation,3,293,False
Problem Set 10,3,303,False
Problem Set 10,3,303,False
3.12 Fair Machine Learning,3,307,False
3.13 A Fair Classifier and Its Connection to GANs,3,315,False
3.14 A Fair Classifier: TensorFlow Implementation,3,321,False
Problem Set 11,3,331,False
Problem Set 11,3,331,False
Chapter A Python Basics,1,337,True
A.1 Jupyter Notebook,2,337,False
A.2 Basic Python Syntaxes,3,342,False
A.2.1 Data Structure,4,342,False
A.2.2 Package,4,344,False
A.2.3 Visualization,4,349,False
Chapter B CVXPY Basics,1,351,False
Chapter C TensorFlow and Keras Basics,1,356,False
References,1,365,False
Index,1,370,False
About the Author,1,378,False
Cover,1,1,False
Title Page,1,2,False
Copyright and Credit,1,3,False
"Contributors",1,4,False
Table of Contents,1,6,False
Preface,1,12,False
Part 1: Introduction to Neural Networks,1,18,False
Chapter 1: Machine Learning – an Introduction,1,20,True
Technical requirements,2,20,False
Introduction to ML,2,21,False
Different ML approaches,2,22,False
Supervised learning,3,22,False
Unsupervised learning,3,28,False
Reinforcement learning,3,32,False
Components of an ML solution,3,35,False
Neural networks,3,38,False
Introducing PyTorch,3,39,False
Summary,2,43,False
Chapter 2: Neural Networks,1,44,True
Technical requirements,2,44,False
The need for NNs,2,45,False
The math of NNs,2,45,False
Linear algebra,3,46,False
An introduction to probability,3,50,False
Differential calculus,3,56,False
An introduction to NNs,2,58,False
Units – the smallest NN building block,3,59,False
Layers as operations,3,61,False
Multi-layer NNs,3,63,False
Activation functions,3,64,False
The universal approximation theorem,3,66,False
Training NNs,2,69,False
GD,3,69,False
Backpropagation,3,73,False
A code example of an NN for the XOR function,3,75,False
Summary,2,81,False
Chapter 3: Deep Learning Fundamentals,1,82,True
Technical requirements,2,82,False
Introduction to DL,2,83,False
Fundamental DL concepts,2,84,False
Feature learning,3,85,False
The reasons for DL’s popularity,3,86,False
Deep neural networks,2,87,False
Training deep neural networks,2,88,False
Improved activation functions,3,89,False
DNN regularization,3,93,False
Applications of DL,2,96,False
Introducing popular DL libraries,2,99,False
Classifying digits with Keras,3,99,False
Classifying digits with PyTorch,3,103,False
Summary,2,106,False
Part 2: Deep Neural Networks for Computer Vision,1,108,False
Chapter 4: Computer Vision with Convolutional Networks,1,110,True
Technical requirements,2,111,False
Intuition and justification for CNNs,2,111,False
Convolutional layers,2,112,False
A coding example of the convolution operation,3,115,False
Cross-channel and depthwise convolutions,3,117,False
Stride and padding in convolutional layers,3,120,False
Pooling layers,2,121,False
The structure of a convolutional network,2,123,False
Classifying images with PyTorch and Keras,2,124,False
Convolutional layers in deep learning libraries,3,124,False
Data augmentation,3,124,False
Classifying images with PyTorch,3,125,False
Classifying images with Keras,3,128,False
Advanced types of convolutions,2,130,False
"1D, 2D, and 3D convolutions",3,130,False
1×1 convolutions,3,131,False
Depthwise separable convolutions,3,131,False
Dilated convolutions,3,132,False
Transposed convolutions,3,133,False
Advanced CNN models,2,136,False
Introducing residual networks,3,137,False
Inception networks,3,140,False
Introducing Xception,3,145,False
Squeeze-and-Excitation Networks,3,146,False
Introducing MobileNet,3,147,False
EfficientNet,3,149,False
Using pre-trained models with PyTorch and Keras,3,150,False
Summary,2,151,False
Chapter 5: Advanced Computer Vision Applications,1,152,True
Technical requirements,2,153,False
Transfer learning (TL),2,153,False
Transfer learning with PyTorch,3,155,False
Transfer learning with Keras,3,158,False
Object detection,2,162,False
Approaches to object detection,3,163,False
Object detection with YOLO,3,165,False
Object detection with Faster R-CNN,3,170,False
Introducing image segmentation,2,176,False
Semantic segmentation with U-Net,3,177,False
Instance segmentation with Mask R-CNN,3,179,False
Image generation with diffusion models,2,182,False
Introducing generative models,3,183,False
Denoising Diffusion Probabilistic Models,3,184,False
Summary,2,187,False
Part 3: Natural Language Processing and Transformers,1,188,False
Chapter 6: Natural Language Processing and Recurrent Neural Networks,1,190,True
Technical requirements,2,191,False
Natural language processing,2,191,False
Tokenization,3,192,False
Introducing word embeddings,3,197,False
Word2Vec,3,199,False
Visualizing embedding vectors,3,203,False
Language modeling,3,204,False
Introducing RNNs,2,206,False
RNN implementation and training,3,209,False
Backpropagation through time,3,211,False
Vanishing and exploding gradients,3,214,False
Long-short term memory,3,216,False
Gated recurrent units,3,220,False
Implementing text classification,2,221,False
Summary,2,226,False
Chapter 7: The Attention Mechanism and Transformers,1,228,True
Technical requirements,2,228,False
Introducing seq2seq models,2,229,False
Understanding the attention mechanism,2,231,False
Bahdanau attention,3,231,False
Luong attention,3,234,False
General attention,3,235,False
Transformer attention,3,237,False
Implementing TA,3,241,False
Building transformers with attention,2,244,False
Transformer encoder,3,245,False
Transformer decoder,3,248,False
Putting it all together,3,251,False
Decoder-only and encoder-only models,3,253,False
Bidirectional Encoder Representations from Transformers,3,253,False
Generative Pre-trained Transformer,3,258,False
Summary,2,261,False
Chapter 8: Exploring Large Language Models in Depth,1,262,True
Technical requirements,2,263,False
Introducing LLMs,2,263,False
LLM architecture,2,264,False
LLM attention variants,3,264,False
Prefix decoder,3,271,False
Transformer nuts and bolts,3,272,False
Models,3,275,False
Training LLMs,2,276,False
Training datasets,3,277,False
Pre-training properties,3,280,False
FT with RLHF,3,285,False
Emergent abilities of LLMs,2,287,False
Introducing Hugging Face Transformers,2,289,False
Summary,2,293,False
Chapter 9: Advanced Applications of Large Language Models,1,294,True
Technical requirements,2,294,False
Classifying images with Vision Transformer,2,295,False
Using ViT with Hugging Face Transformers,3,297,False
Understanding the DEtection TRansformer,2,299,False
Using DetR with Hugging Face Transformers,3,303,False
Generating images with stable diffusion,2,305,False
Autoencoder,3,306,False
Conditioning transformer,3,307,False
Diffusion model,3,309,False
Using stable diffusion with Hugging Face Transformers,3,310,False
Exploring fine-tuning transformers,2,313,False
Harnessing the power of LLMs with LangChain,2,315,False
Using LangChain in practice,3,316,False
Summary,2,319,False
Part 4: Developing and Deploying Deep Neural Networks,1,320,False
Chapter 10: Machine Learning Operations (MLOps),1,322,True
Technical requirements,2,323,False
Understanding model development,2,323,False
Choosing an NN framework,3,323,False
PyTorch versus TensorFlow versus JAX,3,323,False
Open Neural Network Exchange,3,324,False
Introducing TensorBoard,3,329,False
Developing NN models for edge devices with TF Lite,3,333,False
Mixed-precision training with PyTorch,3,336,False
Exploring model deployment,2,337,False
Deploying NN models with Flask,3,337,False
Building ML web apps with Gradio,3,339,False
Summary,2,342,False
Index,1,344,False
Other Books You May Enjoy,1,359,False
1 Introduction,1,17,True
1.1 I Studied Deep Learning the Wrong Way…This Is the Right Way,2,17,False
1.2 Who This Book Is For,2,19,False
1.2.1 Just Getting Started in Deep Learning?,3,19,False
1.2.2 Already a Seasoned Deep Learning Practitioner?,3,19,False
1.3 Book Organization,2,19,False
1.3.1 Volume #1: Starter Bundle,3,19,False
1.3.2 Volume #2: Practitioner Bundle,3,20,False
1.3.3 Volume #3: ImageNet Bundle,3,20,False
1.3.4 Need to Upgrade Your Bundle?,3,20,False
"1.4 Tools of the Trade: Python, Keras, and Mxnet",2,20,False
1.4.1 What About TensorFlow?,3,20,False
1.4.2 Do I Need to Know OpenCV?,3,21,False
1.5 Developing Our Own Deep Learning Toolset,2,21,False
1.6 Summary,2,22,False
2 What Is Deep Learning?,1,23,True
2.1 A Concise History of Neural Networks and Deep Learning,2,24,False
2.2 Hierarchical Feature Learning,2,26,False
"2.3 How ""Deep"" Is Deep?",2,29,False
2.4 Summary,2,32,False
3 Image Fundamentals,1,33,True
3.1 Pixels: The Building Blocks of Images,2,33,False
3.1.1 Forming an Image From Channels,3,36,False
3.2 The Image Coordinate System,2,36,False
3.2.1 Images as NumPy Arrays,3,37,False
3.2.2 RGB and BGR Ordering,3,38,False
3.3 Scaling and Aspect Ratios,2,38,False
3.4 Summary,2,40,False
4 Image Classification Basics,1,41,True
4.1 What Is Image Classification?,2,42,False
4.1.1 A Note on Terminology,3,42,False
4.1.2 The Semantic Gap,3,43,False
4.1.3 Challenges,3,44,False
4.2 Types of Learning,2,47,False
4.2.1 Supervised Learning,3,47,False
4.2.2 Unsupervised Learning,3,48,False
4.2.3 Semi-supervised Learning,3,49,False
4.3 The Deep Learning Classification Pipeline,2,50,False
4.3.1 A Shift in Mindset,3,50,False
4.3.2 Step #1: Gather Your Dataset,3,52,False
4.3.3 Step #2: Split Your Dataset,3,52,False
4.3.4 Step #3: Train Your Network,3,53,False
4.3.5 Step #4: Evaluate,3,53,False
4.3.6 Feature-based Learning versus Deep Learning for Image Classification,3,53,False
4.3.7 What Happens When my Predictions Are Incorrect?,3,54,False
4.4 Summary,2,54,False
5 Datasets for Image Classification,1,55,True
5.1 MNIST,2,55,False
"5.2 Animals: Dogs, Cats, and Pandas",2,56,False
5.3 CIFAR-10,2,57,False
5.4 SMILES,2,57,False
5.5 Kaggle: Dogs vs. Cats,2,58,False
5.6 Flowers-17,2,58,False
5.7 CALTECH-101,2,59,False
5.8 Tiny ImageNet 200,2,59,False
5.9 Adience,2,60,False
5.10 ImageNet,2,60,False
5.10.1 What Is ImageNet?,3,60,False
5.10.2 ImageNet Large Scale Visual Recognition Challenge (ILSVRC),3,60,False
5.11 Kaggle: Facial Expression Recognition Challenge,2,61,False
5.12 Indoor CVPR,2,62,False
5.13 Stanford Cars,2,62,False
5.14 Summary,2,62,False
6 Configuring Your Development Environment,1,65,True
6.1 Libraries and Packages,2,65,False
6.1.1 Python,3,65,False
6.1.2 Keras,3,66,False
6.1.3 Mxnet,3,66,False
"6.1.4 OpenCV, scikit-image, scikit-learn, and more",3,66,False
6.2 Configuring Your Development Environment?,2,66,False
6.3 Preconfigured Virtual Machine,2,67,False
6.4 Cloud-based Instances,2,67,False
6.5 How to Structure Your Projects,2,67,False
6.6 Summary,2,68,False
7 Your First Image Classifier,1,69,True
7.1 Working with Image Datasets,2,69,False
7.1.1 Introducing the “Animals” Dataset,3,69,False
7.1.2 The Start to Our Deep Learning Toolkit,3,70,False
7.1.3 A Basic Image Preprocessor,3,71,False
7.1.4 Building an Image Loader,3,72,False
7.2 k-NN: A Simple Classifier,2,74,False
7.2.1 A Worked k-NN Example,3,76,False
7.2.2 k-NN Hyperparameters,3,77,False
7.2.3 Implementing k-NN,3,77,False
7.2.4 k-NN Results,3,80,False
7.2.5 Pros and Cons of k-NN,3,81,False
7.3 Summary,2,82,False
8 Parameterized Learning,1,83,True
8.1 An Introduction to Linear Classification,2,84,False
8.1.1 Four Components of Parameterized Learning,3,84,False
8.1.2 Linear Classification: From Images to Labels,3,85,False
8.1.3 Advantages of Parameterized Learning and Linear Classification,3,86,False
8.1.4 A Simple Linear Classifier With Python,3,87,False
8.2 The Role of Loss Functions,2,90,False
8.2.1 What Are Loss Functions?,3,90,False
8.2.2 Multi-class SVM Loss,3,91,False
8.2.3 Cross-entropy Loss and Softmax Classifiers,3,93,False
8.3 Summary,2,96,False
9 Optimization Methods and Regularization,1,97,True
9.1 Gradient Descent,2,98,False
9.1.1 The Loss Landscape and Optimization Surface,3,98,False
9.1.2 The “Gradient” in Gradient Descent,3,99,False
9.1.3 Treat It Like a Convex Problem (Even if It’s Not),3,100,False
9.1.4 The Bias Trick,3,100,False
9.1.5 Pseudocode for Gradient Descent,3,101,False
9.1.6 Implementing Basic Gradient Descent in Python,3,102,False
9.1.7 Simple Gradient Descent Results,3,106,False
9.2 Stochastic Gradient Descent (SGD),2,108,False
9.2.1 Mini-batch SGD,3,108,False
9.2.2 Implementing Mini-batch SGD,3,109,False
9.2.3 SGD Results,3,112,False
9.3 Extensions to SGD,2,113,False
9.3.1 Momentum,3,113,False
9.3.2 Nesterov's Acceleration,3,114,False
9.3.3 Anecdotal Recommendations,3,115,False
9.4 Regularization,2,115,False
9.4.1 What Is Regularization and Why Do We Need It?,3,115,False
9.4.2 Updating Our Loss and Weight Update To Include Regularization,3,117,False
9.4.3 Types of Regularization Techniques,3,118,False
9.4.4 Regularization Applied to Image Classification,3,119,False
9.5 Summary,2,121,False
10 Neural Network Fundamentals,1,123,True
10.1 Neural Network Basics,2,123,False
10.1.1 Introduction to Neural Networks,3,124,False
10.1.2 The Perceptron Algorithm,3,131,False
10.1.3 Backpropagation and Multi-layer Networks,3,139,False
10.1.4 Multi-layer Networks with Keras,3,155,False
10.1.5 The Four Ingredients in a Neural Network Recipe,3,165,False
10.1.6 Weight Initialization,3,167,False
10.1.7 Constant Initialization,3,167,False
10.1.8 Uniform and Normal Distributions,3,167,False
10.1.9 LeCun Uniform and Normal,3,168,False
10.1.10 Glorot/Xavier Uniform and Normal,3,168,False
10.1.11 He et al./Kaiming/MSRA Uniform and Normal,3,169,False
10.1.12 Differences in Initialization Implementation,3,169,False
10.2 Summary,2,170,False
11 Convolutional Neural Networks,1,171,True
11.1 Understanding Convolutions,2,172,False
11.1.1 Convolutions versus Cross-correlation,3,172,False
"11.1.2 The “Big Matrix” and “Tiny Matrix"" Analogy",3,173,False
11.1.3 Kernels,3,173,False
11.1.4 A Hand Computation Example of Convolution,3,174,False
11.1.5 Implementing Convolutions with Python,3,175,False
11.1.6 The Role of Convolutions in Deep Learning,3,181,False
11.2 CNN Building Blocks,2,181,False
11.2.1 Layer Types,3,183,False
11.2.2 Convolutional Layers,3,183,False
11.2.3 Activation Layers,3,188,False
11.2.4 Pooling Layers,3,188,False
11.2.5 Fully-connected Layers,3,190,False
11.2.6 Batch Normalization,3,191,False
11.2.7 Dropout,3,192,False
11.3 Common Architectures and Training Patterns,2,193,False
11.3.1 Layer Patterns,3,193,False
11.3.2 Rules of Thumb,3,194,False
"11.4 Are CNNs Invariant to Translation, Rotation, and Scaling?",2,196,False
11.5 Summary,2,197,False
12 Training Your First CNN,1,199,True
12.1 Keras Configurations and Converting Images to Arrays,2,199,False
12.1.1 Understanding the keras.json Configuration File,3,199,False
12.1.2 The Image to Array Preprocessor,3,200,False
12.2 ShallowNet,2,202,False
12.2.1 Implementing ShallowNet,3,202,False
12.2.2 ShallowNet on Animals,3,204,False
12.2.3 ShallowNet on CIFAR-10,3,208,False
12.3 Summary,2,211,False
13 Saving and Loading Your Models,1,213,True
13.1 Serializing a Model to Disk,2,213,False
13.2 Loading a Pre-trained Model from Disk,2,216,False
13.3 Summary,2,219,False
14 LeNet: Recognizing Handwritten Digits,1,221,True
14.1 The LeNet Architecture,2,221,False
14.2 Implementing LeNet,2,222,False
14.3 LeNet on MNIST,2,224,False
14.4 Summary,2,229,False
15 MiniVGGNet: Going Deeper with CNNs,1,231,True
15.1 The VGG Family of Networks,2,231,False
15.1.1 The (Mini) VGGNet Architecture,3,232,False
15.2 Implementing MiniVGGNet,2,232,False
15.3 MiniVGGNet on CIFAR-10,2,236,False
15.3.1 With Batch Normalization,3,238,False
15.3.2 Without Batch Normalization,3,239,False
15.4 Summary,2,240,False
16 Learning Rate Schedulers,1,243,True
16.1 Dropping Our Learning Rate,2,243,False
16.1.1 The Standard Decay Schedule in Keras,3,244,False
16.1.2 Step-based Decay,3,245,False
16.1.3 Implementing Custom Learning Rate Schedules in Keras,3,246,False
16.2 Summary,2,251,False
17 Spotting Underfitting and Overfitting,1,253,True
17.1 What Are Underfitting and Overfitting?,2,253,False
17.1.1 Effects of Learning Rates,3,255,False
17.1.2 Pay Attention to Your Training Curves,3,256,False
17.1.3 What if Validation Loss Is Lower than Training Loss?,3,256,False
17.2 Monitoring the Training Process,2,257,False
17.2.1 Creating a Training Monitor,3,257,False
17.2.2 Babysitting Training,3,259,False
17.3 Summary,2,262,False
18 Checkpointing Models,1,265,True
18.1 Checkpointing Neural Network Model Improvements,2,265,False
18.2 Checkpointing Best Neural Network Only,2,269,False
18.3 Summary,2,271,False
19 Visualizing Network Architectures,1,273,True
19.1 The Importance of Architecture Visualization,2,273,False
19.1.1 Installing graphviz and pydot,3,274,False
19.1.2 Visualizing Keras Networks,3,274,False
19.2 Summary,2,277,False
20 Out-of-the-box CNNs for Classification,1,279,True
20.1 State-of-the-art CNNs in Keras,2,279,False
20.1.1 VGG16 and VGG19,3,280,False
20.1.2 ResNet,3,281,False
20.1.3 Inception V3,3,282,False
20.1.4 Xception,3,282,False
20.1.5 Can We Go Smaller?,3,282,False
20.2 Classifying Images with Pre-trained ImageNet CNNs,2,283,False
20.2.1 Classification Results,3,286,False
20.3 Summary,2,288,False
21 Case Study: Breaking Captchas with a CNN,1,289,True
21.1 Breaking Captchas with a CNN,2,290,False
21.1.1 A Note on Responsible Disclosure,3,290,False
21.1.2 The Captcha Breaker Directory Structure,3,292,False
21.1.3 Automatically Downloading Example Images,3,293,False
21.1.4 Annotating and Creating Our Dataset,3,294,False
21.1.5 Preprocessing the Digits,3,299,False
21.1.6 Training the Captcha Breaker,3,301,False
21.1.7 Testing the Captcha Breaker,3,305,False
21.2 Summary,2,307,False
22 Case Study: Smile Detection,1,309,True
22.1 The SMILES Dataset,2,309,False
22.2 Training the Smile CNN,2,310,False
22.3 Running the Smile CNN in Real-time,2,315,False
22.4 Summary,2,318,False
23 Your Next Steps,1,321,True
"23.1 So, What's Next?",2,321,False
Table of Contents,1,5,False
About the Authors,1,11,False
About the Technical Reviewer,1,13,False
Introduction,1,14,False
Chapter 1: Introduction to Generative AI,1,16,True
"So, What Is Generative AI?",2,17,False
Components of AI,2,18,False
Domains of Generative AI,2,19,False
Text Generation,3,19,False
Image Generation,3,19,False
Audio Generation,3,20,False
Video Generation,3,20,False
Generating Images,4,20,False
Generating Text,4,21,False
Generative AI: Current Players and Their Models,3,24,False
Generative AI Applications,3,26,False
Conclusion,2,28,False
Chapter 2: Evolution of Neural Networks to Large Language Models,1,29,True
Natural Language Processing,2,30,False
Tokenization,3,31,False
N-grams,3,31,False
Language Representation and Embeddings,3,33,False
Word2Vec,4,33,False
GloVe (Global Vectors for Word Representation),4,34,False
Probabilistic Models,2,34,False
Neural Network–Based Language Models,2,35,False
Recurrent Neural Networks (RNNs),3,36,False
Long Short-Term Memory (LSTM),3,37,False
Gated Recurrent Unit (GRU),3,38,False
Encoder-Decoder Networks,3,39,False
Sequence-to-Sequence Models,4,39,False
Encoder,5,40,False
Decoder,5,40,False
Attention Mechanism,4,40,False
Training Sequence-to-Sequence Models,5,41,False
Challenges of Sequence-to-Sequence Models,5,41,False
Transformer,2,41,False
Large Language Models (LLMs),2,43,False
Conclusion,2,44,False
Chapter 3: LLMs and Transformers,1,46,True
The Power of Language Models,2,46,False
Transformer Architecture,2,47,False
Motivation for Transformer,3,48,False
Architecture,3,48,False
Encoder-Decoder Architecture,3,49,False
Encoder,4,49,False
Decoder,4,50,False
Attention,3,52,False
Inputs,4,53,False
Calculating Attention Scores,4,53,False
Calculating Attention Weights,4,53,False
Weighted Sum,4,54,False
Scaled Dot-Product Attention,4,54,False
Input and Matrices,5,55,False
Dot Product and Scaling,5,55,False
Softmax and Attention Weights,5,55,False
Matrix Formulation and Efficiency,5,56,False
Multi-Head Attention,4,56,False
Input and Linear Projections,5,57,False
Multiple Attention Heads,5,58,False
Scaled Dot-Product Attention per Head,5,58,False
Concatenation and Linear Projection,5,58,False
Model’s Flexibility,5,58,False
Position-wise Feed-Forward Networks,3,60,False
Position Encoding,4,61,False
Interpretation,5,63,False
Advantages and Limitations of Transformer Architecture,3,64,False
Advantages,4,64,False
Limitations,4,65,False
Conclusion,2,66,False
Chapter 4: The ChatGPT Architecture: An In-Depth Exploration of OpenAI’s Conversational Language Model,1,67,True
The Evolution of GPT Models,2,68,False
The Transformer Architecture: A Recap,2,69,False
Architecture of ChatGPT,2,71,False
Pre-training and Fine-Tuning in ChatGPT,2,82,False
Pre-training: Learning Language Patterns,3,82,False
Fine-Tuning: Adapting to Specific Tasks,3,83,False
Continuous Learning and Iterative Improvement,3,83,False
Contextual Embeddings in ChatGPT,2,83,False
Response Generation in ChatGPT,2,84,False
Handling Biases and Ethical Considerations,2,85,False
Addressing Biases in Language Models,3,85,False
OpenAI’s Efforts to Mitigate Biases,3,85,False
Strengths and Limitations,2,87,False
Strengths of ChatGPT,3,87,False
Limitations of ChatGPT,3,88,False
Conclusion,2,89,False
Chapter 5: Google Bard and Beyond,1,90,True
The Transformer Architecture,2,91,False
Elevating Transformer: The Genius of Google Bard,2,91,False
Google Bard’s Text and Code Fusion,3,93,False
Self-Supervised Learning,4,94,False
Strengths and Weaknesses of Google Bard,2,94,False
Strengths,3,94,False
Weaknesses,3,95,False
Difference Between ChatGPT and Google Bard,2,95,False
Claude 2,2,97,False
Key Features of Claude 2,3,97,False
Comparing Claude 2 to Other AI Chatbots,3,98,False
The Human-Centered Design Philosophy of Claude,3,99,False
Exploring Claude’s AI Conversation Proficiencies,3,100,False
Constitutional AI,3,100,False
Claude 2 vs. GPT 3.5,3,103,False
Other Large Language Models,2,104,False
Falcon AI,3,104,False
LLaMa 2,3,106,False
Dolly 2,3,109,False
Conclusion,2,110,False
Chapter 6: Implement LLMs Using Sklearn,1,111,True
Install Scikit-LLM and Setup,2,112,False
Obtain an OpenAI API Key,3,113,False
Zero-Shot GPTClassifier,2,113,False
What If You Find Yourself Without Labeled Data?,3,119,False
Multilabel Zero-Shot Text Classification,2,121,False
Implementation,3,121,False
What If You Find Yourself Without Labeled Data?,3,122,False
Implementation,3,122,False
Text Vectorization,2,123,False
Implementation,3,123,False
Text Summarization,2,124,False
Implementation,3,125,False
Conclusion,2,125,False
Chapter 7: LLMs for Enterprise and LLMOps,1,127,True
Private Generalized LLM API,2,128,False
Design Strategy to Enable LLMs for Enterprise: In-Context Learning,2,129,False
Data Preprocessing/Embedding,3,131,False
Prompt Construction/Retrieval,3,133,False
Fine-Tuning,2,136,False
Technology Stack,2,138,False
Gen AI/LLM Testbed,3,138,False
Data Sources,3,139,False
Data Processing,3,139,False
Leveraging Embeddings for Enterprise LLMs,3,140,False
Vector Databases: Accelerating Enterprise LLMs with Semantic Search,3,140,False
LLM APIs: Empowering Enterprise Language Capabilities,3,140,False
LLMOps,2,141,False
What Is LLMOps?,3,141,False
Why LLMOps?,3,143,False
What Is an LLMOps Platform?,3,144,False
Technology Components LLMOps,3,145,False
Monitoring Generative AI Models,3,146,False
Proprietary Generative AI Models,3,149,False
Open Source Models with Permissive Licenses,3,150,False
Playground for Model Selection,3,151,False
Evaluation Metrics,3,151,False
Validating LLM Outputs,3,154,False
Challenges Faced When Deploying LLMs,3,156,False
Implementation,2,158,False
Using the OpenAI API with Python,3,158,False
Using the OpenAI API with Python,4,158,False
Prerequisites,4,158,False
Installation,4,159,False
Initializing the Environment and Setting API Key,4,159,False
Test the Environment,4,159,False
Data Preparation: Loading PDF Data,4,160,False
Embeddings and VectorDB Using LangChain and Chroma,4,160,False
Utilizing OpenAI API,4,161,False
Leveraging Azure OpenAI Service,3,163,False
Conclusion,2,163,False
Chapter 8: Diffusion Model and  Generative AI for Images,1,165,True
Variational Autoencoders (VAEs),2,166,False
Generative Adversarial Networks (GANs),2,167,False
Diffusion Models,2,168,False
Types of Diffusion Models,3,170,False
Architecture,3,172,False
The Technology Behind DALL-E 2,2,175,False
Top Part: CLIP Training Process,3,177,False
Bottom Part: Text-to-Image Generation Process,3,178,False
The Technology Behind Stable Diffusion,2,178,False
Latent Diffusion Model (LDM),3,179,False
Benefits and Significance,3,180,False
The Technology Behind Midjourney,2,180,False
Generative Adversarial Networks (GANs),3,180,False
Text-to-Image Synthesis with GANs,3,181,False
Conditional GANs,3,181,False
Training Process,3,181,False
Loss Functions and Optimization,3,181,False
Attention Mechanisms,3,182,False
Data Augmentation and Preprocessing,3,182,False
Benefits and Applications,3,182,False
"Comparison Between DALL-E 2, Stable Diffusion, and Midjourney",2,182,False
Applications,2,184,False
Conclusion,2,186,False
Chapter 9: ChatGPT Use Cases,1,188,True
Business and Customer Service,2,188,False
Content Creation and Marketing,2,190,False
Software Development and Tech Support,2,192,False
Data Entry and Analysis,2,194,False
Healthcare and Medical Information,2,196,False
Market Research and Analysis,2,198,False
Creative Writing and Storytelling,2,200,False
Education and Learning,2,202,False
Legal and Compliance,2,203,False
HR and Recruitment,2,205,False
Personal Assistant and Productivity,2,207,False
Examples,2,209,False
Conclusion,2,214,False
Index,1,216,False
Cover,1,1,False
Copyright,1,3,False
Contributors,1,5,False
Table of Contents,1,8,False
Preface,1,16,False
Chapter  1: What Is Generative AI?,1,24,True
Introducing generative AI,2,25,False
What are generative models?,3,27,False
Why now?,3,31,False
Understanding LLMs,2,34,False
What is a GPT?,3,36,False
Other LLMs,3,39,False
Major players,3,41,False
How do GPT models work?,3,43,False
Pre-training,4,46,False
Tokenization,4,47,False
Scaling,4,48,False
Conditioning,4,49,False
How to try out these models,3,50,False
What are text-to-image models?,2,50,False
What can AI do in other domains?,2,56,False
Summary,2,57,False
Questions,2,58,False
Chapter  2: LangChain for LLM Apps,1,60,True
Going beyond stochastic parrots,2,61,False
What are the limitations of LLMs?,3,61,False
How can we mitigate LLM limitations?,3,65,False
What is an LLM app?,3,66,False
What is LangChain?,2,69,False
Exploring key components of LangChain,2,73,False
What are chains?,3,73,False
What are agents?,3,75,False
What is memory?,3,77,False
What are tools?,3,78,False
How does LangChain work?,2,80,False
Comparing LangChain with other frameworks,2,83,False
Summary,2,84,False
Questions,2,85,False
Chapter  3: Getting Started with LangChain,1,88,True
How to set up the dependencies for this book,2,88,False
pip,3,90,False
Poetry,3,91,False
Conda,3,91,False
Docker,3,91,False
Exploring API model integrations,2,92,False
Fake LLM,3,95,False
OpenAI,3,96,False
Hugging Face,3,98,False
Google Cloud Platform,3,100,False
Jina AI,3,103,False
Replicate,3,105,False
Others,3,107,False
Azure,4,107,False
Anthropic,4,108,False
Exploring local models,2,108,False
Hugging Face Transformers,3,109,False
llama.cpp,3,110,False
GPT4All,3,111,False
Building an application for customer service,2,112,False
Summary,2,119,False
Questions,2,119,False
Chapter  4: Building Capable Assistants,1,122,True
Mitigating hallucinations through fact-checking,2,123,False
Summarizing information,2,126,False
Basic prompting,3,126,False
Prompt templates,3,127,False
Chain of density,3,128,False
Map-Reduce pipelines,3,130,False
Monitoring token usage,3,132,False
Extracting information from documents,2,135,False
Answering questions with tools,2,139,False
Information retrieval with tools,3,139,False
Building a visual interface,3,141,False
Exploring reasoning strategies,2,144,False
Summary,2,152,False
Questions,2,153,False
Chapter  5: Building a Chatbot like ChatGPT,1,154,True
What is a chatbot?,2,155,False
Understanding retrieval and vectors,2,157,False
Embeddings,3,158,False
Vector storage,3,162,False
Vector indexing,4,163,False
Vector libraries,4,164,False
Vector databases,4,166,False
Loading and retrieving in LangChain,2,171,False
Document loaders,3,172,False
Retrievers in LangChain,3,173,False
kNN retriever,4,174,False
PubMed retriever,4,175,False
Custom retrievers,4,176,False
Implementing a chatbot,2,176,False
Document loader,3,177,False
Vector storage,3,178,False
Memory,3,183,False
Conversation buffers,4,184,False
Remembering conversation summaries,4,187,False
Storing knowledge graphs,4,187,False
Combining several memory mechanisms,4,188,False
Long-term persistence,4,189,False
Moderating responses,2,190,False
Summary,2,193,False
Questions,2,194,False
Chapter  6: Developing Software with Generative AI,1,196,True
Software development and AI,2,197,False
Code LLMs,3,198,False
Writing code with LLMs,2,202,False
StarCoder,3,202,False
StarChat,3,207,False
Llama 2,3,209,False
Small local model,3,210,False
Automating software development,2,212,False
Summary,2,224,False
Questions,2,224,False
Chapter  7: LLMs for Data Science,1,226,True
The impact of generative models on data science,2,227,False
Automated data science,2,230,False
Data collection,3,232,False
Visualization and EDA,3,233,False
Preprocessing and feature extraction,3,233,False
AutoML,3,234,False
Using agents to answer data science questions,2,236,False
Data exploration with LLMs,2,240,False
Summary,2,245,False
Questions,2,246,False
Chapter  8: Customizing LLMs and Their Output,1,248,True
Conditioning LLMs,2,249,False
Methods for conditioning,3,250,False
Reinforcement learning with human feedback,4,251,False
Low-rank adaptation,4,252,False
Inference-time conditioning,4,253,False
Fine-tuning,2,255,False
Setup for fine-tuning,3,256,False
Open-source models,3,259,False
Commercial models,3,264,False
Prompt engineering,2,265,False
Prompt techniques,3,267,False
Zero-shot prompting,4,269,False
Few-shot learning,4,269,False
Chain-of-thought prompting,4,271,False
Self-consistency,4,272,False
Tree-of-thought,4,274,False
Summary,2,278,False
Questions,2,278,False
Chapter  9: Generative AI in Production,1,280,True
How to get LLM apps ready for production,2,281,False
Terminology,3,283,False
How to evaluate LLM apps,2,284,False
Comparing two outputs,3,287,False
Comparing against criteria,3,288,False
String and semantic comparisons,3,290,False
Running evaluations against datasets,3,291,False
How to deploy LLM apps,2,296,False
FastAPI web server,3,299,False
Ray,3,303,False
How to observe LLM apps,2,307,False
Tracking responses,3,310,False
Observability tools,3,312,False
LangSmith,3,314,False
PromptWatch,3,317,False
Summary,2,318,False
Questions,2,319,False
Chapter  10: The Future of Generative Models,1,322,True
The current state of generative AI,2,323,False
Challenges,3,325,False
Trends in model development,3,327,False
Big Tech vs. small enterprises,3,330,False
Artificial General Intelligence,3,332,False
Economic consequences,2,333,False
Creative industries and advertising,3,336,False
Education,3,338,False
Law,3,338,False
Manufacturing,3,339,False
Medicine,3,339,False
Military,3,339,False
Societal implications,2,340,False
Misinformation and cybersecurity,3,341,False
Regulations and implementation challenges,3,342,False
The road ahead,2,344,False
Other Books You May Enjoy,1,348,False
Index,1,352,False
About this Book,1,6,False
How to Use this Book,2,6,False
Why Another Textbook?,2,6,False
Organization and Auxilary Material,2,7,False
Acknowledgements,2,7,False
Decision Trees,1,8,True
What Does it Mean to Learn?,2,8,False
Some Canonical Learning Problems,2,10,False
The Decision Tree Model of Learning,2,10,False
Formalizing the Learning Problem,2,14,False
Chapter Summary and Outlook,2,17,False
Further Reading,2,18,False
Limits of Learning,1,19,True
Data Generating Distributions,2,19,False
Inductive Bias: What We Know Before the Data Arrives,2,20,False
Not Everything is Learnable,2,21,False
Underfitting and Overfitting,2,22,False
Separation of Training and Test Data,2,24,False
"Models, Parameters and Hyperparameters",2,25,False
Real World Applications of Machine Learning,2,27,False
Further Reading,2,28,False
Geometry and Nearest Neighbors,1,29,True
From Data to Feature Vectors,2,29,False
K-Nearest Neighbors,2,31,False
Decision Boundaries,2,34,False
K-Means Clustering,2,35,False
Warning: High Dimensions are Scary,2,37,False
Further Reading,2,40,False
The Perceptron,1,41,True
Bio-inspired Learning,2,41,False
Error-Driven Updating: The Perceptron Algorithm,2,42,False
Geometric Intrepretation,2,45,False
Interpreting Perceptron Weights,2,47,False
Perceptron Convergence and Linear Separability,2,48,False
Improved Generalization: Voting and Averaging,2,51,False
Limitations of the Perceptron,2,53,False
Further Reading,2,54,False
Practical Issues,1,55,True
The Importance of Good Features,2,55,False
Irrelevant and Redundant Features,2,56,False
Feature Pruning and Normalization,2,58,False
Combinatorial Feature Explosion,2,61,False
Evaluating Model Performance,2,62,False
Cross Validation,2,64,False
Hypothesis Testing and Statistical Significance,2,67,False
Debugging Learning Algorithms,2,69,False
Bias/Variance Trade-off,2,71,False
Further Reading,2,72,False
Beyond Binary Classification,1,73,True
Learning with Imbalanced Data,2,73,False
Multiclass Classification,2,77,False
Ranking,2,81,False
Further Reading,2,86,False
Linear Models,1,87,True
The Optimization Framework for Linear Models,2,87,False
Convex Surrogate Loss Functions,2,89,False
Weight Regularization,2,91,False
Optimization with Gradient Descent,2,93,False
From Gradients to Subgradients,2,96,False
Closed-form Optimization for Squared Loss,2,97,False
Support Vector Machines,2,100,False
Further Reading,2,103,False
Bias and Fairness,1,104,True
Train/Test Mismatch,2,104,False
Unsupervised Adaptation,2,106,False
Supervised Adaptation,2,108,False
Fairness and Data Bias,2,110,False
How Badly can it Go?,2,112,False
Further Reading,2,115,False
Probabilistic Modeling,1,116,True
Classification by Density Estimation,2,116,False
Statistical Estimation,2,117,False
Naive Bayes Models,2,120,False
Prediction,2,121,False
Generative Stories,2,123,False
Conditional Models,2,124,False
Regularization via Priors,2,126,False
Further Reading,2,128,False
Neural Networks,1,129,True
Bio-inspired Multi-Layer Networks,2,129,False
The Back-propagation Algorithm,2,132,False
Initialization and Convergence of Neural Networks,2,135,False
Beyond Two Layers,2,136,False
Breadth versus Depth,2,138,False
Basis Functions,2,139,False
Further Reading,2,140,False
Kernel Methods,1,141,True
From Feature Combinations to Kernels,2,141,False
Kernelized Perceptron,2,142,False
Kernelized K-means,2,144,False
What Makes a Kernel,2,145,False
Support Vector Machines,2,148,False
Understanding Support Vector Machines,2,151,False
Further Reading,2,153,False
Learning Theory,1,154,True
The Role of Theory,2,154,False
Induction is Impossible,2,155,False
Probably Approximately Correct Learning,2,156,False
PAC Learning of Conjunctions,2,157,False
Occam's Razor: Simple Solutions Generalize,2,160,False
Complexity of Infinite Hypothesis Spaces,2,161,False
Further Reading,2,163,False
Ensemble Methods,1,164,True
Voting Multiple Classifiers,2,164,False
Boosting Weak Learners,2,166,False
Random Ensembles,2,169,False
Further Reading,2,170,False
Efficient Learning,1,171,True
What Does it Mean to be Fast?,2,171,False
Stochastic Optimization,2,172,False
Sparse Regularization,2,174,False
Feature Hashing,2,176,False
Further Reading,2,177,False
Unsupervised Learning,1,178,True
"K-Means Clustering, Revisited",2,178,False
Linear Dimensionality Reduction,2,182,False
Autoencoders,2,185,False
Further Reading,2,185,False
Expectation Maximization,1,186,True
Grading an Exam without an Answer Key,2,186,False
Clustering with a Mixture of Gaussians,2,189,False
The Expectation Maximization Framework,2,191,False
Further Reading,2,194,False
Structured Prediction,1,195,True
Multiclass Perceptron,2,196,False
Structured Perceptron,2,198,False
Argmax for Sequences,2,199,False
Structured Support Vector Machines,2,202,False
Loss-Augmented Argmax,2,205,False
Argmax in General,2,206,False
Dynamic Programming for Sequences,2,208,False
Further Reading,2,211,False
Imitation Learning,1,212,True
Imitation Learning by Classification,2,213,False
Failure Analysis,2,215,False
Dataset Aggregation,2,216,False
Expensive Algorithms as Experts,2,218,False
Structured Prediction via Imitation Learning,2,219,False
Further Reading,2,221,False
Code and Datasets,1,222,False
Bibliography,1,223,False
Index,1,225,False
Brief Contents,1,3,False
Contents,1,4,False
Preface,1,10,False
About this Book,1,12,False
--- Fundamentals of Deep Learning,1,16,False
What is deep learning?,1,17,True
"Artificial intelligence, machine learning,  and deep learning",2,18,False
Before deep learning:  a brief history of machine learning,2,28,False
Why deep learning? Why now?,2,34,False
Before we begin: the mathematical building blocks of neural networks,1,39,True
A first look at a neural network,2,41,False
Data representations for neural networks,2,45,False
The gears of neural networks: tensor operations,2,52,False
The engine of neural networks:  gradient-based optimization,2,60,False
Looking back at our first example,2,67,False
Chapter summary,2,69,False
Getting started with neural networks,1,70,True
Anatomy of a neural network,2,72,False
Introduction to Keras,2,75,False
Setting up a deep-learning workstation,2,79,False
Classifying movie reviews:  a binary classification example,2,82,False
Classifying newswires:  a multiclass classification example,2,92,False
Predicting house prices: a regression example,2,99,False
Chapter summary,2,106,False
Fundamentals of machine learning,1,107,True
Four branches of machine learning,2,108,False
Evaluating machine-learning models,2,111,False
"Data preprocessing, feature engineering,  and feature learning",2,115,False
Overfitting and underfitting,2,118,False
The universal workflow of machine learning,2,125,False
Chapter summary,2,130,False
--- Deep Learning in Practice,1,131,False
Deep learning for computer vision,1,132,True
Introduction to convnets,2,133,False
Training a convnet from scratch on a small dataset,2,143,False
Using a pretrained convnet,2,156,False
Visualizing what convnets learn,2,173,False
Chapter summary,2,190,False
Deep learning for text and sequences,1,191,True
Working with text data,2,193,False
Understanding recurrent neural networks,2,209,False
Advanced use of recurrent neural networks,2,220,False
Sequence processing with convnets,2,238,False
Chapter summary,2,245,False
Advanced deep-learning best practices,1,246,True
Going beyond the Sequential model:  the Keras functional API,2,247,False
Inspecting and monitoring deep-learning models using  Keras callbacks and TensorBoard,2,262,False
Getting the most out of your models,2,273,False
Chapter summary,2,281,False
Generative deep learning,1,282,True
Text generation with LSTM,2,284,False
DeepDream,2,293,False
Neural style transfer,2,300,False
Generating images with variational autoencoders,2,309,False
Introduction to generative adversarial networks,2,318,False
Chapter summary,2,326,False
Conclusions,1,327,True
Key concepts in review,2,328,False
The limitations of deep learning,2,338,False
The future of deep learning,2,343,False
Staying up to date in a fast-moving field,2,350,False
Final words,2,352,False
Installing Keras & its dependencies on Ubuntu,1,353,True
A.1 Installing the Python scientific suite,2,354,False
A.2 Setting up GPU support,2,355,False
A.3 Installing Theano (optional),2,356,False
A.4 Installing Keras,2,356,False
Running Jupyter notebooks on EC2 GPU instance,1,358,True
B.1 What are Jupyter notebooks?  Why run Jupyter notebooks on AWS GPUs?,2,358,False
B.2 Why would you not want to use Jupyter  on AWS for deep learning?,2,359,False
B.3 Setting up an AWS GPU instance,2,359,False
B.4 Installing Keras,2,363,False
B.5 Setting up local port forwarding,2,363,False
B.6 Using Jupyter from your local browser,2,363,False
Index,1,365,False
Cover,1,1,False
Title Page,1,2,False
Copyright and Credits,1,3,False
Contributors,1,5,False
Table of Contents,1,8,False
Preface,1,14,False
Part 1:Getting Started with OpenAI APIs,1,22,False
Chapter 1: Beginning with the ChatGPT API for NLP Tasks,1,24,True
Technical Requirements,2,25,False
The ChatGPT Revolution,2,25,False
Using ChatGPT from the Web,2,26,False
Creating an OpenAI Account,3,26,False
ChatGPT Web Interface,3,28,False
Getting Started with the ChatGPT API,2,29,False
Obtaining an API Key,3,29,False
API Tokens and Pricing,3,31,False
Setting Up Your Python Development Environment,2,33,False
Installing Python and the PyCharm IDE,3,33,False
Setting Up a Python Virtual Environment,3,34,False
The pip Package Installer,3,36,False
Building a Python Virtual Environment from the Terminal,3,37,False
A Simple ChatGPT API Response,2,37,False
Summary,2,42,False
Chapter 2: Building a ChatGPT Clone,1,44,True
Technical Requirements,2,45,False
Creating a ChatGPT Clone with Flask,2,45,False
Frontend HTML Generation,2,49,False
Enhancing the ChatGPT Clone Design,2,52,False
Intercepting ChatGPT API Endpoints,2,54,False
Summary,2,57,False
Part 2: Building Web Applications with the ChatGPT API,1,58,False
Chapter 3: Creating and Deploying an AI Code Bug Fixing SaaS Application Using Flask,1,60,True
Technical Requirements,2,61,False
Performing Multiple ChatGPT API Requests,2,61,False
Setting Up the Code Bug Fixer Project,2,62,False
Implementing the Code Bug Fixer Backend,2,65,False
Using Text Areas and Containers,2,67,False
Testing the Code Bug Fixer App,2,72,False
Deploying the ChatGPT App to the Azure Cloud,2,75,False
Summary,2,80,False
Chapter 4: Integrating the Code Bug Fixer Application with a Payment Service,1,82,True
Technical Requirements,2,83,False
Integrating Payments with Stripe,2,83,False
Setting Up a SQL User Database,2,86,False
Initializing a SQL Database,3,87,False
Getting a Browser Fingerprint ID,3,89,False
Tracking Application Users,3,90,False
Implementing the Usage Counters,3,92,False
Adding Payments to a ChatGPT Application,2,95,False
Building the Payments Page,3,95,False
Confirming User Payments,3,101,False
Summary,2,104,False
Chapter 5: Quiz Generation App with ChatGPT and Django,1,106,True
Technical Requirements,2,107,False
Building a Django Project,2,107,False
Creating the Exam App Frame and Views,2,112,False
Connecting Django Views and URLs,3,112,False
Developing Django Templates,3,114,False
Running Your Django Application,3,119,False
Integrating ChatGPT and Django for Quiz Generation,2,121,False
Building the Quiz Generation Text Area and Submit Button,3,121,False
Creating ChatGPT API Views with Django,3,123,False
Storing and Downloading Generated Quizzes,2,128,False
Saving the Quizzes in an SQLite Database,3,128,False
Building the Download Quiz View,3,131,False
Designing the Download Template,3,133,False
Summary,2,137,False
"Part 3: The ChatGPT, DALL-E, and Whisper APIs for Desktop Apps Development",1,140,False
Chapter 6: Language Translation Desktop App with the ChatGPT API and Microsoft Word,1,142,True
Technical Requirements,2,143,False
Integrating ChatGPT API with Microsoft Office,2,143,False
Building a User Interface with Tkinter,2,146,False
Integrating Microsoft Word Text with the ChatGPT API,2,149,False
Translating a Word Text with ChatGPT 3.5 Turbo,3,150,False
Summary,2,155,False
Chapter 7: Building an Outlook Email Reply Generator,1,156,True
Technical Requirements,2,157,False
Passing Outlook Data to the ChatGPT API,2,157,False
Setting Up the Outlook Email,3,158,False
Accessing Email Data with the win32com Client,3,160,False
Generating automatic email replies,3,164,False
Summary,2,169,False
Chapter 8: Essay Generation Tool with PyQt and the ChatGPT API,1,172,True
Technical Requirements,2,173,False
Building a Desktop Application with PyQT,2,173,False
Setting Up the Essay Generation Tool Project,3,174,False
Building the Application GUI with PyQt,3,175,False
Creating Essay Generation Methods with the ChatGPT API,2,180,False
Controlling the ChatGPT API Tokens,2,183,False
Summary,2,186,False
Chapter 9: Integrating ChatGPT and DALL-E API: Build End-to-End PowerPoint Presentation Generator,1,188,True
Technical Requirements,2,189,False
Using DALL-E and the DALL-E API,2,189,False
Building PowerPoint Apps with the PPTX Python Framework,2,191,False
Generating Art with the DALL-E API,2,196,False
Finalizing and Testing the AI Presentation Generator,2,198,False
Summary,2,203,False
Chapter 10: Speech Recognition and Text-to-Speech with the Whisper API,1,204,True
Technical Requirements,2,205,False
Implementing Text Translation and Transcription with the Whisper API,2,205,False
Building a Voice Transcriber Application,2,209,False
Using PyDub for Longer Audio Inputs,2,212,False
Summary,2,214,False
Part 4: Advanced Concepts for Powering ChatGPT Apps,1,216,False
Chapter 11: Choosing the Right ChatGPT API Model,1,218,True
Technical Requirements,2,219,False
"ChatGPT API Models – GPT-3, GPT-4, and Beyond",2,219,False
Using Chat Completion Parameters,2,224,False
ChatGPT API Rate Limits,2,230,False
Summary,2,232,False
Chapter 12: Fine-Tuning ChatGPT to Create Unique API Models,1,234,True
Technical Requirements,2,235,False
Fine-Tuning ChatGPT,2,235,False
Fine-Tuned Model Dataset Preparation,2,237,False
Building and Using the Fine-Tuned Model,2,241,False
Summary,2,246,False
Index,1,248,False
Other Books You May Enjoy,1,255,False
Front Cover,1,1,False
Half-Title Page,1,2,False
"LICENSE, DISCLAIMER OF LIABILITY, AND LIMITED WARRANTY",1,3,False
Title Page,1,4,False
Copyright Page,1,5,False
Dedication,1,6,False
Contents,1,8,False
Preface,1,12,False
Chapter 1 Introduction,1,16,True
What is Generative AI?,2,16,False
Conversational AI Versus Generative AI,2,18,False
Is DALL-E Part of Generative AI?,2,20,False
Are ChatGPT-3 and GPT-4 Part of Generative AI?,2,21,False
DeepMind,2,22,False
OpenAI,2,23,False
Cohere,2,24,False
Hugging Face,2,25,False
AI21,2,26,False
InflectionAI,2,26,False
Anthropic,2,26,False
What are LLMs?,2,27,False
What is AI Drift?,2,29,False
Machine Learning and Drift (Optional),2,30,False
What is Attention?,2,31,False
Calculating Attention: A High-Level View,2,34,False
An Example of Self Attention,2,36,False
Multi-Head Attention (MHA),2,40,False
Summary,2,42,False
Chapter 2 Tokenization,1,44,True
What is Pre-Tokenization?,2,44,False
What is Tokenization?,2,49,False
"Word, Character, and Subword Tokenizers",2,54,False
Trade-Offs with Character-Based Tokenizers,2,57,False
Subword Tokenization,2,58,False
Subword Tokenization Algorithms,2,61,False
Hugging Face Tokenizers and Models,2,64,False
Hugging Face Tokenizers,2,68,False
Tokenization for the DistilBERT Model,2,70,False
Token Selection Techniques in LLMs,2,74,False
Summary,2,74,False
Chapter 3 Transformer Architecture Introduction,1,76,True
Sequence-to-Sequence Models,2,77,False
Examples of seq2seq Models,2,79,False
What About RNNs and LSTMs?,2,81,False
Encoder/Decoder Models,2,82,False
Examples of Encoder/Decoder Models,2,84,False
Autoregressive Models,2,85,False
Autoencoding Models,2,87,False
The Transformer Architecture: Introduction,2,89,False
The Transformer is an Encoder/Decoder Model,2,93,False
The Transformer Flow and Its Variants,2,95,False
The transformers Library from Hugging Face,2,97,False
Transformer Architecture Complexity,2,99,False
Hugging Face Transformer Code Samples,2,100,False
Transformer and Mask-Related Tasks,2,106,False
Summary,2,110,False
Chapter 4 Transformer Architecture in Greater Depth,1,112,True
An Overview of the Encoder,2,113,False
What are Positional Encodings?,2,115,False
Other Details Regarding Encoders,2,118,False
An Overview of the Decoder,2,119,False
"Encoder, Decoder, or Both: How to Decide?",2,122,False
Delving Deeper into the Transformer Architecture,2,125,False
Autoencoding Transformers,2,129,False
The “Auto” Classes,2,130,False
Improved Architectures,2,131,False
Hugging Face Pipelines and How They Work,2,132,False
Hugging Face Datasets,2,134,False
Transformers and Sentiment Analysis,2,141,False
Source Code for Transformer-Based Models,2,141,False
Summary,2,142,False
Chapter 5 The BERT Family Introduction,1,144,True
What is Prompt Engineering?,2,145,False
Aspects of LLM Development,2,151,False
Kaplan and Under-Trained Models,2,154,False
What is BERT?,2,155,False
BERT and NLP Tasks,2,161,False
BERT and the Transformer Architecture,2,164,False
BERT and Text Processing,2,164,False
BERT and Data Cleaning Tasks,2,166,False
Three BERT Embedding Layers,2,167,False
Creating a BERT Model,2,168,False
Training and Saving a BERT Model,2,170,False
The Inner Workings of BERT,2,170,False
Summary,2,173,False
Chapter 6 The BERT Family in Greater Depth,1,174,True
A Code Sample for Special BERT Tokens,2,174,False
BERT-Based Tokenizers,2,176,False
Sentiment Analysis with DistilBERT,2,179,False
BERT Encoding: Sequence of Steps,2,181,False
Sentence Similarity in BERT,2,184,False
Generating BERT Tokens (1),2,187,False
Generating BERT Tokens (2),2,189,False
The BERT Family,2,191,False
Working with RoBERTa,2,197,False
Italian and Japanese Language Translation,2,198,False
Multilingual Language Models,2,200,False
"Translation for 1,000 Languages",2,201,False
M-BERT,2,202,False
Comparing BERT-Based Models,2,204,False
Web-Based Tools for BERT,2,205,False
Topic Modeling with BERT,2,207,False
What is T5?,2,208,False
Working with PaLM,2,209,False
Summary,2,210,False
Chapter 7 Working with GPT-3 Introduction,1,212,True
The GPT Family: An Introduction,2,213,False
GPT-2 and Text Generation,2,221,False
What is GPT-3?,2,225,False
GPT-3 Models,2,229,False
What is the Goal of GPT-3?,2,231,False
What Can GPT-3 Do?,2,232,False
Limitations of GPT-3,2,234,False
GPT-3 Task Performance,2,235,False
How GPT-3 and BERT are Different,2,236,False
The GPT-3 Playground,2,237,False
Inference Parameters,2,241,False
Overview of Prompt Engineering,2,244,False
Details of Prompt Engineering,2,246,False
Few-Shot Learning and Fine-Tuning LLMs,2,249,False
Summary,2,252,False
Chapter 8 Working with GPT-3 in Greater Depth,1,254,True
Fine-Tuning and Reinforcement Learning (Optional),2,255,False
GPT-3 and Prompt Samples,2,260,False
Working with Python and OpenAI APIs,2,280,False
Text Completion in OpenAI,2,285,False
The Completion() API in OpenAI,2,287,False
Text Completion and Temperature,2,289,False
Text Classification with GPT-3,2,294,False
Sentiment Analysis with GPT-3,2,296,False
GPT-3 Applications,2,299,False
Open-Source Variants of GPT-3,2,302,False
Miscellaneous Topics,2,306,False
Summary,2,308,False
Chapter 9 ChatGPT and GPT-4,1,310,True
What is ChatGPT?,2,310,False
"Plugins, Code Interpreter, and Code Whisperer",2,315,False
Detecting Generated Text,2,318,False
Concerns about ChatGPT,2,319,False
Sample Queries and Responses from ChatGPT,2,321,False
ChatGPT and Medical Diagnosis,2,324,False
Alternatives to ChatGPT,2,324,False
Machine Learning and ChatGPT: Advanced Data Analytics,2,326,False
What is InstructGPT?,2,327,False
VizGPT and Data Visualization,2,328,False
What is GPT-4?,2,330,False
ChatGPT and GPT-4 Competitors,2,332,False
LlaMa-2,2,335,False
When Will GPT-5 Be Available?,2,337,False
Summary,2,338,False
Chapter 10 Visualization with Generative AI,1,340,True
Generative AI and Art and Copyrights,2,341,False
Generative AI and GANs,2,341,False
What is Diffusion?,2,343,False
CLIP (OpenAI),2,345,False
GLIDE (OpenAI),2,346,False
Text-to-Image Generation,2,347,False
Text-to-Image Models,2,352,False
The DALL-E Models,2,353,False
DALL-E 2,2,359,False
DALL-E Demos,2,362,False
Text-to-Video Generation,2,364,False
Text-to-Speech Generation,2,366,False
Summary,2,367,False
Index,1,368,False
Preface,1,8,False
Conventions Used in This Book,2,9,False
Using Code Examples,2,10,False
O’Reilly Online Learning,2,11,False
How to Contact Us,2,11,False
Acknowledgments,2,12,False
From Sandra,3,12,False
From Shubham,3,13,False
1. The Era of Large Language Models,1,15,True
Natural Language Processing: Under the Hood,2,16,False
Language Models: Bigger and Better,2,19,False
The Generative Pre-Trained Transformer: GPT-3,2,20,False
Generative Models,3,20,False
Pre-trained Models,3,20,False
Transformer Models,3,25,False
A Brief History of GPT-3,2,27,False
GPT-1,3,27,False
GPT-2,3,28,False
GPT-3,3,29,False
Accessing the OpenAI API,2,32,False
2. Using the OpenAI API,1,35,True
Navigating the OpenAI Playground,2,35,False
Prompt Engineering and Design,3,39,False
How the OpenAI API Works,2,43,False
Execution Engine,3,45,False
Response Length,3,45,False
Temperature and Top P,3,46,False
Frequency and Presence Penalties,3,49,False
Best Of,3,49,False
Stop Sequence,3,50,False
Inject Start Text and Inject Restart Text,3,51,False
Show Probabilities,3,51,False
Execution Engines,2,53,False
Davinci,3,53,False
Curie,3,54,False
Babbage,3,54,False
Ada,3,55,False
Instruct Series,3,55,False
Endpoints,2,57,False
List Engines,3,58,False
Retrieve Engine,3,58,False
Completions,3,58,False
Semantic Search,3,58,False
Files,3,59,False
Classification (Beta),3,60,False
Answers (Beta),3,60,False
Embeddings,3,61,False
Customizing GPT-3,2,62,False
Apps Powered by Customized GPT-3 Models,3,63,False
How to Customize GPT-3 for Your Application,3,64,False
Tokens,2,67,False
Pricing,2,70,False
GPT-3’s Performance on Conventional NLP Tasks,2,73,False
Text Classification,3,74,False
Named Entity Recognition,3,81,False
Text Summarization,3,82,False
Text Generation,3,86,False
Conclusion,2,90,False
3. Programming with GPT-3,1,91,True
Using the OpenAI API with Python,2,91,False
Using the OpenAI API with Go,2,94,False
Using the OpenAI API with Java,2,98,False
GPT-3 Sandbox Powered by Streamlit,2,100,False
Going Live with GPT-3-Powered Applications,2,104,False
Conclusion,2,104,False
4. GPT-3 as a Launchpad for Next-Generation Start-ups,1,105,True
Model-as-a-Service,2,105,False
The New Start-up Ecosystem: Case Studies,2,109,False
Creative Applications of GPT-3: Fable Studio,3,109,False
Data Analysis Applications of GPT-3: Viable,3,114,False
Chatbot Applications of GPT-3: Quickchat,3,117,False
Marketing Applications of GPT-3: Copysmith,3,120,False
Coding Applications of GPT-3: Stenography,3,122,False
An Investor’s Perspective on the GPT-3 Start-up Ecosystem,2,125,False
Conclusion,2,126,False
5. GPT-3 for Corporations,1,128,True
Case Study: GitHub Copilot,2,129,False
How It Works,3,130,False
Developing Copilot,3,132,False
No-Code/Low-Code: Simplifying Software Development?,3,133,False
Scaling with the API,3,134,False
What’s Next for GitHub Copilot?,3,136,False
Case Study: Algolia Answers,2,136,False
Evaluating NLP Options,3,137,False
Data Privacy,3,138,False
Cost,3,138,False
Speed and Latency,3,139,False
Lessons Learned,3,140,False
Case Study: Microsoft Azure OpenAI Service,2,141,False
A Partnership That Was Meant to Be,3,141,False
An Azure-Native OpenAI API,3,142,False
Resource Management,3,143,False
Security and Data Privacy,3,143,False
Model-as-a-Service at the Enterprise Level,3,145,False
Other Microsoft AI and ML Services,3,146,False
Advice for Enterprises,3,147,False
OpenAI or Azure OpenAI Service: Which Should You Use?,3,147,False
Conclusion,2,148,False
"6. Challenges, Controversies, and Shortcomings",1,149,True
The Challenge of AI Bias,2,150,False
Anti-Bias Countermeasures,3,152,False
Low-Quality Content and the Spread of Misinformation,2,156,False
The Environmental Impact of LLMs,2,166,False
Proceeding with Caution,2,168,False
Conclusion,2,169,False
7. Democratizing Access to AI,1,171,True
No Code? No Problem!,2,171,False
Access and Model-as-a-Service,2,175,False
Conclusion,2,176,False
Index,1,177,False
About the Authors,1,201,False
"Cover",1,1,False
Copyright,1,3,False
Credits,1,4,False
Foreword,1,6,False
About the Author,1,7,False
About the Reviewers,1,8,False
www.PacktPub.com,1,10,False
Table of Contents,1,12,False
Preface,1,18,False
"Chapter 1: Giving Computers the Ability to Learn from Data",1,26,True
Building intelligent machines to transform data into knowledge,2,27,False
"The three different types of 
machine learning",2,27,False
Making predictions about the future with supervised learning,3,28,False
Classification for predicting class labels,4,28,False
Regression for predicting continuous outcomes,4,29,False
Solving interactive problems with reinforcement learning,3,31,False
Discovering hidden structures with unsupervised learning,3,31,False
Finding subgroups with clustering,4,32,False
Dimensionality reduction for data compression,4,32,False
An introduction to the basic terminology and notations,2,33,False
A roadmap for building machine learning systems,2,35,False
Preprocessing – getting data into shape,3,36,False
Training and selecting a predictive model,3,37,False
Evaluating models and predicting unseen data instances,3,38,False
Using Python for machine learning,2,38,False
Installing Python packages,3,38,False
Summary,2,40,False
"Chapter 2: Training Machine Learning Algorithms for Classification",1,42,True
Artificial neurons – a brief glimpse into the early history of machine learning,2,43,False
Implementing a perceptron learning algorithm in Python,2,49,False
Training a perceptron model on the Iris dataset,3,52,False
Adaptive linear neurons and the convergence of learning,2,58,False
Minimizing cost functions with gradient descent,3,59,False
"Implementing an Adaptive Linear Neuron in Python",3,61,False
Large scale machine learning and stochastic gradient descent,3,67,False
Summary,2,72,False
"Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-learn ",1,74,True
Choosing a classification algorithm,2,74,False
First steps with scikit-learn,2,75,False
Training a perceptron via scikit-learn,3,75,False
Modeling class probabilities via logistic regression,2,81,False
Logistic regression intuition and conditional probabilities,3,81,False
Learning the weights of the logistic cost function,3,84,False
Training a logistic regression model with scikit-learn,3,87,False
Tackling overfitting via regularization,3,90,False
Maximum margin classification with support vector machines,2,94,False
Maximum margin intuition,3,95,False
Dealing with the nonlinearly separable case using slack variables,3,96,False
Alternative implementations in scikit-learn,3,99,False
Solving nonlinear problems using a kernel SVM,2,100,False
Using the kernel trick to find separating hyperplanes in higher dimensional space,3,102,False
Decision tree learning,2,105,False
Maximizing information gain – getting the most bang for the buck,3,107,False
Building a decision tree,3,113,False
Combining weak to strong learners via random forests,3,115,False
K-nearest neighbors – a lazy learning algorithm,2,117,False
Summary,2,121,False
"Chapter 4: Building Good Training Sets – Data Preprocessing",1,124,True
Dealing with missing data,2,124,False
Eliminating samples or features with missing values,3,126,False
Imputing missing values,3,127,False
Understanding the scikit-learn estimator API,3,127,False
Handling categorical data,2,129,False
Mapping ordinal features,3,129,False
Encoding class labels,3,130,False
Performing one-hot encoding on nominal features,3,131,False
Partitioning a dataset in training and test sets,2,133,False
Bringing features onto the same scale,2,135,False
Selecting meaningful features,2,137,False
Sparse solutions with L1 regularization,3,137,False
Sequential feature selection algorithms,3,143,False
"Assessing feature importance with random forests",2,149,False
Summary,2,151,False
"Chapter 5: Compressing Data via Dimensionality Reduction",1,152,True
Unsupervised dimensionality reduction via principal component analysis,2,153,False
Total and explained variance,3,155,False
Feature transformation,3,158,False
Principal component analysis in scikit-learn,3,160,False
Supervised data compression via linear discriminant analysis,2,163,False
Computing the scatter matrices,3,165,False
Selecting linear discriminants for the new feature subspace,3,168,False
Projecting samples onto the new feature space,3,170,False
LDA via scikit-learn,3,171,False
Using kernel principal component analysis for nonlinear mappings,2,173,False
Kernel functions and the kernel trick,3,173,False
Implementing a kernel principal component analysis in Python,3,179,False
"Example 1 – separating half-moon shapes",4,180,False
"Example 2 – separating concentric circles",4,184,False
Projecting new data points,3,187,False
"Kernel principal component analysis in 
scikit-learn",3,191,False
Summary,2,192,False
"Chapter 6: Learning Best Practices for Model Evaluation and Hyperparameter Tuning",1,194,True
Streamlining workflows with pipelines,2,194,False
Loading the Breast Cancer Wisconsin dataset,3,195,False
Combining transformers and estimators in a pipeline,3,196,False
Using k-fold cross-validation to assess model performance,2,198,False
The holdout method,3,198,False
K-fold cross-validation,3,200,False
Debugging algorithms with learning and validation curves,2,204,False
Diagnosing bias and variance problems with learning curves,3,205,False
Addressing overfitting and underfitting with validation curves,3,208,False
Fine-tuning machine learning models via grid search,2,210,False
Tuning hyperparameters via grid search,3,211,False
"Algorithm selection with nested 
cross-validation",3,212,False
Looking at different performance evaluation metrics,2,214,False
Reading a confusion matrix,3,215,False
Optimizing the precision and recall of a classification model,3,216,False
Plotting a receiver operating characteristic,3,218,False
The scoring metrics for multiclass classification,3,222,False
Summary,2,223,False
"Chapter 7: Combining Different Models for Ensemble Learning",1,224,True
Learning with ensembles,2,224,False
Implementing a simple majority vote classifier,2,228,False
Combining different algorithms for classification with majority vote,3,235,False
Evaluating and tuning the ensemble classifier,2,238,False
Bagging – building an ensemble of classifiers from bootstrap samples,2,244,False
Leveraging weak learners via adaptive boosting,2,249,False
Summary,2,257,False
"Chapter 8: Applying Machine Learning to Sentiment Analysis",1,258,True
Obtaining the IMDb movie review dataset,2,258,False
Introducing the bag-of-words model,2,261,False
Transforming words into feature vectors,3,261,False
Assessing word relevancy via term frequency-inverse document frequency,3,263,False
Cleaning text data,3,265,False
Processing documents into tokens,3,267,False
Training a logistic regression model for document classification,2,269,False
Working with bigger data – online algorithms and out-of-core learning,2,271,False
Summary,2,275,False
"Chapter 9: Embedding a Machine Learning Model into a Web Application",1,276,True
Serializing fitted scikit-learn estimators,2,277,False
Setting up a SQLite database for data storage,2,280,False
Developing a web application with Flask,2,282,False
Our first Flask web application,3,283,False
Form validation and rendering,3,284,False
Turning the movie classifier into a web application,2,289,False
Deploying the web application to a public server,2,297,False
Updating the movie review classifier,3,299,False
Summary,2,301,False
"Chapter 10: Predicting Continuous Target Variables with Regression Analysis ",1,302,True
Introducing a simple linear regression model,2,303,False
Exploring the Housing Dataset,2,304,False
Visualizing the important characteristics of a dataset,3,305,False
Implementing an ordinary least squares linear regression model,2,310,False
Solving regression for regression parameters with gradient descent,3,310,False
Estimating the coefficient of a regression model via scikit-learn,3,314,False
Fitting a robust regression model using RANSAC,2,316,False
Evaluating the performance of linear regression models,2,319,False
Using regularized methods for regression,2,322,False
Turning a linear regression model into a curve – polynomial regression,2,323,False
Modeling nonlinear relationships in the Housing Dataset,3,325,False
Dealing with nonlinear relationships using random forests,3,329,False
Decision tree regression,4,329,False
Random forest regression,4,331,False
Summary,2,334,False
"Chapter 11: Working with Unlabeled Data – Clustering Analysis",1,336,True
Grouping objects by similarity using k-means,2,337,False
K-means++,3,340,False
Hard versus soft clustering,3,342,False
Using the elbow method to find the optimal number of clusters,3,345,False
Quantifying the quality of clustering via silhouette plots,3,346,False
Organizing clusters as a hierarchical tree,2,351,False
Performing hierarchical clustering on a distance matrix,3,353,False
Attaching dendrograms to a heat map,3,357,False
"Applying agglomerative clustering via 
scikit-learn",3,359,False
Locating regions of high density via DBSCAN,2,359,False
Summary,2,365,False
"Chapter 12: Training Artificial Neural Networks for Image Recognition",1,366,True
Modeling complex functions with artificial neural networks,2,367,False
Single-layer neural network recap,3,368,False
Introducing the multi-layer neural network architecture,3,370,False
Activating a neural network via forward propagation,3,372,False
Classifying handwritten digits,2,375,False
Obtaining the MNIST dataset,3,376,False
Implementing a multi-layer perceptron,3,381,False
Training an artificial neural network,2,390,False
Computing the logistic cost function,3,390,False
Training neural networks via backpropagation,3,393,False
Developing your intuition for backpropagation,2,397,False
Debugging neural networks with gradient checking,2,398,False
Convergence in neural networks,2,404,False
Other neural network architectures,2,406,False
Convolutional Neural Networks,3,406,False
Recurrent Neural Networks,3,408,False
A few last words about neural network implementation,2,409,False
Summary,2,410,False
"Chapter 13: Parallelizing Neural Network Training with Theano",1,412,True
"Building, compiling, and running expressions with Theano",2,413,False
What is Theano?,3,415,False
First steps with Theano,3,416,False
Configuring Theano,3,417,False
Working with array structures,3,419,False
Wrapping things up – a linear regression example,3,422,False
Choosing activation functions for feedforward neural networks,2,426,False
Logistic function recap,3,427,False
Estimating probabilities in multi-class classification via the softmax function,3,429,False
Broadening the output spectrum by using a hyperbolic tangent,3,430,False
Training neural networks efficiently using Keras,2,433,False
Summary,2,439,False
Index,1,442,False
